import os
import re
import sys
import json
import torch
import requests
from collections import Counter
from dotenv import load_dotenv
from elasticsearch import Elasticsearch
from neo4j import GraphDatabase
from transformers import AutoTokenizer, AutoModel
from ts_define_case_type import get_case_type  # âœ… æ”¹ç‚ºå¼•ç”¨åŒå­¸é‚è¼¯
from ts_input_filter import get_people  # âœ… åŠ å…¥å§“åæŠ½å–æ¨¡çµ„

# è‡ªå‹•è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# æ¨¡å‹èˆ‡è£ç½®è¨­å®š
BERT_MODEL = "shibing624/text2vec-base-chinese"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
TOKENIZER = AutoTokenizer.from_pretrained(BERT_MODEL)
MODEL = AutoModel.from_pretrained(BERT_MODEL, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32).to(device)

# ES èˆ‡ Neo4j åˆå§‹åŒ–
ES = Elasticsearch(
    os.getenv("ELASTIC_HOST"),
    basic_auth=(os.getenv("ELASTIC_USER"), os.getenv("ELASTIC_PASSWORD")),
    verify_certs=False,
)
CHUNK_INDEX = "legal_kg_chunks"

NEO4J_DRIVER = GraphDatabase.driver(
    os.getenv("NEO4J_URI"),
    auth=(os.getenv("NEO4J_USERNAME"), os.getenv("NEO4J_PASSWORD")),
)

# é¡å‹ fallback å°ç…§è¡¨
CASE_TYPE_MAP = {
    "æ•¸ååŸå‘Š": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    "æ•¸åè¢«å‘Š": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    "åŸè¢«å‘Šçš†æ•¸å": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    "åŸè¢«å‘Šçš†æ•¸å+Â§188åƒ±ç”¨äººæ¡ˆå‹": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    "æ•¸ååŸå‘Š+Â§188åƒ±ç”¨äººæ¡ˆå‹": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    "æ•¸åè¢«å‘Š+Â§188åƒ±ç”¨äººæ¡ˆå‹": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    "æ•¸åè¢«å‘Š+Â§187æœªæˆå¹´æ¡ˆå‹": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    "åŸè¢«å‘Šçš†æ•¸å+Â§187æœªæˆå¹´æ¡ˆå‹": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
}

# å¾ ts_input_filter æŠ½å–å§“åçµæœä¸­è§£æåŸå‘Šèˆ‡è¢«å‘Šå§“å
def extract_parties_from(party_info: str) -> dict:
    result = {"åŸå‘Š": "æœªæåŠ", "è¢«å‘Š": "æœªæåŠ"}
    match = re.search(r"åŸå‘Š[:ï¼š]?(.*?)\nè¢«å‘Š[:ï¼š]?(.*?)\n", party_info + "\n", re.S)
    if match:
        result["åŸå‘Š"] = match.group(1).strip()
        result["è¢«å‘Š"] = match.group(2).strip()
    return result

# æ–‡å­—å‘é‡åŒ–
def embed(text: str):
    t = TOKENIZER(text, truncation=True, padding="max_length", max_length=512, return_tensors="pt")
    t = {k: v.to(device) for k, v in t.items()}
    with torch.no_grad():
        vec = MODEL(**t).last_hidden_state.mean(dim=1).squeeze()
    return vec.cpu().numpy().tolist()

# ES æœå°‹ï¼ˆå¼·åŒ– fallback æ©Ÿåˆ¶ + å°å‡º payloadï¼‰
def es_search(query_vector, case_type: str, top_k: int = 5, label: str = "Facts"):
    def _search(label_filter, case_type_filter):
        must_clause = [{"match": {"label": label_filter}}]
        if case_type_filter:
            must_clause.append({"term": {"case_type.keyword": case_type_filter}})
        body = {
            "size": top_k,
            "query": {
                "script_score": {
                    "query": {"bool": {"must": must_clause}},
                    "script": {
                        "source": "cosineSimilarity(params.qv,'embedding')+1.0",
                        "params": {"qv": query_vector},
                    },
                }
            },
        }
        print("\nğŸ“¤ æŸ¥è©¢ payload (å‘é‡ç•¥å»)...")
        return ES.search(index=CHUNK_INDEX, body=body)["hits"]["hits"]

    print(f"ğŸ” ä½¿ç”¨ case_type='{case_type}' æœç´¢...")
    hits = _search(label, case_type)
    if not hits:
        fallback = CASE_TYPE_MAP.get(case_type, "å–®ç´”åŸè¢«å‘Šå„ä¸€")
        if fallback != case_type:
            print(f"âš ï¸ ä½¿ç”¨ case_type='{case_type}' ç„¡çµæœï¼Œæ”¹ç‚º fallback='{fallback}' é‡æ–°æœå°‹...")
            hits = _search(label, fallback)
    if not hits:
        print("âš ï¸ å†æ¬¡ç„¡çµæœï¼Œå°‡ä¸é™æ¡ˆä»¶é¡å‹é€²è¡Œæœ€å¯¬é¬†æœå°‹...")
        hits = _search(label, None)
    return hits

# Neo4j æŠ“å–è£œå¼·è³‡è¨Š
def query_laws(case_ids):
    counter = Counter()
    law_text_map = {}
    with NEO4J_DRIVER.session() as session:
        for cid in case_ids:
            result = session.run("""
                MATCH (c:Case {case_id: $cid})-[:åŒ…å«]->(:Facts)-[:é©ç”¨]->(l:Laws)-[:åŒ…å«]->(ld:LawDetail)
                RETURN collect(distinct ld.name) AS law_names, collect(distinct ld.text) AS law_texts
            """, cid=cid).single()
            if result:
                names = result["law_names"]
                texts = result["law_texts"]
                counter.update(names)
                for n, t in zip(names, texts):
                    if n not in law_text_map:
                        law_text_map[n] = t
    return counter, law_text_map

def parse_amount_string(raw):
    # å˜—è©¦å¾æ–‡å­—ä¸­æ‰¾å‡ºã€Œxxxå…ƒã€æˆ–ã€Œxxxè¬å…ƒã€
    match_million = re.search(r"(\d{1,3}(?:,\d{3})*|\d+)(?=\s*è¬[\s\u5143å…ƒ])", raw)
    if match_million:
        return float(match_million.group(1).replace(",", "")) * 10000
    match_plain = re.search(r"(\d{1,3}(?:,\d{3})*|\d+)(?=\s*[\u5143å…ƒ])", raw)
    if match_plain:
        return float(match_plain.group(1).replace(",", ""))
    return None

def query_conclusion_amounts(case_ids):
    value_dict = {}
    with NEO4J_DRIVER.session() as session:
        for cid in case_ids:
            result = session.run("""
                MATCH (c:Case {case_id: $cid})-[:åŒ…å«]->(:Facts)-[:é©ç”¨]->(:Laws)
                      -[:è¨ˆç®—]->(:Compensation)-[:æ¨å°]->(:Conclusion)-[:åŒ…å«]->(cd:ConclusionDetail)
                WHERE cd.name CONTAINS "ç¸½è¨ˆ"
                RETURN cd.name AS name, cd.value AS value
            """, cid=cid)

            rows = list(result)
            if not rows:
                continue
            for row in rows:
                raw = row["value"]
                print(f"ğŸ§¾ Case ID {cid} - {row['name']} åŸå§‹ value: {raw}")
                parsed = parse_amount_string(raw)
                if parsed:
                    value_dict[cid] = parsed
    if not value_dict:
        return {}, 0
    avg = sum(value_dict.values()) / len(value_dict)
    return value_dict, avg


def query_compensation_details(case_ids):
    detail_dict = {}
    with NEO4J_DRIVER.session() as session:
        for cid in case_ids:
            result = session.run("""
                MATCH (c:Case {case_id: $cid})-[:åŒ…å«]->(:Facts)-[:é©ç”¨]->(:Laws)
                      -[:è¨ˆç®—]->(:Compensation)-[:åŒ…å«]->(cd:CompensationDetail)
                RETURN cd.text AS text
            """, cid=cid)
            details = [r["text"] for r in result if r["text"]]
            if details:
                detail_dict[cid] = details
    return detail_dict

def extract_facts_and_injuries(text):
    fact_match = re.search(r"äº‹æ•…ç™¼ç”Ÿç·£ç”±[:ï¼š]?\s*(.*?)(?=åŸå‘Šå—å‚·æƒ…å½¢|è«‹æ±‚è³ å„Ÿ|$)", text, re.S)
    injury_match = re.search(r"(åŸå‘Š)?å—å‚·æƒ…å½¢[:ï¼š]?\s*(.*?)(?=è«‹æ±‚è³ å„Ÿ|$)", text, re.S)
    return (fact_match.group(1).strip() if fact_match else text,
            injury_match.group(2).strip() if injury_match else text)

def get_case_summary_prompt(accident_facts, injuries):
    return f"""è¦å‰‡:
1. ä¾æ“šè¼¸å…¥çš„äº‹å¯¦æè¿°ï¼Œç”Ÿæˆçµæ§‹æ¸…æ™°çš„äº‹æ•…æ‘˜è¦ï¼Œå®Œæ•´ä¿ç•™æ‰€æœ‰é—œéµè³‡è¨Šï¼Œä¸å¾—éºæ¼ã€‚
2. åƒ…ä½¿ç”¨è¼¸å…¥ä¸­æ˜ç¢ºæä¾›çš„è³‡è¨Šï¼Œä¸å¾—æ¨æ¸¬æˆ–è£œå……æœªå‡ºç¾åœ¨è¼¸å…¥ä¸­çš„å…§å®¹ï¼ˆä¾‹å¦‚ï¼šåˆ‘äº‹åˆ¤æ±ºï¼‰ã€‚
3. ä»¥ç°¡æ½”æ‰¼è¦çš„æ–¹å¼é™³è¿°å…§å®¹ï¼Œé¿å…å†—é•·æ•˜è¿°ï¼Œç¢ºä¿è³‡è¨Šæ¸…æ¥šæ˜“è®€ã€‚
4. è‹¥æŸå€‹è³‡è¨Šç¼ºå¤±ï¼Œå‰‡ä¸è¼¸å‡ºè©²é …ç›®ï¼Œå¡«å…¥ã€Œç„¡ã€æˆ–ã€Œä¸è©³ã€ã€‚
5. è‹¥äº‹å¯¦ä¸­å‡ºç¾å¦‚ã€Œå¤©å€™æ­£å¸¸ã€ã€Œç„¡ä¸èƒ½æ³¨æ„æƒ…äº‹ã€ã€Œè·¯æ³è‰¯å¥½ã€ç­‰é–“æ¥æè¿°ï¼Œäº¦è«‹ç´å…¥ [ç•¶å¤©ç’°å¢ƒ]ã€‚
è¼¸å‡ºæ ¼å¼ï¼š
=======================
[äº‹æ•…ç·£ç”±]: [å…§å®¹]
[ç•¶å¤©ç’°å¢ƒ]: [å…§å®¹]
[å‚·å‹¢æƒ…å½¢]: [å…§å®¹]
=======================
åš´æ ¼éµç…§ä¸Šè¿°è¦å‰‡ï¼Œæ ¹æ“šè¼¸å…¥è³‡è¨Šç”Ÿæˆäº‹æ•…æ‘˜è¦ã€‚

äº‹æ•…äº‹å¯¦ï¼š
{accident_facts}

å—å‚·æƒ…å½¢ï¼š
{injuries}
"""


def generate_case_summary(text):
    print("\nç”Ÿæˆæ¡ˆä»¶æ‘˜è¦...")
    facts, injuries = extract_facts_and_injuries(text)
    prompt = get_case_summary_prompt(facts, injuries)

    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": "kenneth85/llama-3-taiwan:8b-instruct-dpo",
            "prompt": prompt,
            "stream": False
        }
    )

    if response.status_code == 200:
        result = response.json()["response"]
        print("\nğŸ“‹ æ¡ˆä»¶æ‘˜è¦ï¼š\n")
        print(result.strip())
    else:
        print("âŒ Ollama è«‹æ±‚éŒ¯èª¤:", response.status_code, response.text)

def generate_final_prompt(user_query, summary_text, similar_facts, law_counts, law_texts, amount_stats, compensation_details):
    prompt = "æ ¹æ“šä»¥ä¸‹å¾‹å¸«è¼¸å…¥å…§å®¹èˆ‡ç›¸é—œæ¡ˆä¾‹è³‡è¨Šï¼Œè«‹ç”Ÿæˆä¸€ä»½æ ¼å¼æ¸…æ™°çš„èµ·è¨´ç‹€è‰ç¨¿ï¼š\n\n"

    # A. å¾‹å¸«è¼¸å…¥åŸæ–‡
    prompt += "ğŸ”¹ã€å¾‹å¸«è¼¸å…¥å…§å®¹ã€‘ï¼š\n" + user_query.strip() + "\n\n"

    # B. æ¡ˆä»¶æ‘˜è¦
    prompt += "ğŸ”¹ã€æ¡ˆä»¶æ‘˜è¦ã€‘ï¼š\n" + summary_text.strip() + "\n\n"

    # C. Top-K èªæ„ç›¸ä¼¼æ®µè½
    if similar_facts:
        prompt += "ğŸ”¹ã€ç›¸ä¼¼æ¡ˆä¾‹æ®µè½ï¼ˆä¾èªæ„ç›¸è¿‘æ’åºï¼‰ã€‘ï¼š\n"
        for i, fact in enumerate(similar_facts[:3], 1):
            prompt += f"{i}. {fact.strip()}\n"
        prompt += "\n"

    # D. æ³•æ¢æ‘˜è¦
    if law_counts:
        prompt += "ğŸ”¹ã€Top æ¡ˆä¾‹æ³•æ¢ã€‘ï¼š\n"
        for name, count in law_counts.most_common(5):
            text = law_texts.get(name, "")
            prompt += f"ã€{name}ã€‘(å‡ºç¾æ¬¡æ•¸ï¼š{count})\n{text.strip()}\n\n"

    # E. è³ å„Ÿé‡‘é¡çµ±è¨ˆèˆ‡æ˜ç´°
    if amount_stats:
        amount_dict, avg = amount_stats
        prompt += "ğŸ”¹ã€è³ å„Ÿé‡‘é¡çµ±è¨ˆã€‘ï¼š\n"
        for cid, amt in amount_dict.items():
            prompt += f"Case ID {cid}: {amt:,.0f} å…ƒ\n"
        prompt += f"\nğŸ’¡ å¹³å‡é‡‘é¡ï¼š{avg:,.0f} å…ƒ\n\n"

    if compensation_details:
        prompt += "ğŸ”¹ã€è³ å„Ÿé …ç›®æ˜ç´°ã€‘ï¼š\n"
        for cid, items in compensation_details.items():
            for i in items:
                prompt += f"Case ID {cid}: {i.strip()}\n"
        prompt += "\n"

    # F. ä»»å‹™æç¤º
    prompt += "è«‹ä¾ç…§ä¸€èˆ¬æ°‘äº‹èµ·è¨´æ›¸æ ¼å¼ï¼Œæ’°å¯«å‡ºå®Œæ•´èµ·è¨´æ›¸è‰ç¨¿ï¼Œçµæ§‹å»ºè­°ç‚ºï¼š\nä¸€ã€äº‹æ•…ç™¼ç”Ÿç¶“é\näºŒã€æ³•å¾‹ä¾æ“š\nä¸‰ã€æå®³é …ç›®\nå››ã€çµè«–ã€‚\n\nè«‹åƒè€ƒä»¥ä¸Šè³‡è¨Šé€²è¡Œæ’°å¯«ã€‚"

    return prompt.strip()



def process_query(query_text: str):
    print("ğŸ” è™•ç†ç”¨æˆ¶æŸ¥è©¢åˆ†é¡...")

    # èƒå–åŸå‘Šèˆ‡è¢«å‘Š
    party_info_raw = get_people(query_text)
    parties = extract_parties_from(party_info_raw)
    print("åŸå‘Š:", parties.get("åŸå‘Š", "-"))
    print("è¢«å‘Š:", parties.get("è¢«å‘Š", "-"))
    print("CASE INFO:", f"åŸå‘Š:{parties.get('åŸå‘Š', '-')}")

    # åˆ¤æ–·æ¡ˆä»¶é¡å‹
    case_type = get_case_type(query_text)
    if isinstance(case_type, (tuple, list)):
        case_type = case_type[0]
    print("æ¡ˆä»¶é¡å‹:", case_type)

    # äº’å‹•å¼é¸æ“‡
    search_type = input("è«‹é¸æ“‡æœå°‹é¡å‹ (1=å…¨æ–‡, 2=fact): ").strip()
    index_label = "Facts" if search_type == "2" else "FullText"
    top_k = int(input("è«‹è¼¸å…¥è¦æœå°‹çš„ Top-K æ•¸é‡: ").strip())
    grab_type = input("è«‹é¸æ“‡è¦æŠ“å–çš„å…§å®¹ (1=law, 2=law+conclusion): ")

    # Elasticsearch æœå°‹
    print(f"\nåœ¨ Elasticsearch ä¸­æœç´¢ '{index_label}' é¡å‹çš„ Top {top_k} å€‹æ–‡æª”...")
    hits = es_search(embed(query_text), case_type, top_k, label=index_label)
    if not hits:
        print("âŒ æŸ¥ç„¡ç›¸ä¼¼æ¡ˆä¾‹")
        return

    # æœå°‹çµæœé¡¯ç¤º
    case_ids = []
    top_facts = []  # ğŸ”¸ æ”¶é›†åŸå§‹æ®µè½æ–‡å­—
    for i, hit in enumerate(hits, 1):
        cid = hit['_source']['case_id']
        case_ids.append(cid)
        score = hit["_score"]
        original_text = hit["_source"].get("original_text", "").strip()
        print(f"{i}. Case ID: {cid}, ç›¸ä¼¼åº¦åˆ†æ•¸: {score:.4f}")
        if original_text:
            print(f"ğŸ”¸ ç›¸ä¼¼æ®µè½å…§å®¹:\n{original_text}\n")
            top_facts.append(original_text)

    # è£œå¼·è³‡è¨Šï¼šæ³•æ¢
    if grab_type.strip() in {"1", "2"}:
        law_counts, law_texts = query_laws(case_ids)
        print("\nğŸ“š æ³•æ¢å‡ºç¾é »ç‡:")
        for k, v in law_counts.most_common():
            print(f"æ³•æ¢ {k}: å‡ºç¾ {v} æ¬¡")
        print("\nğŸ“˜ æ³•æ¢å…§å®¹å°ç…§è¡¨:")
        for k, text in law_texts.items():
            print(f"ã€{k}ã€‘\n{text}\n")

    # è£œå¼·è³‡è¨Šï¼šçµè«–é‡‘é¡èˆ‡æ˜ç´°
    if grab_type.strip() == "2":
        values_dict, avg = query_conclusion_amounts(case_ids)
        print("\nğŸ’° è³ å„Ÿé‡‘é¡çµ±è¨ˆ:")
        for cid in case_ids:
            if cid in values_dict:
                print(f"Case ID {cid}: {values_dict[cid]:,.2f} å…ƒ")
            else:
                print(f"Case ID {cid}: â›” ç„¡æ³•è§£æé‡‘é¡")
        print(f"å¹³å‡è³ å„Ÿé‡‘é¡: {avg:,.2f} å…ƒ")

        detail_dict = query_compensation_details(case_ids)
        print("\nğŸ“„ å„æ¡ˆè³ å„Ÿé …ç›®æ˜ç´°:")
        for cid in case_ids:
            if cid in detail_dict:
                for desc in detail_dict[cid]:
                    print(f"Case ID {cid}: {desc}")
            else:
                print(f"Case ID {cid}: â›” ç„¡æ³•å–å¾—çµè«–æ˜ç´°")

    # âœ… ç”¢ç”Ÿæ¡ˆä»¶æ‘˜è¦
    generate_case_summary(query_text)

    # âœ… çµ„è£ Final Prompt
    facts, injuries = extract_facts_and_injuries(query_text)
    summary_prompt = get_case_summary_prompt(facts, injuries)
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={"model": "kenneth85/llama-3-taiwan:8b-instruct-dpo", "prompt": summary_prompt, "stream": False}
    )
    summary = response.json()["response"] if response.status_code == 200 else "â›” ç„¡æ³•å–å¾—æ‘˜è¦"

    final_prompt = generate_final_prompt(
    query_text,
    summary,
    top_facts[:3],
    law_counts,
    law_texts,
    (values_dict, avg),
    detail_dict
)
    print("\nğŸ§  çµ„è£å¾Œ Final Promptï¼š\n")
    print(final_prompt)
    
    # === åŸ·è¡Œ LLM èµ·è¨´æ›¸ç”Ÿæˆ ===
    print("\nğŸ“¤ æ­£åœ¨ä½¿ç”¨ LLM ç”Ÿæˆèµ·è¨´æ›¸...\n")
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": "kenneth85/llama-3-taiwan:8b-instruct-dpo",
            "prompt": final_prompt,
            "stream": False
        }
    )

    if response.status_code == 200:
        result = response.json()["response"]
        print("\nğŸ“‘ æœ€çµ‚ç”Ÿæˆçš„èµ·è¨´æ›¸ï¼š\n")
        print(result.strip())
    else:
        print("âŒ LLM è«‹æ±‚å¤±æ•—:", response.status_code, response.text)




if __name__ == "__main__":
    print("è«‹è¼¸å…¥ User Query (è«‹è²¼ä¸Šå®Œæ•´çš„å¾‹å¸«å›è¦†æ–‡æœ¬ï¼Œæ ¼å¼éœ€åŒ…å«ã€Œä¸€ã€äºŒã€ä¸‰ã€ã€ä¸‰å€‹éƒ¨åˆ†)")
    print("è¼¸å…¥å®Œç•¢å¾ŒæŒ‰ Enter å†è¼¸å…¥ 'q' æˆ– 'quit' çµæŸ:")

    buf = []
    while True:
        line = input()
        if line.strip().lower() in {"q", "quit"}:
            break
        buf.append(line)

    query_text = "\n".join(buf).strip()
    if query_text:
        process_query(query_text)
    else:
        print("âš ï¸  æœªè¼¸å…¥ä»»ä½•å…§å®¹ã€‚")