# âœ… KG_6.75_Chunkwise_Query_RAG.pyï¼šé•·è¼¸å…¥åˆ‡å¥å¾ŒæŸ¥ ES ä¸¦çµ„æˆ LLM prompt æ ¼å¼ + æ¡ˆä¾‹ç¸½çµ + citation æ¨™è¨˜ + é¡å‹çµ±è¨ˆ

import os
import re
import torch
import json
import numpy as np
from dotenv import load_dotenv
from elasticsearch import Elasticsearch
from transformers import AutoTokenizer, AutoModel
from collections import Counter, defaultdict

load_dotenv()
BERT_MODEL = "shibing624/text2vec-base-chinese"
torch_dtype = torch.float16

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

class ChunkwiseSemanticSearcher:
    def __init__(self):
        self.es = Elasticsearch(
            os.getenv("ELASTIC_HOST"),
            basic_auth=(os.getenv("ELASTIC_USER"), os.getenv("ELASTIC_PASSWORD")),
            verify_certs=False
        )
        self.tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL)
        self.model = AutoModel.from_pretrained(BERT_MODEL, torch_dtype=torch_dtype).to(device)

    def split_by_punctuation(self, text):
        return [s.strip() for s in re.split(r'[ã€‚ï¼ï¼Ÿï¼›\n]', text) if s.strip()]

    def embed(self, text):
        inputs = self.tokenizer(text, return_tensors="pt", padding="max_length", truncation=True, max_length=512)
        inputs = {k: v.to(device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = self.model(**inputs)
        return outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()

    def search_single_chunk(self, query_text, index_name="legal_kg_chunks", top_k=1):
        vector = self.embed(query_text)
        es_query = {
            "size": top_k,
            "query": {
                "script_score": {
                    "query": {"match_all": {}},
                    "script": {
                        "source": "cosineSimilarity(params.query_vector, 'embedding') + 1.0",
                        "params": {"query_vector": vector.tolist()}
                    }
                }
            }
        }
        response = self.es.search(index=index_name, body=es_query)
        return response["hits"]["hits"]

    def process_long_input(self, long_text, index_name="legal_kg_chunks", top_k=1):
        chunks = self.split_by_punctuation(long_text)
        results = []
        case_counter = Counter()
        label_counter = Counter()
        for i, chunk in enumerate(chunks):
            hits = self.search_single_chunk(chunk, index_name=index_name, top_k=top_k)
            if hits:
                top = hits[0]
                cid = top["_source"].get("case_id")
                label = top["_source"].get("label")
                score_raw = top["_score"]
                score_normalized = min(max(round((score_raw / 2.0) * 100), 0), 100)  # Normalize to 0~100
                case_counter[cid] += 1
                label_counter[label] += 1
                results.append({
                    "query_chunk": chunk,
                    "matched_chunk": top["_source"].get("semantic_text"),
                    "original_text": top["_source"].get("original_text", ""),
                    "case_id": cid,
                    "label": label,
                    "score": score_raw,
                    "score_100": score_normalized
                })
        return results, case_counter, label_counter

if __name__ == "__main__":
    searcher = ChunkwiseSemanticSearcher()

    long_query = """
    ä¸€ã€äº‹æ•…ç™¼ç”Ÿç·£ç”±:
    è¢«å‘Šæ–¼æ°‘åœ‹105å¹´4æœˆ12æ—¥13æ™‚27åˆ†è¨±ï¼Œé§•é§›ç§Ÿè³ƒå°å®¢è»Šæ²¿æ–°åŒ—å¸‚æŸå€æŸè·¯å¾€å¯Œåœ‹è·¯æ–¹å‘è¡Œé§›ã€‚
    è¡Œç¶“ç¦ç‡Ÿè·¯342è™Ÿå‰æ™‚ï¼Œè¢«å‘Šè·¨è¶Šåˆ†å‘é™åˆ¶ç·šæ¬²ç¹è¶Šå‰æ–¹ç”±åŸå‘Šæ‰€é§•é§›ä½µæ’æ–¼è·¯é‚Šè‡¨æ™‚åœè»Šå¾Œé©æ¬²èµ·é§›ä¹‹è»Šè¼›ã€‚
    è¢«å‘Šç‚ºé–ƒé¿å°å‘ä¾†è»Šï¼Œå› è€Œé§•è»Šè‡ªå¾Œè¿½æ’åŸå‘Šé§•é§›è»Šè¼›å·¦å¾Œè»Šå°¾ã€‚
    ç•¶æ™‚å¤©å€™æ™´æœ—ã€æ—¥é–“è‡ªç„¶å…‰ç·šã€æŸæ²¹é“è·¯ä¹¾ç‡¥ç„¡ç¼ºé™·æˆ–éšœç¤™ç‰©ã€è¦–è·è‰¯å¥½ï¼Œ
    è¢«å‘Šç†æ‡‰æ³¨æ„è»Šå‰ç‹€æ³åŠå…©è»Šä¸¦è¡Œä¹‹é–“éš”ï¼Œéš¨æ™‚æ¡å–å¿…è¦ä¹‹å®‰å…¨æªæ–½ï¼Œä½†å»ç–æœªæ³¨æ„è€Œç™¼ç”Ÿäº‹æ•…ã€‚

    äºŒã€åŸå‘Šå—å‚·æƒ…å½¢:
    åŸå‘Šå› æ­¤è»Šç¦å—æœ‰å·¦è†æŒ«å‚·ã€åŠæœˆè»Ÿéª¨å—å‚·ç­‰å‚·å®³ã€‚
    åŸå‘Šæ–¼105å¹´5æœˆ2æ—¥ã€7æ—¥ã€7æœˆ16æ—¥ã€8æœˆ13æ—¥ã€8æœˆ29æ—¥è‡³é†«é™¢é–€è¨ºå°±è¨ºï¼Œ105å¹´8æœˆ2æ—¥é€²è¡Œæ ¸ç£å…±æŒ¯é€ å½±æª¢æŸ¥ã€‚
    æ ¹æ“šé†«é™¢é–‹ç«‹çš„è¨ºæ–·è­‰æ˜æ›¸ï¼ŒåŸå‘Šéœ€ä¼‘é¤Š1å€‹æœˆã€‚

    ä¸‰ã€è«‹æ±‚è³ å„Ÿçš„äº‹å¯¦æ ¹æ“š:
    é†«ç™‚è²»ç”¨190å…ƒã€è»Šè¼›ä¿®å¾©è²»ç”¨181,144å…ƒã€äº¤é€šè²»ç”¨4,500å…ƒã€ä¼‘é¤Šæå¤±33,000å…ƒã€æ…°æ’«é‡‘99,000å…ƒã€‚
    åŸå‘Šå› å‚·ä¸è‰¯æ–¼è¡Œï¼Œä¿®ç†è²»ç”¨åŒ…æ‹¬å·¥è³‡è²»ç”¨88,774å…ƒå’Œé›¶ä»¶è²»92,370å…ƒï¼Œå¦éœ€æ­ä¹˜è¨ˆç¨‹è»Šä¸Šä¸‹ç­ã€‚
    åŸå‘Šè«‹æ±‚è¢«å‘Šä¾æ°‘æ³•ç¬¬184æ¢ã€ç¬¬191æ¢ä¹‹2ã€ç¬¬193æ¢åŠç¬¬195æ¢è³ å„Ÿä¸Šè¿°æå®³ï¼Œç¸½è¨ˆ317,834å…ƒã€‚
    """

    results, case_counter, label_counter = searcher.process_long_input(long_query)

    print("\nğŸ“š LLM Prompt è¼¸å…¥å€ï¼š\n")
    for res in results:
        print(f"[Q] {res['query_chunk']}")
        print(f"[R] {res['matched_chunk']}\nï¼ˆå‡ºè‡ª Case #{res['case_id']} çš„ {res['label']}ï¼Œèªæ„ç›¸ä¼¼åº¦è©•åˆ†ï¼š{res['score_100']} åˆ†ï¼‰\n")

    print("\nğŸ“Š èˆ‡è¼¸å…¥æœ€ç›¸ä¼¼çš„ Top 3 æ¡ˆä»¶ IDï¼š")
    for cid, count in case_counter.most_common(3):
        print(f"âœ”ï¸ Case ID: {cid} å‡ºç¾æ¬¡æ•¸: {count}")

    print("\nğŸ“‘ å‘½ä¸­ä¸»å¹¹çµ±è¨ˆï¼ˆFacts / Laws / Compensation / Conclusionï¼‰ï¼š")
    for label, count in label_counter.most_common():
        print(f"- {label}: {count} ç­†")
