# KG_700.py
import os
import re
import sys
import json
import torch
import requests
import jieba
import time  
from collections import Counter
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
from elasticsearch import Elasticsearch
from neo4j import GraphDatabase
from transformers import AutoTokenizer, AutoModel
from ts_define_case_type import get_case_type  
from ts_input_filter import get_people  
from ts_prompt import (
    get_facts_prompt,
    get_compensation_prompt_part1_single_plaintiff,
    get_compensation_prompt_part1_multiple_plaintiffs,
    get_compensation_prompt_part3,
    get_compensation_prompt_from_raw_input
)
from KG_110_input_enhancer import register_to_jieba
register_to_jieba()


# è‡ªå‹•è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# æ¨¡å‹èˆ‡è£ç½®è¨­å®š
BERT_MODEL = "shibing624/text2vec-base-chinese"
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
TOKENIZER = AutoTokenizer.from_pretrained(BERT_MODEL)
MODEL = AutoModel.from_pretrained(BERT_MODEL, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32).to(device)

# ES èˆ‡ Neo4j åˆå§‹åŒ–
ES = Elasticsearch(
    os.getenv("ELASTIC_HOST"),
    basic_auth=(os.getenv("ELASTIC_USER"), os.getenv("ELASTIC_PASSWORD")),
    verify_certs=False,
)
CHUNK_INDEX = "legal_kg_chunks"

NEO4J_DRIVER = GraphDatabase.driver(
    os.getenv("NEO4J_URI"),
    auth=(os.getenv("NEO4J_USERNAME"), os.getenv("NEO4J_PASSWORD")),
)

# é¡å‹ fallback å°ç…§è¡¨
CASE_TYPE_MAP = {
    "æ•¸ååŸå‘Š": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    "æ•¸åè¢«å‘Š": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    "åŸè¢«å‘Šçš†æ•¸å": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    "åŸè¢«å‘Šçš†æ•¸å+Â§188åƒ±ç”¨äººæ¡ˆå‹": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    "æ•¸ååŸå‘Š+Â§188åƒ±ç”¨äººæ¡ˆå‹": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    "æ•¸åè¢«å‘Š+Â§188åƒ±ç”¨äººæ¡ˆå‹": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    "æ•¸åè¢«å‘Š+Â§187æœªæˆå¹´æ¡ˆå‹": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    "åŸè¢«å‘Šçš†æ•¸å+Â§187æœªæˆå¹´æ¡ˆå‹": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
}

# å¾ ts_input_filter æŠ½å–å§“åçµæœä¸­è§£æåŸå‘Šèˆ‡è¢«å‘Šå§“å
def extract_parties_from(party_info: str) -> dict:
    result = {"åŸå‘Š": "æœªæåŠ", "è¢«å‘Š": "æœªæåŠ"}
    match = re.search(r"åŸå‘Š[:ï¼š]?(.*?)\nè¢«å‘Š[:ï¼š]?(.*?)\n", party_info + "\n", re.S)
    if match:
        result["åŸå‘Š"] = match.group(1).strip()
        result["è¢«å‘Š"] = match.group(2).strip()
    return result

# æ–‡å­—å‘é‡åŒ–
def embed(text: str):
    t = TOKENIZER(text, truncation=True, padding="max_length", max_length=512, return_tensors="pt")
    t = {k: v.to(device) for k, v in t.items()}
    with torch.no_grad():
        vec = MODEL(**t).last_hidden_state.mean(dim=1).squeeze()
    return vec.cpu().numpy().tolist()

# ES æœå°‹ï¼ˆå¼·åŒ– fallback æ©Ÿåˆ¶ + å°å‡º payloadï¼‰
def es_search(query_vector, case_type: str, top_k: int = 5, label: str = "Facts"):
    def _search(label_filter, case_type_filter):
        must_clause = [{"match": {"label": label_filter}}]
        if case_type_filter:
            must_clause.append({"term": {"case_type.keyword": case_type_filter}})
        body = {
            "size": top_k,
            "query": {
                "script_score": {
                    "query": {"bool": {"must": must_clause}},
                    "script": {
                        "source": "cosineSimilarity(params.qv,'embedding')+1.0",
                        "params": {"qv": query_vector},
                    },
                }
            },
        }
        print("\nğŸ“¤ æŸ¥è©¢ ç›¸ä¼¼æ®µè½ (å‘é‡ç•¥å»)...")
        return ES.search(index=CHUNK_INDEX, body=body)["hits"]["hits"]

    print(f"ğŸ” ä½¿ç”¨ case_type='{case_type}' æœç´¢...")
    hits = _search(label, case_type)
    if not hits:
        fallback = CASE_TYPE_MAP.get(case_type, "å–®ç´”åŸè¢«å‘Šå„ä¸€")
        if fallback != case_type:
            print(f"âš ï¸ ä½¿ç”¨ case_type='{case_type}' ç„¡çµæœï¼Œæ”¹ç‚º fallback='{fallback}' é‡æ–°æœå°‹...")
            hits = _search(label, fallback)
    if not hits:
        print("âš ï¸ å†æ¬¡ç„¡çµæœï¼Œå°‡ä¸é™æ¡ˆä»¶é¡å‹é€²è¡Œæœ€å¯¬é¬†æœå°‹...")
        hits = _search(label, None)
    return hits

def rerank_case_ids_by_paragraphs(query_text: str, case_ids: List[str], label: str = "Facts") -> List[str]:
    """
    æ ¹æ“šæ®µè½ç´šè³‡æ–™ï¼ˆlegal_kg_paragraphsï¼‰ï¼Œä»¥ cosine ç›¸ä¼¼åº¦é‡æ–°æ’åºæ¡ˆä¾‹
    """
    print("\nğŸ“˜ å•Ÿå‹•æ®µè½ç´š rerank...")
    from sklearn.metrics.pairwise import cosine_similarity
    import numpy as np

    # åµŒå…¥ç”¨æˆ¶è¼¸å…¥å…¨æ–‡
    query_vec = embed(query_text)
    query_vec_np = np.array(query_vec).reshape(1, -1)

    scored_cases = []
    for cid in case_ids:
        res = ES.search(
            index="legal_kg_paragraphs",
            query={
                "bool": {
                    "must": [
                        {"term": {"case_id": cid}},
                        {"term": {"label": label}}
                    ]
                }
            },
            size=1
        )
        hits = res.get("hits", {}).get("hits", [])
        if hits:
            vec = hits[0]['_source']['embedding']
            para_vec_np = np.array(vec).reshape(1, -1)
            score = cosine_similarity(query_vec_np, para_vec_np)[0][0]
            scored_cases.append((cid, score))

    # æ ¹æ“šç›¸ä¼¼åº¦æ’åº
    scored_cases.sort(key=lambda x: x[1], reverse=True)
    print("\nğŸ¯ Rerank å¾Œç›¸ä¼¼åº¦æ’åº:")
    for i, (cid, score) in enumerate(scored_cases, 1):
        print(f"{i}. Case {cid} âœ ç›¸ä¼¼åº¦: {score:.4f}")

    return [cid for cid, _ in scored_cases]

# Neo4j æŠ“å–è£œå¼·è³‡è¨Š
def query_laws(case_ids):
    counter = Counter()
    law_text_map = {}
    with NEO4J_DRIVER.session() as session:
        for cid in case_ids:
            result = session.run("""
                MATCH (c:Case {case_id: $cid})-[:åŒ…å«]->(:Facts)-[:é©ç”¨]->(l:Laws)-[:åŒ…å«]->(ld:LawDetail)
                RETURN collect(distinct ld.name) AS law_names, collect(distinct ld.text) AS law_texts
            """, cid=cid).single()
            if result:
                names = result["law_names"]
                texts = result["law_texts"]
                counter.update(names)
                for n, t in zip(names, texts):
                    if n not in law_text_map:
                        law_text_map[n] = t
    return counter, law_text_map

def keyword_law_filter(fact_text: str, injury_text: str, compensation_text: str) -> List[str]:
    """å¾ä¸‰æ®µäº‹å¯¦ä¸­æ¯”å°å‡ºå‘½ä¸­çš„æ³•æ¢æ¢è™Ÿ"""
    legal_mapping = {
        "æ°‘æ³•ç¬¬184æ¢ç¬¬1é …å‰æ®µ": ["æœªæ³¨æ„", "éå¤±", "æå®³è³ å„Ÿ", "ä¾µå®³ä»–äºº", "ä¾µå®³æ¬Šåˆ©"],
        "æ°‘æ³•ç¬¬185æ¢": ["å…±åŒä¾µå®³", "å…±åŒè¡Œç‚º", "æ•¸äººä¾µå®³", "é€ æ„äºº", "å…±åŒåŠ å®³"],
        "æ°‘æ³•ç¬¬187æ¢": ["ç„¡è¡Œç‚ºèƒ½åŠ›", "é™åˆ¶è¡Œç‚ºèƒ½åŠ›", "æ³•å®šä»£ç†äºº", "è­˜åˆ¥èƒ½åŠ›", "æœªæˆå¹´", "ç²¾ç¥éšœç¤™"],
        "æ°‘æ³•ç¬¬188æ¢": ["å—åƒ±äºº", "åƒ±ç”¨äºº", "é›‡å‚­", "é€£å¸¶è³ å„Ÿ", "è·å‹™ä¸Š", "é›‡ä¸»è²¬ä»»", "å—é›‡"],
        "æ°‘æ³•ç¬¬191-2æ¢": ["æ±½è»Š", "æ©Ÿè»Š", "äº¤é€šäº‹æ•…", "é«˜é€Ÿå…¬è·¯", "å‹•åŠ›è»Šè¼›", "é§•é§›"],
        "æ°‘æ³•ç¬¬193æ¢ç¬¬1é …": ["æå¤±", "é†«ç™‚è²»ç”¨", "å·¥ä½œæå¤±", "è–ªè³‡", "å°±é†«", "å‹å‹•èƒ½åŠ›", "æ”¶å…¥æå¤±"],
        "æ°‘æ³•ç¬¬195æ¢ç¬¬1é …å‰æ®µ": ["ç²¾ç¥", "æ…°æ’«é‡‘", "ç—›è‹¦", "åè­½", "å¥åº·", "éš±ç§", "è²æ“", "äººæ ¼"],
        "æ°‘æ³•ç¬¬213æ¢": ["å›å¾©åŸç‹€", "å›å¾©", "çµ¦ä»˜é‡‘éŒ¢", "æå®³ç™¼ç”Ÿ"],
        "æ°‘æ³•ç¬¬216æ¢": ["å¡«è£œæå®³", "æ‰€å¤±åˆ©ç›Š", "é æœŸåˆ©ç›Š", "æå¤±è£œå„Ÿ"],
        "æ°‘æ³•ç¬¬217æ¢": ["è¢«å®³äººèˆ‡æœ‰éå¤±", "éå¤±ç›¸æŠµ", "é‡å¤§æå®³åŸå› ", "æå®³æ“´å¤§"],
        "æ°‘æ³•ç¬¬190æ¢": ["å‹•ç‰©", "å¯µç‰©", "ç‹—", "è²“", "å‹•ç‰©æ”»æ“Š", "å‹•ç‰©å’¬å‚·"],

    }

    combined_text = "ã€‚".join([fact_text, injury_text, compensation_text])
    matched = set()
    for law, keywords in legal_mapping.items():
        if any(k in combined_text for k in keywords):
            matched.add(law)

    print("ğŸ“Œ é—œéµå­—å‘½ä¸­çš„æ³•æ¢:", matched)
    return sorted(matched)


def fetch_full_lawsuit_from_neo4j(driver, case_id):
    """å¾ Neo4j æ’ˆå‡ºå®Œæ•´å››æ®µèµ·è¨´ç‹€å…§å®¹ï¼ˆFacts, Laws, Compensation, Conclusionï¼‰"""
    with driver.session() as session:
        result = session.run("""
            MATCH (c:Case {case_id: $cid})-[:åŒ…å«]->(f:Facts)-[:é©ç”¨]->(l:Laws)-[:è¨ˆç®—]->(comp:Compensation)-[:æ¨å°]->(con:Conclusion)
            RETURN f.description AS fact, l.description AS law, comp.description AS comp, con.description AS con
        """, cid=case_id).single()

        if result:
            return [
                result.get("fact", "").strip(),
                result.get("law", "").strip(),
                result.get("comp", "").strip(),
                result.get("con", "").strip(),
            ]
        else:
            return []

def parse_amount_string(raw):
    # å˜—è©¦å¾æ–‡å­—ä¸­æ‰¾å‡ºã€Œxxxå…ƒã€æˆ–ã€Œxxxè¬å…ƒã€
    match_million = re.search(r"(\d{1,3}(?:,\d{3})*|\d+)(?=\s*è¬[\s\u5143å…ƒ])", raw)
    if match_million:
        return float(match_million.group(1).replace(",", "")) * 10000
    match_plain = re.search(r"(\d{1,3}(?:,\d{3})*|\d+)(?=\s*[\u5143å…ƒ])", raw)
    if match_plain:
        return float(match_plain.group(1).replace(",", ""))
    return None

def extract_calculate_tags(text: str) -> Dict[str, float]:
    pattern = r'<calculate>(.*?)</calculate>'
    matches = re.findall(pattern, text)
    print(f"æ‰¾åˆ° {len(matches)} å€‹ <calculate> æ¨™ç±¤å…§å®¹")
    sums = {}
    default_count = 0

    for match in matches:
        plaintiff_id = "default"
        name_match = re.search(r'åŸå‘Š(\w+)', match)
        if name_match:
            plaintiff_id = name_match.group(1)
        else:
            if "default" in sums:
                default_count += 1
                plaintiff_id = f"åŸå‘Š{default_count}"

        numbers = re.findall(r'\d+', match)
        if numbers:
            total = sum(float(num) for num in numbers)
            if plaintiff_id in sums:
                default_count += 1
                plaintiff_id = f"åŸå‘Š{default_count}"
            sums[plaintiff_id] = total
            print(f"è¨ˆç®— {plaintiff_id}: {total}")

    return sums


def query_conclusion_amounts(case_ids):
    value_dict = {}
    with NEO4J_DRIVER.session() as session:
        for cid in case_ids:
            result = session.run("""
                MATCH (c:Case {case_id: $cid})-[:åŒ…å«]->(:Facts)-[:é©ç”¨]->(:Laws)
                      -[:è¨ˆç®—]->(:Compensation)-[:æ¨å°]->(:Conclusion)-[:åŒ…å«]->(cd:ConclusionDetail)
                WHERE cd.name CONTAINS "ç¸½è¨ˆ"
                RETURN cd.name AS name, cd.value AS value
            """, cid=cid)

            rows = list(result)
            if not rows:
                continue
            for row in rows:
                raw = row["value"]
                print(f"ğŸ§¾ Case ID {cid} - {row['name']} åŸå§‹ value: {raw}")
                parsed = parse_amount_string(raw)
                if parsed:
                    value_dict[cid] = parsed
    if not value_dict:
        return {}, 0
    avg = sum(value_dict.values()) / len(value_dict)
    return value_dict, avg


def query_compensation_details(case_ids):
    detail_dict = {}
    with NEO4J_DRIVER.session() as session:
        for cid in case_ids:
            result = session.run("""
                MATCH (c:Case {case_id: $cid})-[:åŒ…å«]->(:Facts)-[:é©ç”¨]->(:Laws)
                      -[:è¨ˆç®—]->(:Compensation)-[:åŒ…å«]->(cd:CompensationDetail)
                RETURN cd.text AS text
            """, cid=cid)
            details = [r["text"] for r in result if r["text"]]
            if details:
                detail_dict[cid] = details
    return detail_dict

def extract_facts_and_injuries(text):
    fact_match = re.search(r"äº‹æ•…ç™¼ç”Ÿç·£ç”±[:ï¼š]?\s*(.*?)(?=åŸå‘Šå—å‚·æƒ…å½¢|è«‹æ±‚è³ å„Ÿ|$)", text, re.S)
    injury_match = re.search(r"(åŸå‘Š)?å—å‚·æƒ…å½¢[:ï¼š]?\s*(.*?)(?=è«‹æ±‚è³ å„Ÿ|$)", text, re.S)
    return (fact_match.group(1).strip() if fact_match else text,
            injury_match.group(2).strip() if injury_match else text)

def get_case_summary_prompt(accident_facts, injuries):
    return f"""è¦å‰‡:
1. ä¾æ“šè¼¸å…¥çš„äº‹å¯¦æè¿°ï¼Œç”Ÿæˆçµæ§‹æ¸…æ™°çš„äº‹æ•…æ‘˜è¦ï¼Œå®Œæ•´ä¿ç•™æ‰€æœ‰é—œéµè³‡è¨Šï¼Œä¸å¾—éºæ¼ã€‚
2. åƒ…ä½¿ç”¨è¼¸å…¥ä¸­æ˜ç¢ºæä¾›çš„è³‡è¨Šï¼Œä¸å¾—æ¨æ¸¬æˆ–è£œå……æœªå‡ºç¾åœ¨è¼¸å…¥ä¸­çš„å…§å®¹ï¼ˆä¾‹å¦‚ï¼šåˆ‘äº‹åˆ¤æ±ºï¼‰ã€‚
3. ä»¥ç°¡æ½”æ‰¼è¦çš„æ–¹å¼é™³è¿°å…§å®¹ï¼Œé¿å…å†—é•·æ•˜è¿°ï¼Œç¢ºä¿è³‡è¨Šæ¸…æ¥šæ˜“è®€ã€‚
4. è‹¥æŸå€‹è³‡è¨Šç¼ºå¤±ï¼Œå‰‡ä¸è¼¸å‡ºè©²é …ç›®ï¼Œå¡«å…¥ã€Œç„¡ã€æˆ–ã€Œä¸è©³ã€ã€‚
5. è‹¥äº‹å¯¦ä¸­å‡ºç¾å¦‚ã€Œå¤©å€™æ­£å¸¸ã€ã€Œç„¡ä¸èƒ½æ³¨æ„æƒ…äº‹ã€ã€Œè·¯æ³è‰¯å¥½ã€ç­‰é–“æ¥æè¿°ï¼Œäº¦è«‹ç´å…¥ [ç•¶å¤©ç’°å¢ƒ]ã€‚
è¼¸å‡ºæ ¼å¼ï¼š
=======================
[äº‹æ•…ç·£ç”±]: [å…§å®¹]
[ç•¶å¤©ç’°å¢ƒ]: [å…§å®¹]
[å‚·å‹¢æƒ…å½¢]: [å…§å®¹]
=======================
åš´æ ¼éµç…§ä¸Šè¿°è¦å‰‡ï¼Œæ ¹æ“šè¼¸å…¥è³‡è¨Šç”Ÿæˆäº‹æ•…æ‘˜è¦ã€‚

äº‹æ•…äº‹å¯¦ï¼š
{accident_facts}

å—å‚·æƒ…å½¢ï¼š
{injuries}
"""

def query_case_fulltext_sections(case_id: str) -> dict:
    """ å¾ Neo4j æŠ“å–èµ·è¨´ç‹€å››æ®µè½ """
    with NEO4J_DRIVER.session() as session:
        result = session.run("""
            MATCH (c:Case {case_id: $cid})-[:åŒ…å«]->(f:Facts)
            OPTIONAL MATCH (f)-[:é©ç”¨]->(l:Laws)
            OPTIONAL MATCH (l)-[:è¨ˆç®—]->(comp:Compensation)
            OPTIONAL MATCH (comp)-[:æ¨å°]->(con:Conclusion)
            RETURN f.description AS facts,
                   l.description AS laws,
                   comp.description AS compensation,
                   con.description AS conclusion
        """, cid=case_id).single()

        return {
            "facts": result["facts"] if result else "",
            "laws": result["laws"] if result else "",
            "compensation": result["compensation"] if result else "",
            "conclusion": result["conclusion"] if result else "",
        }

def generate_final_prompt(
    user_query: str,
    summary: str,
    top_facts: List[str],
    law_counts: Dict[str, int],
    law_texts: List[str],
    amount_stats: Dict[str, Any],
    compensation_details: Optional[str],
    full_sections: Dict[str, str],
    parties: Dict[str, str],
    case_type: str
) -> str:
    """
    é‡å¯«å¾Œçš„èµ·è¨´æ›¸ Prompt çµ„è£å‡½å¼ï¼Œå€åˆ†è³‡è¨Šä¾†æºï¼Œæ¸…æ¥šæç¤º LLMã€‚
    """

    # ğŸ”¹ã€æ¡ˆä»¶ç•¶äº‹äººã€‘
    party_block = (
        f"ğŸ”¹ã€æ¡ˆä»¶ç•¶äº‹äººã€‘\n"
        f"- åŸå‘Šï¼š{parties.get('åŸå‘Š', 'æœªæåŠ')}\n"
        f"- è¢«å‘Šï¼š{parties.get('è¢«å‘Š', 'æœªæåŠ')}\n"
        f"- æ¡ˆä»¶é¡å‹ï¼š{case_type or 'æœªåˆ†é¡'}\n"
    )

    # ğŸ”¹ã€å¾‹å¸«è¼¸å…¥åŸæ–‡ã€‘
    query_block = f"ğŸ”¹ã€å¾‹å¸«è¼¸å…¥åŸæ–‡ã€‘\n{user_query.strip()}"

    # ğŸ”¹ã€æœ¬æ¡ˆæ‘˜è¦ï¼ˆç”± LLM å¾è¼¸å…¥æ¨è«–ï¼‰ã€‘
    summary_block = f"ğŸ”¹ã€æœ¬æ¡ˆæ‘˜è¦ã€‘ï¼ˆä¾æ“šè¼¸å…¥ï¼Œè‡ªå‹•æ•´ç†ï¼‰\n{summary.strip()}"

    # ğŸ”¹ã€åƒè€ƒæ¡ˆä¾‹æ‘˜è¦ï¼ˆTop-Kï¼‰ã€‘
    if top_facts:
        similar_case_block = "ğŸ”¹ã€åƒè€ƒæ¡ˆä¾‹æ‘˜è¦ã€‘ï¼ˆTop-K ç›¸ä¼¼æ¡ˆä¾‹ç¯€éŒ„ï¼Œåƒ…ä¾›åƒè€ƒï¼‰\n"
        for i, s in enumerate(top_facts, 1):
            similar_case_block += f"{i}. {s.strip()}\n"
    else:
        similar_case_block = "ğŸ”¹ã€åƒè€ƒæ¡ˆä¾‹æ‘˜è¦ã€‘\nç„¡"

    # ğŸ”¹ã€å¸¸è¦‹æ³•æ¢æ‘˜è¦ã€‘
    if law_texts:
        law_text_block = "ğŸ”¹ã€å¸¸è¦‹æ³•æ¢æ‘˜è¦ã€‘ï¼ˆå¾ Top-K é¡ä¼¼æ¡ˆä»¶çµ±è¨ˆå½™æ•´ï¼‰\n"
        for i, text in enumerate(law_texts, 1):
            law_text_block += f"{i}. {text.strip()}\n"
        law_text_block += "\nä»¥ä¸Šæ³•æ¢ä¾›åƒè€ƒï¼Œä¸ä»£è¡¨æœ¬æ¡ˆä¸€å®šé©ç”¨ã€‚"
    else:
        law_text_block = "ğŸ”¹ã€å¸¸è¦‹æ³•æ¢æ‘˜è¦ã€‘\nç„¡"

    # ğŸ”¹ã€è³ å„Ÿçµ±è¨ˆæ‘˜è¦ã€‘
    amount_lines = []
    if isinstance(amount_stats.get("avg"), (int, float)):
        amount_lines.append(f"- å¹³å‡è³ å„Ÿé‡‘é¡ï¼šç´„ {format(amount_stats['avg'], ',')} å…ƒ")
    for cid, val in amount_stats.get("values", {}).items():
        amount_lines.append(f"- Case {cid}ï¼š{format(val, ',')} å…ƒ")
    if not amount_lines:
        amount_lines = ["ï¼ˆç„¡çµ±è¨ˆçµæœï¼‰"]
    amount_text = "ğŸ”¹ã€è³ å„Ÿçµ±è¨ˆæ‘˜è¦ã€‘ï¼ˆå¾ Top-K çµè«–ç¯€é»æ“·å–ï¼‰\n" + "\n".join(amount_lines)

    # ğŸ”¹ã€Top1 ç¯„æœ¬å››æ®µã€‘ï¼ˆåƒ…ä¾›æ ¼å¼åƒè€ƒï¼‰
    full_example = ""
    if full_sections:
        full_example = (
            "ğŸ”¹ã€Top1 èµ·è¨´æ›¸ç¯„æœ¬ã€‘ï¼ˆä»¥ä¸‹åƒ…ç‚ºæ ¼å¼åƒè€ƒï¼Œè«‹å‹¿ç›´æ¥å¼•ç”¨å…§å®¹ï¼‰\n"
            f"ä¸€ã€äº‹æ•…ç™¼ç”Ÿç¶“éï¼š\n{full_sections.get('facts', '').strip()}\n\n"
            f"äºŒã€æ³•å¾‹ä¾æ“šï¼š\n{full_sections.get('laws', '').strip()}\n\n"
            f"ä¸‰ã€æå®³é …ç›®ï¼š\n{full_sections.get('compensation', '').strip()}\n\n"
            f"å››ã€çµè«–ï¼š\n{full_sections.get('conclusion', '').strip()}\n"
        )

    # ğŸ“Œã€æ’°å¯«æŒ‡ä»¤ã€‘
    instruction = (
        "ğŸ“Œã€æ’°å¯«æŒ‡ä»¤ã€‘\n"
        "è«‹æ ¹æ“šä¸Šè¿°è³‡è¨Šæ’°å¯«å®Œæ•´çš„èµ·è¨´ç‹€è‰ç¨¿ã€‚\n"
        "å¿…é ˆä¾ç…§ä»¥ä¸‹çµæ§‹ç”¢å‡ºï¼š\n"
        "ä¸€ã€äº‹æ•…ç™¼ç”Ÿç¶“é\n"
        "äºŒã€æ³•å¾‹ä¾æ“š\n"
        "ä¸‰ã€æå®³é …ç›®\n"
        "å››ã€çµè«–\n\n"
        "è«‹å‹¿æŠ„è¥²ç¯„æœ¬æˆ–å…¶ä»–æ¡ˆä¾‹å…§å®¹ï¼Œæ‡‰æ ¹æ“šå¾‹å¸«è¼¸å…¥èˆ‡æœ¬æ¡ˆæ‘˜è¦é‡æ–°çµ„ç¹”èªå¥ï¼Œä½¿ç”¨æ¸…æ™°ã€å®¢è§€çš„æ³•å¾‹èªè¨€æè¿°ã€‚"
    )

    # ğŸ”§ çµ„è£ Final Prompt
    final_prompt = "\n\n".join([
        party_block,
        query_block,
        summary_block,
        similar_case_block,
        law_text_block,
        amount_text,
        instruction,
        full_example
    ])

    return final_prompt
    


def generate_case_summary(text):
    print("\nç”Ÿæˆæ¡ˆä»¶æ‘˜è¦...")
    facts, injuries = extract_facts_and_injuries(text)
    prompt = get_case_summary_prompt(facts, injuries)

    response = requests.post(
        "http://localhost:11434/api/generate",
        json={
            "model": "gemma3:27b",
            "prompt": prompt,
            "stream": True
        }
    )

    if response.status_code == 200:
        result = response.json()["response"]
        print("\nğŸ“‹ æ¡ˆä»¶æ‘˜è¦ï¼š\n")
        print(result.strip())
    else:
        print("âŒ Ollama è«‹æ±‚éŒ¯èª¤:", response.status_code, response.text)

def get_laws_prompt(article_ids: List[str], law_descriptions: dict) -> str:
    """
    æ ¹æ“šæ¢è™Ÿèˆ‡å°æ‡‰æè¿°ï¼Œè‡ªå‹•çµ„æˆæ³•å¾‹ä¾æ“šæ®µè½ prompt
    """
    if not article_ids:
        return "æŸ¥ç„¡ä»»ä½•ç›¸é—œæ³•æ¢ï¼Œè«‹ç¢ºèªæ˜¯å¦æœ‰æä¾›è¶³å¤ äº‹å¯¦è³‡æ–™ã€‚"

    law_segments = [f"ã€Œ{law_descriptions[a]}ã€" for a in article_ids if a in law_descriptions]
    law_text_block = "ã€\n".join(law_segments)
    article_list = "ã€".join(article_ids)

    return f"""ä½ æ˜¯ä¸€ä½ç†Ÿæ‚‰å°ç£æ°‘äº‹è¨´è¨Ÿçš„å¾‹å¸«åŠ©ç†ã€‚è«‹ä¾æ“šä¸‹åˆ—æ¢æ–‡èªªæ˜èˆ‡æ¢è™Ÿï¼Œæ’°å¯«æ³•å¾‹ä¾æ“šæ®µè½ï¼Œæ ¼å¼éœ€æ­£å¼ã€å®¢è§€ï¼Œä¸å¾—éåº¦æ¨è«–æˆ–åŠ å…¥æœªæä¾›äº‹å¯¦ã€‚

ã€æ¢æ–‡èªªæ˜ã€‘
{law_text_block}

ã€æ¢è™Ÿã€‘
{article_list}

è«‹ä¾ä»¥ä¸‹æ ¼å¼æ’°å¯«ï¼š

æŒ‰ã€Œï¼ˆæ¢æ–‡ç°¡è¿°1ï¼‰ã€ã€ã€Œï¼ˆæ¢æ–‡ç°¡è¿°2ï¼‰ã€...ï¼Œæ°‘æ³•ç¬¬XXXæ¢ã€ç¬¬YYYæ¢...åˆ†åˆ¥å®šæœ‰æ˜æ–‡ã€‚æŸ¥è¢«å‘Šå› ä¸Šé–‹ä¾µæ¬Šè¡Œç‚ºï¼Œè‡´åŸå‘Šå—æœ‰ä¸‹åˆ—æå®³ï¼Œä¾å‰æ­è¦å®šï¼Œè¢«å‘Šæ‡‰è² æå®³è³ å„Ÿè²¬ä»»ï¼š
"""

# æ³•æ¢æ¢æ–‡èªªæ˜è¡¨
law_descriptions_dict = {
    "æ°‘æ³•ç¬¬184æ¢ç¬¬1é …å‰æ®µ": "å› æ•…æ„æˆ–éå¤±ï¼Œä¸æ³•ä¾µå®³ä»–äººä¹‹æ¬Šåˆ©è€…ï¼Œè² æå®³è³ å„Ÿè²¬ä»»ã€‚",
    "æ°‘æ³•ç¬¬185æ¢": "æ•¸äººå…±åŒä¸æ³•ä¾µå®³ä»–äººä¹‹æ¬Šåˆ©è€…ï¼Œé€£å¸¶è² æå®³è³ å„Ÿè²¬ä»»ã€‚",
    "æ°‘æ³•ç¬¬187æ¢": "ç„¡è¡Œç‚ºèƒ½åŠ›æˆ–é™åˆ¶è¡Œç‚ºèƒ½åŠ›äººä¹‹æ³•å®šä»£ç†äººï¼Œæ‡‰è² è³ å„Ÿè²¬ä»»ã€‚",
    "æ°‘æ³•ç¬¬188æ¢": "å—åƒ±äººå› åŸ·è¡Œè·å‹™ï¼Œä¸æ³•ä¾µå®³ä»–äººä¹‹æ¬Šåˆ©è€…ï¼Œç”±åƒ±ç”¨äººèˆ‡è¡Œç‚ºäººé€£å¸¶è² è²¬ã€‚",
    "æ°‘æ³•ç¬¬191-2æ¢": "å‹•åŠ›è»Šè¼›åœ¨ä½¿ç”¨ä¸­è‡´äººæå®³è€…ï¼Œé§•é§›äººæ‡‰è² æå®³è³ å„Ÿè²¬ä»»ã€‚",
    "æ°‘æ³•ç¬¬193æ¢ç¬¬1é …": "ä¸æ³•ä¾µå®³ä»–äººä¹‹èº«é«”æˆ–å¥åº·è€…ï¼Œæ‡‰è² æå®³è³ å„Ÿè²¬ä»»ã€‚",
    "æ°‘æ³•ç¬¬195æ¢ç¬¬1é …å‰æ®µ": "ä¸æ³•ä¾µå®³äººæ ¼æ³•ç›Šè€Œæƒ…ç¯€é‡å¤§è€…ï¼Œå¾—è«‹æ±‚ç²¾ç¥æ…°æ’«é‡‘ã€‚",
    "æ°‘æ³•ç¬¬213æ¢": "è² æå®³è³ å„Ÿè²¬ä»»è€…ï¼Œæ‡‰å›å¾©æå®³ç™¼ç”Ÿå‰ä¹‹åŸç‹€ã€‚",
    "æ°‘æ³•ç¬¬216æ¢": "æå®³è³ å„ŸåŒ…æ‹¬å·²ç™¼ç”Ÿæå®³èˆ‡æ‰€å¤±åˆ©ç›Šã€‚",
    "æ°‘æ³•ç¬¬217æ¢": "è¢«å®³äººèˆ‡æœ‰éå¤±æ™‚ï¼Œæ³•é™¢å¾—æ¸›è¼•è³ å„Ÿé‡‘é¡ã€‚",
    "æ°‘æ³•ç¬¬190æ¢": "å‹•ç‰©åŠ æå®³æ–¼ä»–äººè€…ï¼Œç”±å…¶å æœ‰äººè² æå®³è³ å„Ÿè²¬ä»»ã€‚"
    
}

def generate_four_parts(
    user_query: str,
    accident_facts: str,
    injuries: str,
    summary: str,
    reference_facts: str,
    law_texts: list,
    comp_details: list,
    avg_amount: float,
    plaintiffs_info: str = "",
    top_law_numbers: List[str] = None,
    raw_comp_text: str = ""
) -> str:
    """
    å››æ®µå¼ç”Ÿæˆèµ·è¨´ç‹€ï¼Œä½¿ç”¨ raw_comp_text ä½œç‚ºæå®³æ®µè¼¸å…¥ï¼Œæ¸…ç†æ¨™é¡Œèˆ‡é™„ä»¶èªå¥ã€‚
    """

    # ğŸŸ¦ ç¬¬ä¸€æ®µï¼šäº‹æ•…ç™¼ç”Ÿç¶“é
    print("\nğŸ“é–‹å§‹ç”Ÿæˆç¬¬ä¸€æ®µï¼ˆäº‹æ•…ç™¼ç”Ÿç¶“éï¼‰...")
    facts_prompt = get_facts_prompt(accident_facts, reference_facts)
    facts_resp = requests.post(
        "http://localhost:11434/api/generate",
        json={"model": "gemma3:27b", "prompt": facts_prompt, "stream": False}
    )
    facts_result = facts_resp.json()["response"].strip() if facts_resp.ok else "âš ï¸ ç„¡æ³•ç”Ÿæˆäº‹å¯¦æ®µè½"
    facts_result = re.sub(r'^ä¸€[ã€.ï¼ ]+', '', facts_result)
    facts_result = "ä¸€ã€äº‹å¯¦æ¦‚è¿°ï¼š\n" + facts_result

    time.sleep(1)

    # ğŸŸ§ ç¬¬äºŒæ®µï¼šæ³•å¾‹ä¾æ“š
    print("\nğŸ“é–‹å§‹ç”Ÿæˆç¬¬äºŒæ®µï¼ˆæ³•å¾‹ä¾æ“šï¼‰...")
    laws_prompt = get_laws_prompt(top_law_numbers, law_descriptions_dict)
    laws_resp = requests.post(
        "http://localhost:11434/api/generate",
        json={"model": "gemma3:27b", "prompt": laws_prompt, "stream": False}
    )
    laws_result = laws_resp.json()["response"].strip() if laws_resp.ok else "âš ï¸ ç„¡æ³•ç”Ÿæˆæ³•å¾‹æ®µè½"
    laws_result = re.sub(r'^#+ *äºŒ[ã€.ï¼ ]*æ³•å¾‹ä¾æ“š[:ï¼š]?', '', laws_result)
    laws_result = "äºŒã€æ³•å¾‹ä¾æ“šï¼š\n" + laws_result

    time.sleep(1)

    # ğŸŸ¥ ç¬¬ä¸‰æ®µï¼šæå®³é …ç›®ï¼ˆä½¿ç”¨ raw_comp_textï¼‰
    print("\nğŸ“é–‹å§‹ç”Ÿæˆç¬¬ä¸‰æ®µï¼ˆæå®³é …ç›®ï¼‰...")
    comp_prompt = get_compensation_prompt_from_raw_input(
        raw_text=raw_comp_text,
        avg=avg_amount,
        plaintiffs_info=plaintiffs_info
    )
    comp_resp = requests.post(
        "http://localhost:11434/api/generate",
        json={"model": "gemma3:27b", "prompt": comp_prompt, "stream": False}
    )
    comp_result = comp_resp.json()["response"].strip() if comp_resp.ok else "âš ï¸ ç„¡æ³•ç”Ÿæˆæå®³æ®µè½"
    comp_result = re.sub(r'(è©³å¦‚é™„ä»¶.*?|é™„ä»¶.*?æ‰€ç¤º)', '', comp_result)
    comp_result = "ä¸‰ã€æå®³é …ç›®ï¼š\n" + comp_result

    time.sleep(1)

    # ğŸŸ© ç¬¬å››æ®µï¼šçµè«–
    print("\nğŸ“é–‹å§‹ç”Ÿæˆç¬¬å››æ®µï¼ˆçµè«–ï¼‰...")
    conclusion_prompt = get_compensation_prompt_part3(comp_result, "è«‹æ±‚å¦‚ä¸Šæ‰€åˆ—", plaintiffs_info=plaintiffs_info)
    con_resp = requests.post(
        "http://localhost:11434/api/generate",
        json={"model": "gemma3:27b", "prompt": conclusion_prompt, "stream": False}
    )
    conclusion_result = con_resp.json()["response"].strip() if con_resp.ok else "âš ï¸ ç„¡æ³•ç”Ÿæˆçµè«–æ®µè½"
    conclusion_result = "å››ã€çµè«–ï¼š" + conclusion_result

    # ğŸ§¾ çµ„è£
    return "\n\n".join([facts_result, laws_result, comp_result, conclusion_result])



def generate_compensation_facts_snippet(details: list) -> str:
    if not details:
        return "ç„¡è©³ç´°èªªæ˜ã€‚"
    return "\n".join(f"- {d.strip()}" for d in details if d and isinstance(d, str))


def process_query(query_text: str):
    print("ğŸ” è™•ç†ç”¨æˆ¶æŸ¥è©¢åˆ†é¡...")
    
    print("\nğŸ§ª æ–·è©æ¸¬è©¦ï¼ˆå«æ–°è©è¡¨ï¼‰:")
    test = "åŸå‘Šæ–¼äº‹æ•…ä¸­å—æœ‰è…¦éœ‡ç›ªåŠå·¦è†è“‹éª¨è£‚ï¼Œé ˆä¼‘é¤Šä¸‰å€‹æœˆã€‚"
    print("/".join(jieba.cut(test)))

    # 1ï¸âƒ£ æŠ½å–å§“åèˆ‡é¡å‹
    party_info_raw = get_people(query_text)
    parties = extract_parties_from(party_info_raw)

    print("åŸå‘Š:", parties.get("åŸå‘Š", "-"))
    print("è¢«å‘Š:", parties.get("è¢«å‘Š", "-"))

    case_type = get_case_type(query_text)
    if isinstance(case_type, (tuple, list)):
        case_type = case_type[0]
    print("æ¡ˆä»¶é¡å‹:", case_type)

    # 2ï¸âƒ£ é¸æ“‡æœå°‹åƒæ•¸
    search_type = input("è«‹é¸æ“‡æœå°‹é¡å‹ (1=å…¨æ–‡, 2=fact): ").strip()
    index_label = "Facts" if search_type == "2" else "FullText"
    top_k = int(input("è«‹è¼¸å…¥è¦æœå°‹çš„ Top-K æ•¸é‡: ").strip())
    grab_type = input("è«‹é¸æ“‡è¦æŠ“å–çš„å…§å®¹ (1=law, 2=law+conclusion): ")

    # 3ï¸âƒ£ ES æœå°‹
    hits = es_search(embed(query_text), case_type, top_k, label=index_label)
    if not hits:
        print("âŒ æŸ¥ç„¡ç›¸ä¼¼æ¡ˆä¾‹")
        return
    retrieved_case_ids = [hit['_source']['case_id'] for hit in hits]
    case_ids = rerank_case_ids_by_paragraphs(query_text, retrieved_case_ids, label=index_label)


    # 4ï¸âƒ£ å°å‡ºç›¸ä¼¼æ®µè½æ‘˜è¦
    top_facts = []
    for i, hit in enumerate(hits, 1):
        cid = hit['_source']['case_id']
        print(f"{i}. Case ID: {cid}, ç›¸ä¼¼åº¦åˆ†æ•¸: {hit['_score']:.4f}")
        original_text = hit["_source"].get("original_text", "").strip()
        if original_text:
            print(f"ğŸ”¸ ç›¸ä¼¼æ®µè½å…§å®¹:\n{original_text}\n")
            top_facts.append(original_text)

        
    # 5ï¸âƒ£ Neo4j è£œå¼·è³‡è¨Š
    if grab_type.strip() in {"1", "2"}:
        law_counts, law_texts = query_laws(case_ids)
    else:
        law_counts, law_texts = {}, {}

    if grab_type.strip() == "2":
        values_dict, avg = query_conclusion_amounts(case_ids)
        detail_dict = query_compensation_details(case_ids)
    else:
        values_dict, avg = {}, 0
        detail_dict = {}


    # 6ï¸âƒ£ æ¡ˆä»¶æ‘˜è¦ by Gemma
    print("\nğŸ“‹ æ¡ˆä»¶æ‘˜è¦ï¼ˆGemma é€è¡Œç”Ÿæˆä¸­ï¼‰...\n")
    accident_facts, injuries = extract_facts_and_injuries(query_text)
    # â¬ æŠ½å–ã€Œä¸‰ã€è«‹æ±‚è³ å„Ÿçš„äº‹å¯¦æ ¹æ“šã€æ®µè½ä½œç‚ºæå®³è¼¸å…¥
    def extract_raw_compensation_text(user_input: str) -> str:
        match = re.search(r"ä¸‰[ã€.ï¼ï¼š:]?\s*è«‹æ±‚è³ å„Ÿçš„äº‹å¯¦æ ¹æ“š[:ï¼š]?\s*(.*)", user_input, re.S)
        return match.group(1).strip() if match else ""

    raw_comp_text = extract_raw_compensation_text(query_text)

    summary_prompt = get_case_summary_prompt(accident_facts, injuries)
    response = requests.post(
        "http://localhost:11434/api/generate",
        json={"model": "gemma3:27b", "prompt": summary_prompt, "stream": True}
    )
    summary_lines = []
    if response.status_code == 200:
        for line in response.iter_lines():
            if line:
                try:
                    content = json.loads(line.decode("utf-8"))["response"]
                    print(content, end="", flush=True)
                    summary_lines.append(content)
                except Exception:
                    continue
        print("\n")
    else:
        print("âŒ LLM è«‹æ±‚éŒ¯èª¤:", response.status_code, response.text)
        return
    summary = "".join(summary_lines).strip()

    # 7ï¸âƒ£ æŠ“ Top1 ç¯„æœ¬å››æ®µï¼ˆæä¾›æ ¼å¼åƒè€ƒï¼‰
    top1_case_id = case_ids[0]
    full_sections = query_case_fulltext_sections(top1_case_id)
    print("\nğŸ“˜ Top1 ç¯„ä¾‹èµ·è¨´æ›¸ï¼ˆåƒ…ä¾›æ ¼å¼åƒè€ƒï¼‰:")
    # print("ä¸€ã€äº‹å¯¦æ¦‚è¿°ï¼š")
    print(full_sections.get("facts", "").strip(), "\n")
    # print("äºŒã€æ³•å¾‹ä¾æ“šï¼š")
    print(full_sections.get("laws", "").strip(), "\n")
    print("ä¸‰ã€æå®³è³ å„Ÿé …ç›®ï¼š")
    print(full_sections.get("compensation", "").strip(), "\n")
    print("å››ã€çµè«–ï¼š")
    print(full_sections.get("conclusion", "").strip(), "\n")


    # 8ï¸âƒ£ æ•´ç†æå®³äº‹å¯¦å­—ä¸²
    comp_details = detail_dict.get(top1_case_id, [])
    comp_facts = generate_compensation_facts_snippet(comp_details)

    # 9ï¸âƒ£ å››æ®µå¼ LLM ç”Ÿæˆ
    top_law_numbers = keyword_law_filter(accident_facts, injuries, comp_facts)
    top_law_texts = [law_texts[l] for l in top_law_numbers if l in law_texts]
    if not top_law_texts:
        top_law_texts = list(law_texts.values())[:3]  # fallback


    full_text = generate_four_parts(
        user_query=query_text,
        accident_facts=accident_facts,
        injuries=injuries,
        summary=summary,
        reference_facts=full_sections.get("facts", ""),
        law_texts=top_law_texts,
        comp_details=comp_details,
        avg_amount=avg,
        plaintiffs_info=parties.get("åŸå‘Š", ""),
        top_law_numbers=top_law_numbers,
        raw_comp_text=raw_comp_text  # âœ… æ–°å¢é€™è¡Œ
    )


    # ğŸ”Ÿ é¡¯ç¤ºæœ€çµ‚çµæœ
    print("\nğŸ“‘ æœ€çµ‚ç”Ÿæˆçš„å››æ®µèµ·è¨´ç‹€ï¼š\n")
    print(full_text)


if __name__ == "__main__":
    print("è«‹è¼¸å…¥ User Query (è«‹è²¼ä¸Šå®Œæ•´çš„å¾‹å¸«å›è¦†æ–‡æœ¬ï¼Œæ ¼å¼éœ€åŒ…å«ã€Œä¸€ã€äºŒã€ä¸‰ã€ã€ä¸‰å€‹éƒ¨åˆ†)")
    print("è¼¸å…¥å®Œç•¢å¾ŒæŒ‰ Enter å†è¼¸å…¥ 'q' æˆ– 'quit' çµæŸ:")

    buf = []
    while True:
        line = input()
        if line.strip().lower() in {"q", "quit"}:
            break
        buf.append(line)

    query_text = "\n".join(buf).strip()
    if query_text:
        process_query(query_text)
    else:
        print("âš ï¸  æœªè¼¸å…¥ä»»ä½•å…§å®¹ã€‚")