#!/usr/bin/env python3
"""
KG_700_CoT_Hybrid.py
æ··åˆæ¨¡å¼ï¼šäº‹å¯¦ã€æ³•æ¢å’Œæå®³ç”¨æ¨™æº–æ–¹æ³•ï¼Œçµè«–ç”¨CoT
å„ªåŒ–ç‰ˆæœ¬ï¼šä¿®æ­£æœªæˆå¹´èª¤åˆ¤ã€æ”¹é€²æå®³é …ç›®è™•ç†ã€å¢å¼·ç©©å®šæ€§
"""

import os
import re
import json
import requests
import time
import sys
from typing import List, Dict, Any, Optional
from collections import Counter

# å°å…¥å¿…è¦æ¨¡çµ„
try:
    import torch
    from transformers import AutoTokenizer, AutoModel
    from elasticsearch import Elasticsearch
    from neo4j import GraphDatabase
    from dotenv import load_dotenv
    FULL_MODE = True
    print("âœ… å®Œæ•´æ¨¡å¼ï¼šæ‰€æœ‰æª¢ç´¢åŠŸèƒ½å¯ç”¨")
except ImportError as e:
    print(f"âš ï¸ éƒ¨åˆ†æ¨¡çµ„æœªå®‰è£ï¼š{e}")
    print("âš ï¸ ä½¿ç”¨ç°¡åŒ–æ¨¡å¼ï¼ˆåƒ…LLMç”ŸæˆåŠŸèƒ½ï¼‰")
    FULL_MODE = False

# å°å…¥çµæ§‹åŒ–é‡‘é¡è™•ç†å™¨
try:
    from structured_legal_amount_processor import StructuredLegalAmountProcessor
    STRUCTURED_PROCESSOR_AVAILABLE = True
    print("âœ… çµæ§‹åŒ–é‡‘é¡è™•ç†å™¨è¼‰å…¥æˆåŠŸ")
except ImportError:
    STRUCTURED_PROCESSOR_AVAILABLE = False
    print("âš ï¸ çµæ§‹åŒ–é‡‘é¡è™•ç†å™¨æœªæ‰¾åˆ°")

# å°å…¥åŸºæœ¬é‡‘é¡æ¨™æº–åŒ–å™¨
try:
    from legal_amount_standardizer import LegalAmountStandardizer
    BASIC_STANDARDIZER_AVAILABLE = True
    print("âœ… åŸºæœ¬é‡‘é¡æ¨™æº–åŒ–å™¨è¼‰å…¥æˆåŠŸ")
except ImportError:
    BASIC_STANDARDIZER_AVAILABLE = False
    print("âš ï¸ åŸºæœ¬é‡‘é¡æ¨™æº–åŒ–å™¨æœªæ‰¾åˆ°")

# ===== åŸºæœ¬è¨­å®š =====
LLM_URL = "http://localhost:11434/api/generate"
DEFAULT_MODEL = "gemma3:27b"

# ===== æª¢ç´¢ç³»çµ±è¨­å®š =====
if FULL_MODE:
    # è¼‰å…¥ç’°å¢ƒè®Šæ•¸
    env_path = os.path.join(os.path.dirname(__file__), '..', '01_è¨­å®šèˆ‡é…ç½®', '.env')
    load_dotenv(dotenv_path=env_path)
    
    # åµŒå…¥æ¨¡å‹è¨­å®š
    BERT_MODEL = "shibing624/text2vec-base-chinese"
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    try:
        TOKENIZER = AutoTokenizer.from_pretrained(BERT_MODEL)
        MODEL = AutoModel.from_pretrained(BERT_MODEL, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32).to(device)
        print("âœ… åµŒå…¥æ¨¡å‹è¼‰å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ åµŒå…¥æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        FULL_MODE = False
    
    # ES å’Œ Neo4j é€£æ¥
    try:
        # ä½¿ç”¨ requests ç›´æ¥èª¿ç”¨ ES API é¿å…ç‰ˆæœ¬å…¼å®¹æ€§å•é¡Œ
        ES_HOST = os.getenv("ELASTIC_HOST")
        ES_USER = os.getenv("ELASTIC_USER")
        ES_PASSWORD = os.getenv("ELASTIC_PASSWORD")
        ES_AUTH = (ES_USER, ES_PASSWORD)
        
        # æ¸¬è©¦ ES é€£æ¥
        response = requests.get(f"{ES_HOST}/_cluster/health", auth=ES_AUTH, verify=False)
        if response.status_code != 200:
            raise Exception(f"ESé€£æ¥å¤±æ•—: {response.status_code}")
        
        NEO4J_DRIVER = GraphDatabase.driver(
            os.getenv("NEO4J_URI"),
            auth=(os.getenv("NEO4J_USERNAME"), os.getenv("NEO4J_PASSWORD")),
        )
        CHUNK_INDEX = "legal_kg_chunks"
        print("âœ… è³‡æ–™åº«é€£æ¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ è³‡æ–™åº«é€£æ¥å¤±æ•—: {e}")
        FULL_MODE = False
else:
    ES_HOST = None
    ES_AUTH = None
    NEO4J_DRIVER = None
    CHUNK_INDEX = None

# æ¡ˆä»¶é¡å‹å°ç…§è¡¨ï¼ˆESæª¢ç´¢fallbackç”¨ï¼‰
CASE_TYPE_MAP = {
    # ç‰¹æ®Šæ¡ˆå‹å¦‚æœæ‰¾ä¸åˆ°ï¼Œfallbackåˆ°ç›¸é—œåŸºç¤é¡å‹
    "Â§190å‹•ç‰©æ¡ˆå‹": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    "Â§188åƒ±ç”¨äººæ¡ˆå‹": "å–®ç´”åŸè¢«å‘Šå„ä¸€", 
    "Â§187æœªæˆå¹´æ¡ˆå‹": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    
    # è¤‡åˆæ¡ˆå‹çš„fallback
    "åŸè¢«å‘Šçš†æ•¸å+Â§188åƒ±ç”¨äººæ¡ˆå‹": "Â§188åƒ±ç”¨äººæ¡ˆå‹",
    "æ•¸ååŸå‘Š+Â§188åƒ±ç”¨äººæ¡ˆå‹": "Â§188åƒ±ç”¨äººæ¡ˆå‹",
    "æ•¸åè¢«å‘Š+Â§188åƒ±ç”¨äººæ¡ˆå‹": "Â§188åƒ±ç”¨äººæ¡ˆå‹",
    "æ•¸åè¢«å‘Š+Â§187æœªæˆå¹´æ¡ˆå‹": "Â§187æœªæˆå¹´æ¡ˆå‹", 
    "åŸè¢«å‘Šçš†æ•¸å+Â§187æœªæˆå¹´æ¡ˆå‹": "Â§187æœªæˆå¹´æ¡ˆå‹",
    "åŸè¢«å‘Šçš†æ•¸å+Â§190å‹•ç‰©æ¡ˆå‹": "Â§190å‹•ç‰©æ¡ˆå‹",
    
    # åŸºç¤ç•¶äº‹äººæ•¸é‡é¡å‹ï¼ˆé€šå¸¸ä¸éœ€è¦fallbackï¼‰
    "æ•¸ååŸå‘Š": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    "æ•¸åè¢«å‘Š": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    "åŸè¢«å‘Šçš†æ•¸å": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
}

# ===== è¼”åŠ©å‡½æ•¸ =====
def extract_sections(text: str) -> dict:
    """æå–æ–‡æœ¬æ®µè½"""
    result = {
        "accident_facts": "",
        "injuries": "",
        "compensation_facts": ""
    }
    
    # äº‹æ•…ç™¼ç”Ÿç·£ç”±
    fact_match = re.search(r"ä¸€[ã€ï¼.\s]*äº‹æ•…ç™¼ç”Ÿç·£ç”±[:ï¼š]?\s*(.*?)(?=äºŒ[ã€ï¼.]|$)", text, re.S)
    if fact_match:
        result["accident_facts"] = fact_match.group(1).strip()
    
    # å—å‚·æƒ…å½¢
    injury_match = re.search(r"äºŒ[ã€ï¼.\s]*(?:åŸå‘Š)?å—å‚·æƒ…å½¢[:ï¼š]?\s*(.*?)(?=ä¸‰[ã€ï¼.]|$)", text, re.S)
    if injury_match:
        result["injuries"] = injury_match.group(1).strip()
    
    # è³ å„Ÿäº‹å¯¦æ ¹æ“š
    comp_match = re.search(r"ä¸‰[ã€ï¼.\s]*è«‹æ±‚è³ å„Ÿçš„äº‹å¯¦æ ¹æ“š[:ï¼š]?\s*(.*?)$", text, re.S)
    if comp_match:
        result["compensation_facts"] = comp_match.group(1).strip()
    
    return result

def extract_parties_with_llm(text: str) -> dict:
    """ä½¿ç”¨LLMæå–ç•¶äº‹äººï¼ˆæ›´æº–ç¢ºçš„æ–¹æ³•ï¼‰"""
    print("ğŸ¤– ä½¿ç”¨LLMæ™ºèƒ½æå–ç•¶äº‹äºº...")
    
    # å‰µå»ºæ›´ç²¾ç¢ºçš„æç¤ºæ¨¡æ¿
    prompt = f"""è«‹ä½ å¹«æˆ‘å¾ä»¥ä¸‹è»Šç¦æ¡ˆä»¶çš„æ³•å¾‹æ–‡ä»¶ä¸­æå–ä¸¦åˆ—å‡ºæ‰€æœ‰åŸå‘Šå’Œè¢«å‘Šçš„çœŸå¯¦å§“åã€‚

ä»¥ä¸‹æ˜¯æ¡ˆä»¶å…§å®¹ï¼š
{text}

æå–è¦æ±‚ï¼š
1. åƒ…æå–ã€ŒåŸå‘Šâ—‹â—‹â—‹ã€å’Œã€Œè¢«å‘Šâ—‹â—‹â—‹ã€ä¸­æ˜ç¢ºæåˆ°çš„çœŸå¯¦å§“å
2. ä¸è¦æå–ã€Œè¨´å¤–äººã€çš„å§“åï¼Œè¨´å¤–äººä¸æ˜¯ç•¶äº‹äºº
3. å®Œæ•´ä¿ç•™å§“åï¼Œä¸å¯æˆªæ–·ï¼ˆå¦‚ï¼šé„­å‡±ç¥¥ä¸èƒ½å¯«æˆé„­ç¥¥ï¼‰
4. å¦‚æœæ–‡ä¸­æ²’æœ‰æ˜ç¢ºçš„å§“åï¼Œå°±ç›´æ¥å¯«ã€ŒåŸå‘Šã€ã€ã€Œè¢«å‘Šã€
5. å¤šå€‹å§“åç”¨é€—è™Ÿåˆ†éš”

è¼¸å‡ºæ ¼å¼ï¼ˆåªè¼¸å‡ºé€™å…©è¡Œï¼‰ï¼š
åŸå‘Š:å§“å1,å§“å2...
è¢«å‘Š:å§“å1,å§“å2...

ç¯„ä¾‹èªªæ˜ï¼š
- ã€ŒåŸå‘Šå³éº—å¨Ÿã€â†’ åŸå‘Š:å³éº—å¨Ÿ
- ã€Œè¢«å‘Šé„­å‡±ç¥¥ã€â†’ è¢«å‘Š:é„­å‡±ç¥¥  
- ã€Œè¨´å¤–äººé™³æ²³ç”°ã€â†’ ä¸æ˜¯ç•¶äº‹äººï¼Œå¿½ç•¥
- å¦‚æœåªèªªã€ŒåŸå‘Šã€æ²’æœ‰å§“å â†’ åŸå‘Š:åŸå‘Š
- å¦‚æœåªèªªã€Œè¢«å‘Šã€æ²’æœ‰å§“å â†’ è¢«å‘Š:è¢«å‘Š"""

    try:
        # èª¿ç”¨LLM
        response = requests.post(
            LLM_URL,
            json={
                "model": DEFAULT_MODEL,
                "prompt": prompt,
                "stream": False
            },
            timeout=60
        )
        
        if response.status_code == 200:
            llm_result = response.json()["response"].strip()
            print(f"ğŸ¤– LLMæå–çµæœ: {llm_result}")
            return parse_llm_parties_result(llm_result)
        else:
            print(f"âŒ LLMèª¿ç”¨å¤±æ•—: {response.status_code}")
            return extract_parties_fallback(text)
            
    except Exception as e:
        print(f"âŒ LLMæå–ç•°å¸¸: {e}")
        return extract_parties_fallback(text)

def parse_llm_parties_result(llm_result: str) -> dict:
    """è§£æLLMçš„ç•¶äº‹äººæå–çµæœ"""
    result = {"åŸå‘Š": "åŸå‘Š", "è¢«å‘Š": "è¢«å‘Š", "è¢«å‘Šæ•¸é‡": 1, "åŸå‘Šæ•¸é‡": 1}
    
    # æª¢æŸ¥LLMæ˜¯å¦è¿”å›äº†ç„¡æ•ˆçš„å›æ‡‰
    invalid_responses = ["è«‹æä¾›", "ç„¡æ³•æå–", "æ²’æœ‰æä¾›", "ç”±æ–¼æ‚¨æ²’æœ‰"]
    if any(invalid in llm_result for invalid in invalid_responses):
        print("âš ï¸ LLMè¿”å›ç„¡æ•ˆå›æ‡‰ï¼Œä½¿ç”¨fallback")
        return result
    
    lines = llm_result.split('\n')
    
    for line in lines:
        line = line.strip()
        if line.startswith('åŸå‘Š:') or line.startswith('åŸå‘Šï¼š'):
            plaintiff_text = line.split(':', 1)[1].strip() if ':' in line else line.split('ï¼š', 1)[1].strip()
            if plaintiff_text:
                # åˆ†å‰²å¤šå€‹åŸå‘Š
                plaintiffs = [p.strip() for p in plaintiff_text.split(',') if p.strip()]
                result["åŸå‘Š"] = "ã€".join(plaintiffs)
                result["åŸå‘Šæ•¸é‡"] = len(plaintiffs)
        
        elif line.startswith('è¢«å‘Š:') or line.startswith('è¢«å‘Šï¼š'):
            defendant_text = line.split(':', 1)[1].strip() if ':' in line else line.split('ï¼š', 1)[1].strip()
            if defendant_text:
                # åˆ†å‰²å¤šå€‹è¢«å‘Š
                defendants = [d.strip() for d in defendant_text.split(',') if d.strip()]
                result["è¢«å‘Š"] = "ã€".join(defendants)
                result["è¢«å‘Šæ•¸é‡"] = len(defendants)
    
    return result

def extract_parties_fallback(text: str) -> dict:
    """ç•¶LLMæå–å¤±æ•—æ™‚çš„fallbackæ–¹æ³•ï¼ˆç°¡åŒ–ç‰ˆæ­£å‰‡ï¼‰"""
    print("âš ï¸ ä½¿ç”¨fallbackæ–¹æ³•æå–ç•¶äº‹äºº...")
    result = {"åŸå‘Š": "åŸå‘Š", "è¢«å‘Š": "è¢«å‘Š", "è¢«å‘Šæ•¸é‡": 1, "åŸå‘Šæ•¸é‡": 1}
    
    # ç°¡åŒ–çš„æ­£å‰‡è¡¨é”å¼æå–
    plaintiffs = set()
    defendants = set()
    
    # åŸºæœ¬æ¨¡å¼
    plaintiff_patterns = [
        r'åŸå‘Š([\u4e00-\u9fff]{2,4})',
        r'åŸå‘Š([ç”²ä¹™ä¸™ä¸æˆŠ])'
    ]
    
    defendant_patterns = [
        r'è¢«å‘Š([\u4e00-\u9fff]{2,4})',
        r'è¢«å‘Š([ç”²ä¹™ä¸™ä¸æˆŠ])'
    ]
    
    for pattern in plaintiff_patterns:
        matches = re.findall(pattern, text)
        plaintiffs.update(matches)
    
    for pattern in defendant_patterns:
        matches = re.findall(pattern, text)
        defendants.update(matches)
    
    # æ¸…ç†å’Œçµ„åˆçµæœ
    if plaintiffs:
        result["åŸå‘Š"] = "ã€".join(sorted(plaintiffs))
        result["åŸå‘Šæ•¸é‡"] = len(plaintiffs)
    elif "åŸå‘Š" in text:
        result["åŸå‘Š"] = "åŸå‘Š"
    
    if defendants:
        result["è¢«å‘Š"] = "ã€".join(sorted(defendants))
        result["è¢«å‘Šæ•¸é‡"] = len(defendants)
    elif "è¢«å‘Š" in text:
        result["è¢«å‘Š"] = "è¢«å‘Š"
    
    return result

def extract_parties(text: str) -> dict:
    """ä¸»è¦çš„ç•¶äº‹äººæå–å‡½æ•¸ï¼ˆå„ªå…ˆä½¿ç”¨LLMï¼‰"""
    return extract_parties_with_llm(text)

# ===== æª¢ç´¢ç›¸é—œå‡½æ•¸ =====
def embed(text: str):
    """æ–‡å­—å‘é‡åŒ–"""
    if not FULL_MODE:
        return []
    
    t = TOKENIZER(text, truncation=True, padding="max_length", max_length=512, return_tensors="pt")
    t = {k: v.to(device) for k, v in t.items()}
    with torch.no_grad():
        vec = MODEL(**t).last_hidden_state.mean(dim=1).squeeze()
    return vec.cpu().numpy().tolist()

def es_search(query_vector, case_type: str, top_k: int = 3, label: str = "Facts", quiet: bool = False):
    """ES æœå°‹ï¼ˆå«fallbackæ©Ÿåˆ¶ï¼‰"""
    if not FULL_MODE or not ES_HOST:
        return []
    
    def _search(label_filter, case_type_filter):
        must_clause = [{"match": {"label": label_filter}}]
        
        if case_type_filter:
            # å˜—è©¦å¤šç¨®å¯èƒ½çš„ case_type æ¬„ä½æ ¼å¼
            case_type_options = [
                {"term": {"case_type.keyword": case_type_filter}},
                {"term": {"case_type": case_type_filter}},
                {"match": {"case_type": case_type_filter}}
            ]
            
            # ä½¿ç”¨ should æŸ¥è©¢ï¼Œä»»ä¸€ç¬¦åˆå³å¯
            must_clause.append({
                "bool": {
                    "should": case_type_options,
                    "minimum_should_match": 1
                }
            })
            
        body = {
            "size": top_k,
            "query": {
                "script_score": {
                    "query": {"bool": {"must": must_clause}},
                    "script": {
                        "source": "cosineSimilarity(params.qv,'embedding')+1.0",
                        "params": {"qv": query_vector},
                    },
                }
            },
        }
        
        # èª¿è©¦ä¿¡æ¯ï¼šåªåœ¨éå®‰éœæ¨¡å¼ä¸‹è¼¸å‡º
        if not quiet:
            print(f"ğŸ” ESæŸ¥è©¢æ¢ä»¶: index={CHUNK_INDEX}, label={label_filter}, case_type={case_type_filter}")
        
        try:
            url = f"{ES_HOST}/{CHUNK_INDEX}/_search"
            response = requests.post(url, auth=ES_AUTH, json=body, verify=False)
            if response.status_code == 200:
                result = response.json()
                hits = result["hits"]["hits"]
                total_docs = result["hits"]["total"]["value"] if isinstance(result["hits"]["total"], dict) else result["hits"]["total"]
                if not quiet:
                    print(f"ğŸ“Š ESæŸ¥è©¢çµæœ: æ‰¾åˆ° {len(hits)} å€‹åŒ¹é…çµæœï¼Œç¸½æ–‡æª”æ•¸: {total_docs}")
                return hits
            else:
                if not quiet:
                    print(f"âŒ ESæŸ¥è©¢å¤±æ•—: {response.status_code} - {response.text}")
                return []
        except Exception as e:
            if not quiet:
                print(f"âŒ ESæŸ¥è©¢å¤±æ•—: {e}")
            return []

    if not quiet:
        print(f"ğŸ” ä½¿ç”¨ case_type='{case_type}' æœç´¢ç›¸ä¼¼æ¡ˆä¾‹...")
    hits = _search(label, case_type)
    
    if not hits:
        # å…ˆæª¢æŸ¥ç´¢å¼•æ˜ å°„å’Œå¯ç”¨çš„æ¡ˆä»¶é¡å‹
        try:
            # æª¢æŸ¥ mapping
            mapping_url = f"{ES_HOST}/{CHUNK_INDEX}/_mapping"
            mapping_response = requests.get(mapping_url, auth=ES_AUTH, verify=False)
            if mapping_response.status_code == 200:
                mapping = mapping_response.json()
                properties = mapping[CHUNK_INDEX]["mappings"]["properties"]
                has_case_type = "case_type" in properties
                print(f"ğŸ—ºï¸ case_typeæ¬„ä½å­˜åœ¨: {has_case_type}")
                
                if has_case_type:
                    # å˜—è©¦ä¸åŒçš„æ¬„ä½åç¨±
                    for field_name in ["case_type", "case_type.keyword"]:
                        try:
                            check_body = {
                                "size": 0,
                                "aggs": {
                                    "case_type_count": {"terms": {"field": field_name, "size": 20}}
                                }
                            }
                            check_url = f"{ES_HOST}/{CHUNK_INDEX}/_search"
                            check_response = requests.post(check_url, auth=ES_AUTH, json=check_body, verify=False)
                            if check_response.status_code == 200:
                                check_result = check_response.json()
                                available_types = [bucket["key"] for bucket in check_result["aggregations"]["case_type_count"]["buckets"]]
                                print(f"ğŸ“‹ ä½¿ç”¨æ¬„ä½ {field_name} æ‰¾åˆ°çš„æ¡ˆä»¶é¡å‹: {available_types}")
                                break
                        except Exception as field_e:
                            print(f"âš ï¸ æ¬„ä½ {field_name} æŸ¥è©¢å¤±æ•—: {field_e}")
                else:
                    print("âŒ case_typeæ¬„ä½ä¸å­˜åœ¨æ–¼ç´¢å¼•æ˜ å°„ä¸­")
            else:
                print(f"âŒ ç²å–æ˜ å°„å¤±æ•—: {mapping_response.status_code}")
                
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•æª¢æŸ¥ç´¢å¼•æ˜ å°„æˆ–æ¡ˆä»¶é¡å‹: {e}")
        
        fallback = CASE_TYPE_MAP.get(case_type, "å–®ç´”åŸè¢«å‘Šå„ä¸€")
        if fallback != case_type:
            print(f"âš ï¸ ä½¿ç”¨ fallback='{fallback}' é‡æ–°æœå°‹...")
            hits = _search(label, fallback)
    
    if not hits:
        print("âš ï¸ ä¸é™æ¡ˆä»¶é¡å‹é€²è¡Œæœå°‹...")
        hits = _search(label, None)
    
    return hits

def rerank_case_ids_by_paragraphs(query_text: str, case_ids: List[str], label: str = "Facts", quiet: bool = False) -> List[str]:
    """æ ¹æ“šæ®µè½ç´šè³‡æ–™é‡æ–°æ’åºæ¡ˆä¾‹"""
    if not FULL_MODE or not ES_HOST:
        return case_ids
    
    if not quiet:
        print("ğŸ“˜ å•Ÿå‹•æ®µè½ç´š rerank...")
    try:
        from sklearn.metrics.pairwise import cosine_similarity
        import numpy as np
    except ImportError:
        if not quiet:
            print("âš ï¸ sklearnæœªå®‰è£ï¼Œè·³érerank")
        return case_ids

    query_vec = embed(query_text)
    if not query_vec:
        return case_ids
    
    query_vec_np = np.array(query_vec).reshape(1, -1)
    scored_cases = []
    
    for cid in case_ids:
        try:
            # ä½¿ç”¨ requests ç›´æ¥èª¿ç”¨ ES API
            search_body = {
                "query": {
                    "bool": {
                        "must": [
                            {"term": {"case_id": cid}},
                            {"term": {"label": label}}
                        ]
                    }
                },
                "size": 1
            }
            url = f"{ES_HOST}/legal_kg_paragraphs/_search"
            response = requests.post(url, auth=ES_AUTH, json=search_body, verify=False)
            if response.status_code != 200:
                continue
            res = response.json()
            hits = res.get("hits", {}).get("hits", [])
            if hits:
                vec = hits[0]['_source']['embedding']
                para_vec_np = np.array(vec).reshape(1, -1)
                score = cosine_similarity(query_vec_np, para_vec_np)[0][0]
                scored_cases.append((cid, score))
        except Exception as e:
            print(f"âš ï¸ Case {cid} rerankå¤±æ•—: {e}")
            scored_cases.append((cid, 0.0))

    scored_cases.sort(key=lambda x: x[1], reverse=True)
    return [cid for cid, _ in scored_cases]

def get_complete_cases_content(case_ids: List[str]) -> List[str]:
    """ç²å–å®Œæ•´æ¡ˆä¾‹å…§å®¹"""
    if not FULL_MODE or not NEO4J_DRIVER:
        return []
    
    complete_cases = []
    
    try:
        with NEO4J_DRIVER.session() as session:
            for case_id in case_ids:
                # æŸ¥è©¢å®Œæ•´æ¡ˆä¾‹çš„äº‹å¯¦æ®µè½
                result = session.run("""
                    MATCH (c:Case {case_id: $case_id})-[:åŒ…å«]->(f:Facts)
                    RETURN f.content AS facts_content
                    ORDER BY f.order_index
                """, case_id=case_id).data()
                
                if result:
                    # çµ„åˆæ¡ˆä¾‹çš„æ‰€æœ‰äº‹å¯¦æ®µè½
                    case_content = "\n".join([record["facts_content"] for record in result if record["facts_content"]])
                    if case_content:
                        complete_cases.append(case_content)
                else:
                    print(f"âš ï¸ æ¡ˆä¾‹ {case_id} ç„¡æ³•ç²å–å®Œæ•´å…§å®¹")
                    
    except Exception as e:
        print(f"âš ï¸ ç²å–å®Œæ•´æ¡ˆä¾‹å…§å®¹å¤±æ•—: {e}")
    
    return complete_cases

def query_laws(case_ids):
    """å¾Neo4jæŸ¥è©¢æ³•æ¢è³‡è¨Š"""
    if not FULL_MODE or not NEO4J_DRIVER:
        return Counter(), {}
    
    counter = Counter()
    law_text_map = {}
    
    try:
        with NEO4J_DRIVER.session() as session:
            for cid in case_ids:
                result = session.run("""
                    MATCH (c:Case {case_id: $cid})-[:åŒ…å«]->(:Facts)-[:é©ç”¨]->(l:Laws)-[:åŒ…å«]->(ld:LawDetail)
                    RETURN collect(distinct ld.name) AS law_names, collect(distinct ld.text) AS law_texts
                """, cid=cid).single()
                if result:
                    names = result["law_names"]
                    texts = result["law_texts"]
                    counter.update(names)
                    for n, t in zip(names, texts):
                        if n not in law_text_map:
                            law_text_map[n] = t
    except Exception as e:
        print(f"âš ï¸ Neo4jæŸ¥è©¢å¤±æ•—: {e}")
    
    return counter, law_text_map

def normalize_article_number(article: str) -> str:
    """æ¢è™Ÿæ ¼å¼æ¨™æº–åŒ–ï¼šç¬¬191-2æ¢ â†’ ç¬¬191æ¢ä¹‹2"""
    # è™•ç†ç‰¹æ®Šæ ¼å¼çš„æ¢è™Ÿ
    article = re.sub(r'ç¬¬(\d+)-(\d+)æ¢', r'ç¬¬\1æ¢ä¹‹\2', article)
    return article

def detect_special_relationships(text: str, parties: dict) -> dict:
    """åµæ¸¬ç‰¹æ®Šæ³•å¾‹é—œä¿‚ï¼ˆå„ªåŒ–ç‰ˆï¼‰"""
    relationships = {
        "æœªæˆå¹´": False,
        "é›‡å‚­é—œä¿‚": False,
        "å‹•ç‰©æå®³": False,
        "å¤šè¢«å‘Š": parties.get('è¢«å‘Šæ•¸é‡', 1) > 1,
        "å¤šåŸå‘Š": parties.get('åŸå‘Šæ•¸é‡', 1) > 1   # æ–°å¢å¤šåŸå‘Šåˆ¤æ–·
    }
    
    # æ›´ç²¾ç¢ºçš„æœªæˆå¹´æª¢æ¸¬
    # 1. æ˜ç¢ºæåˆ°æœªæˆå¹´ç›¸é—œè©å½™
    explicit_minor_keywords = ["æœªæˆå¹´", "æ³•å®šä»£ç†äºº", "ç›£è­·äºº", "æœªæ»¿åå…«æ­²", "æœªæ»¿18æ­²"]
    if any(keyword in text for keyword in explicit_minor_keywords):
        relationships["æœªæˆå¹´"] = True
    
    # 2. æª¢æŸ¥å…·é«”å¹´é½¡ï¼ˆ18æ­²ä»¥ä¸‹ï¼‰
    age_pattern = r'(\d+)\s*æ­²'
    age_matches = re.findall(age_pattern, text)
    for age_str in age_matches:
        age = int(age_str)
        if age < 18:
            relationships["æœªæˆå¹´"] = True
            break
    
    # 3. å­¸æ ¡é—œéµå­—éœ€è¦æ›´è¬¹æ…
    school_keywords = ["åœ‹ä¸­ç”Ÿ", "åœ‹å°ç”Ÿ", "é«˜ä¸­ç”Ÿ"]  # ä¸æ˜¯å–®ç´”çš„"åœ‹ä¸­"ã€"é«˜ä¸­"
    if any(keyword in text for keyword in school_keywords):
        relationships["æœªæˆå¹´"] = True
    
    # æª¢æŸ¥é›‡å‚­é—œä¿‚
    employment_keywords = ["å—åƒ±", "åƒ±ç”¨", "é›‡ä¸»", "å“¡å·¥", "è·å‹™", "å·¥ä½œæ™‚é–“", "å…¬å¸è»Š", "åŸ·è¡Œè·å‹™"]
    relationships["é›‡å‚­é—œä¿‚"] = any(keyword in text for keyword in employment_keywords)
    
    # æª¢æŸ¥å‹•ç‰©æå®³
    animal_keywords = ["ç‹—", "è²“", "çŠ¬", "å‹•ç‰©", "å¯µç‰©", "å’¬å‚·", "æŠ“å‚·"]
    relationships["å‹•ç‰©æå®³"] = any(keyword in text for keyword in animal_keywords)
    
    return relationships

def determine_case_type(accident_facts: str, parties: dict) -> str:
    """åˆ¤æ–·æ¡ˆä»¶é¡å‹ï¼ˆä¸ƒå¤§åŸºæœ¬é¡å‹ï¼‰"""
    relationships = detect_special_relationships(accident_facts, parties)
    
    # å„ªå…ˆåˆ¤æ–·ç‰¹æ®Šæ³•æ¢é¡å‹ï¼ˆäº’æ–¥å„ªå…ˆç´šï¼‰
    if relationships["å‹•ç‰©æå®³"]:
        return "Â§190å‹•ç‰©æ¡ˆå‹"
    elif relationships["é›‡å‚­é—œä¿‚"]: 
        return "Â§188åƒ±ç”¨äººæ¡ˆå‹"
    elif relationships["æœªæˆå¹´"]:
        return "Â§187æœªæˆå¹´æ¡ˆå‹"
    
    # å…¶æ¬¡åˆ¤æ–·ç•¶äº‹äººæ•¸é‡é¡å‹
    elif relationships["å¤šåŸå‘Š"] and relationships["å¤šè¢«å‘Š"]:
        return "åŸè¢«å‘Šçš†æ•¸å"
    elif relationships["å¤šåŸå‘Š"]:
        return "æ•¸ååŸå‘Š"
    elif relationships["å¤šè¢«å‘Š"]:
        return "æ•¸åè¢«å‘Š"
    else:
        return "å–®ç´”åŸè¢«å‘Šå„ä¸€"

def determine_applicable_laws(accident_facts: str, injuries: str, comp_facts: str, parties: dict) -> List[str]:
    """æ ¹æ“šæ¡ˆä»¶äº‹å¯¦æ™ºèƒ½åˆ¤æ–·é©ç”¨æ³•æ¢"""
    applicable_laws = []
    
    # åµæ¸¬ç‰¹æ®Šé—œä¿‚
    relationships = detect_special_relationships(accident_facts + injuries + comp_facts, parties)
    
    # 1. ç¬¬184æ¢ç¬¬1é …å‰æ®µ - åŸºæœ¬ä¾µæ¬Šè²¬ä»»ï¼ˆå¿…é ˆï¼‰
    applicable_laws.append("æ°‘æ³•ç¬¬184æ¢ç¬¬1é …å‰æ®µ")
    
    # 2. è»Šç¦æ¡ˆä»¶ - ç¬¬191æ¢ä¹‹2ï¼ˆäº¤é€šå·¥å…·ï¼‰
    traffic_keywords = ["æ±½è»Š", "æ©Ÿè»Š", "è»Šè¼›", "é§•é§›", "äº¤é€š", "æ’", "ç¢°æ’"]
    if any(keyword in accident_facts for keyword in traffic_keywords):
        applicable_laws.append("æ°‘æ³•ç¬¬191æ¢ä¹‹2")
    
    # 3. èº«é«”å¥åº·æå®³ - ç¬¬193æ¢ç¬¬1é …
    health_damage_keywords = ["é†«ç™‚", "çœ‹è­·", "å·¥ä½œæå¤±", "è–ªè³‡", "æ”¶å…¥", "å‹å‹•èƒ½åŠ›"]
    if injuries or any(keyword in comp_facts for keyword in health_damage_keywords):
        applicable_laws.append("æ°‘æ³•ç¬¬193æ¢ç¬¬1é …")
    
    # 4. ç²¾ç¥æ…°æ’«é‡‘ - ç¬¬195æ¢ç¬¬1é …å‰æ®µ
    mental_damage_keywords = ["ç²¾ç¥", "æ…°æ’«", "ç—›è‹¦", "åè­½", "äººæ ¼"]
    if any(keyword in comp_facts for keyword in mental_damage_keywords):
        applicable_laws.append("æ°‘æ³•ç¬¬195æ¢ç¬¬1é …å‰æ®µ")
    
    # 5. ç‰¹æ®Šæƒ…æ³è™•ç†ï¼ˆäº’æ–¥è¦å‰‡ï¼‰
    if relationships["é›‡å‚­é—œä¿‚"]:
        # é›‡å‚­é—œä¿‚å„ªå…ˆé©ç”¨ç¬¬188æ¢ç¬¬1é …æœ¬æ–‡
        applicable_laws.append("æ°‘æ³•ç¬¬188æ¢ç¬¬1é …æœ¬æ–‡")
    elif relationships["å¤šè¢«å‘Š"]:
        # å¤šè¢«å‘Šä½†ç„¡é›‡å‚­é—œä¿‚æ™‚é©ç”¨ç¬¬185æ¢ç¬¬1é …
        applicable_laws.append("æ°‘æ³•ç¬¬185æ¢ç¬¬1é …")
    
    # 6. æœªæˆå¹´æ¡ˆä»¶ - ç¬¬187æ¢ç¬¬1é …
    if relationships["æœªæˆå¹´"]:
        applicable_laws.append("æ°‘æ³•ç¬¬187æ¢ç¬¬1é …")
    
    # 7. å‹•ç‰©æå®³ - ç¬¬190æ¢ç¬¬1é …
    if relationships["å‹•ç‰©æå®³"]:
        applicable_laws.append("æ°‘æ³•ç¬¬190æ¢ç¬¬1é …")
    
    # æ¨™æº–åŒ–æ¢è™Ÿæ ¼å¼
    applicable_laws = [normalize_article_number(law) for law in applicable_laws]
    
    return list(dict.fromkeys(applicable_laws))  # å»é‡ä½†ä¿æŒé †åº


    

# ===== æ··åˆæ¨¡å¼ç”Ÿæˆå™¨ =====
class HybridCoTGenerator:
    """æ··åˆæ¨¡å¼ç”Ÿæˆå™¨ï¼šäº‹å¯¦æ³•æ¢æå®³ç”¨æ¨™æº–ï¼Œçµè«–ç”¨CoT"""
    
    def __init__(self, model_name: str = DEFAULT_MODEL):
        self.model_name = model_name
        self.llm_url = LLM_URL
        
        # åˆå§‹åŒ–é‡‘é¡è™•ç†å™¨
        if STRUCTURED_PROCESSOR_AVAILABLE:
            self.structured_processor = StructuredLegalAmountProcessor()
        else:
            self.structured_processor = None
        
        if BASIC_STANDARDIZER_AVAILABLE:
            self.basic_standardizer = LegalAmountStandardizer()
        else:
            self.basic_standardizer = None
        
        # æª¢æŸ¥LLMé€£æ¥
        self.llm_available = self._check_llm_connection()
    
    def _check_llm_connection(self) -> bool:
        """æª¢æŸ¥LLMé€£æ¥"""
        try:
            response = requests.get("http://localhost:11434/api/version", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def call_llm(self, prompt: str, timeout: int = 180) -> str:
        """èª¿ç”¨LLM"""
        if not self.llm_available:
            return "âŒ LLMæœå‹™ä¸å¯ç”¨"
        
        try:
            response = requests.post(
                self.llm_url,
                json={
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False
                },
                timeout=timeout
            )
            
            if response.status_code == 200:
                return response.json()["response"].strip()
            else:
                return f"âŒ LLM APIéŒ¯èª¤: {response.status_code}"
                
        except Exception as e:
            return f"âŒ LLMèª¿ç”¨å¤±æ•—: {str(e)}"
    
    def _chinese_num(self, num: int) -> str:
        """æ•¸å­—è½‰ä¸­æ–‡"""
        chinese = ["", "ä¸€", "äºŒ", "ä¸‰", "å››", "äº”", "å…­", "ä¸ƒ", "å…«", "ä¹", "å"]
        if num <= 10:
            return chinese[num]
        return str(num)
    
    def _extract_all_plaintiffs(self, text: str) -> List[str]:
        """æå–æ‰€æœ‰åŸå‘Šå§“å"""
        plaintiffs = []
        
        # æ–¹æ³•1ï¼šå¾æ–‡æœ¬ä¸­æ‰¾ã€ŒåŸå‘Šâ—‹â—‹â—‹ã€çš„æ¨¡å¼
        pattern = r'åŸå‘Š([^ï¼Œã€‚ï¼›ã€\s]{2,4})(?:ç‚º|å› |å—|å‰å¾€|æ”¯å‡º)'
        matches = re.findall(pattern, text)
        plaintiffs.extend(matches)
        
        # å»é‡ä¸¦ä¿æŒé †åº
        seen = set()
        unique_plaintiffs = []
        for p in plaintiffs:
            if p not in seen:
                seen.add(p)
                unique_plaintiffs.append(p)
        
        return unique_plaintiffs
    
    def generate_standard_facts(self, accident_facts: str, similar_cases: List[str] = None) -> str:
        """æ¨™æº–æ–¹å¼ç”Ÿæˆäº‹å¯¦æ®µè½ï¼ˆå«ç›¸ä¼¼æ¡ˆä¾‹åƒè€ƒï¼‰"""
        print("ğŸ“ ä½¿ç”¨æ¨™æº–æ–¹å¼ç”Ÿæˆäº‹å¯¦æ®µè½...")
        
        # çµ„åˆç›¸ä¼¼æ¡ˆä¾‹åƒè€ƒ
        reference_text = ""
        if similar_cases:
            reference_text = "\n\nåƒè€ƒç›¸ä¼¼æ¡ˆä¾‹ï¼š\n" + "\n".join([f"{i+1}. {case}" for i, case in enumerate(similar_cases[:2])])
        
        prompt = f"""ä½ æ˜¯å°ç£å¾‹å¸«ï¼Œè«‹æ ¹æ“šä»¥ä¸‹äº‹å¯¦ææ–™æ’°å¯«èµ·è¨´ç‹€çš„äº‹å¯¦æ®µè½ï¼š

äº‹å¯¦ææ–™ï¼š
{accident_facts}{reference_text}

è¦æ±‚ï¼š
1. ä»¥ã€Œç·£è¢«å‘Šã€é–‹é ­
2. ä½¿ç”¨ã€ŒåŸå‘Šã€ã€ã€Œè¢«å‘Šã€ç¨±è¬‚ï¼Œä½†å¿…é ˆä¿æŒå§“åçš„å®Œæ•´æ€§å’Œæº–ç¢ºæ€§
3. å®¢è§€æè¿°äº‹æ•…ç¶“é
4. åƒè€ƒç›¸ä¼¼æ¡ˆä¾‹çš„æ•˜è¿°æ–¹å¼ï¼Œä½†ä¸å¾—æŠ„è¥²
5. æ ¼å¼ï¼šä¸€ã€äº‹å¯¦æ¦‚è¿°ï¼š[å…§å®¹]
6. **é‡è¦**ï¼šå¦‚æœäº‹å¯¦ææ–™ä¸­æœ‰å…·é«”å§“åï¼Œè«‹å®Œæ•´ä¿ç•™ï¼Œä¸è¦æˆªæ–·æˆ–æ”¹è®Šä»»ä½•å­—å…ƒ

è«‹ç›´æ¥è¼¸å‡ºäº‹å¯¦æ®µè½ï¼š"""
        
        result = self.call_llm(prompt)
        
        # æå–äº‹å¯¦æ®µè½
        fact_match = re.search(r"ä¸€ã€äº‹å¯¦æ¦‚è¿°ï¼š\s*(.*?)(?:\n\n|$)", result, re.S)
        if fact_match:
            return f"ä¸€ã€äº‹å¯¦æ¦‚è¿°ï¼š\n{fact_match.group(1).strip()}"
        elif "ç·£è¢«å‘Š" in result:
            # æ‰¾åˆ°åŒ…å«"ç·£è¢«å‘Š"çš„è¡Œ
            for line in result.split('\n'):
                if "ç·£è¢«å‘Š" in line:
                    return f"ä¸€ã€äº‹å¯¦æ¦‚è¿°ï¼š\n{line.strip()}"
        
        # Fallback
        facts_content = accident_facts.replace('ç·£è¢«å‘Š', '').strip()
        return f"ä¸€ã€äº‹å¯¦æ¦‚è¿°ï¼š\nç·£è¢«å‘Š{facts_content}"
    
    def generate_standard_laws(self, accident_facts: str, injuries: str, parties: dict, compensation_facts: str = "") -> str:
        """æ¨™æº–æ–¹å¼ç”Ÿæˆæ³•å¾‹ä¾æ“šï¼ˆç¬¦åˆæ³•æ¢å¼•ç”¨è¦ç¯„ï¼‰"""
        print("âš–ï¸ ä½¿ç”¨æ¨™æº–æ–¹å¼ç”Ÿæˆæ³•å¾‹ä¾æ“š...")
        
        # æ™ºèƒ½åˆ¤æ–·é©ç”¨æ³•æ¢
        applicable_laws = determine_applicable_laws(accident_facts, injuries, compensation_facts, parties)
        
        # å®Œæ•´çš„æ³•æ¢èªªæ˜å°ç…§è¡¨ï¼ˆç²¾ç¢ºåˆ°é …ã€æ®µã€ä½†æ›¸ï¼‰
        law_descriptions = {
            "æ°‘æ³•ç¬¬184æ¢ç¬¬1é …å‰æ®µ": "å› æ•…æ„æˆ–éå¤±ï¼Œä¸æ³•ä¾µå®³ä»–äººä¹‹æ¬Šåˆ©è€…ï¼Œè² æå®³è³ å„Ÿè²¬ä»»ã€‚",
            "æ°‘æ³•ç¬¬185æ¢ç¬¬1é …": "æ•¸äººå…±åŒä¸æ³•ä¾µå®³ä»–äººä¹‹æ¬Šåˆ©è€…ï¼Œé€£å¸¶è² æå®³è³ å„Ÿè²¬ä»»ã€‚",
            "æ°‘æ³•ç¬¬187æ¢ç¬¬1é …": "ç„¡è¡Œç‚ºèƒ½åŠ›äººæˆ–é™åˆ¶è¡Œç‚ºèƒ½åŠ›äººï¼Œä¸æ³•ä¾µå®³ä»–äººä¹‹æ¬Šåˆ©è€…ï¼Œä»¥è¡Œç‚ºæ™‚æœ‰è­˜åˆ¥èƒ½åŠ›ç‚ºé™ï¼Œèˆ‡å…¶æ³•å®šä»£ç†äººé€£å¸¶è² æå®³è³ å„Ÿè²¬ä»»ã€‚",
            "æ°‘æ³•ç¬¬188æ¢ç¬¬1é …æœ¬æ–‡": "å—åƒ±äººå› åŸ·è¡Œè·å‹™ï¼Œä¸æ³•ä¾µå®³ä»–äººä¹‹æ¬Šåˆ©è€…ï¼Œç”±åƒ±ç”¨äººèˆ‡è¡Œç‚ºäººé€£å¸¶è² æå®³è³ å„Ÿè²¬ä»»ã€‚",
            "æ°‘æ³•ç¬¬190æ¢ç¬¬1é …": "å‹•ç‰©åŠ æå®³æ–¼ä»–äººè€…ï¼Œç”±å…¶å æœ‰äººè² æå®³è³ å„Ÿè²¬ä»»ã€‚",
            "æ°‘æ³•ç¬¬191æ¢ä¹‹2": "æ±½è»Šã€æ©Ÿè»Šæˆ–å…¶ä»–éä¾è»Œé“è¡Œé§›ä¹‹å‹•åŠ›è»Šè¼›ï¼Œåœ¨ä½¿ç”¨ä¸­åŠ æå®³æ–¼ä»–äººè€…ï¼Œé§•é§›äººæ‡‰è³ å„Ÿå› æ­¤æ‰€ç”Ÿä¹‹æå®³ã€‚",
            "æ°‘æ³•ç¬¬193æ¢ç¬¬1é …": "ä¸æ³•ä¾µå®³ä»–äººä¹‹èº«é«”æˆ–å¥åº·è€…ï¼Œå°æ–¼è¢«å®³äººå› æ­¤å–ªå¤±æˆ–æ¸›å°‘å‹å‹•èƒ½åŠ›æˆ–å¢åŠ ç”Ÿæ´»ä¸Šä¹‹éœ€è¦æ™‚ï¼Œæ‡‰è² æå®³è³ å„Ÿè²¬ä»»ã€‚",
            "æ°‘æ³•ç¬¬195æ¢ç¬¬1é …å‰æ®µ": "ä¸æ³•ä¾µå®³ä»–äººä¹‹èº«é«”ã€å¥åº·ã€åè­½ã€è‡ªç”±ã€ä¿¡ç”¨ã€éš±ç§ã€è²æ“ï¼Œæˆ–ä¸æ³•ä¾µå®³å…¶ä»–äººæ ¼æ³•ç›Šè€Œæƒ…ç¯€é‡å¤§è€…ï¼Œè¢«å®³äººé›–éè²¡ç”¢ä¸Šä¹‹æå®³ï¼Œäº¦å¾—è«‹æ±‚è³ å„Ÿç›¸ç•¶ä¹‹é‡‘é¡ã€‚"
        }
        
        # çµ„åˆæ³•æ¢å…§å®¹ï¼ˆå…ˆåˆ—æ³•æ¢å…§å®¹ï¼‰
        law_texts = []
        valid_laws = []
        
        for law in applicable_laws:
            if law in law_descriptions:
                law_texts.append(f"ã€Œ{law_descriptions[law]}ã€")
                valid_laws.append(law)
        
        if not law_texts:
            # Fallbackï¼šè‡³å°‘åŒ…å«åŸºæœ¬ä¾µæ¬Šæ¢æ–‡
            law_texts = ["ã€Œå› æ•…æ„æˆ–éå¤±ï¼Œä¸æ³•ä¾µå®³ä»–äººä¹‹æ¬Šåˆ©è€…ï¼Œè² æå®³è³ å„Ÿè²¬ä»»ã€‚ã€"]
            valid_laws = ["æ°‘æ³•ç¬¬184æ¢ç¬¬1é …å‰æ®µ"]
        
        # æŒ‰æ­£ç¢ºæ ¼å¼çµ„åˆï¼šå…ˆæ³•æ¢å…§å®¹ï¼Œå¾Œæ¢è™Ÿ
        law_content_block = "ã€".join(law_texts)
        article_list = "ã€".join(valid_laws)
        
        print(f"âœ… é©ç”¨æ³•æ¢: {', '.join(valid_laws)}")
        
        return f"""äºŒã€æ³•å¾‹ä¾æ“šï¼š
æŒ‰{law_content_block}ï¼Œ{article_list}åˆ†åˆ¥å®šæœ‰æ˜æ–‡ã€‚æŸ¥è¢«å‘Šå› ä¸Šé–‹ä¾µæ¬Šè¡Œç‚ºï¼Œè‡´åŸå‘Šå—æœ‰ä¸‹åˆ—æå®³ï¼Œä¾å‰æ­è¦å®šï¼Œè¢«å‘Šæ‡‰è² æå®³è³ å„Ÿè²¬ä»»ï¼š"""
    
    def _parse_damage_from_sentence(self, sentence: str, plaintiff: str) -> List[dict]:
        """å¾å¥å­ä¸­è§£ææå®³é …ç›®ï¼ˆæ”¹é€²ç‰ˆï¼‰"""
        damages = []
        
        # æ“´å……æå®³é¡å‹æ¨¡å¼
        damage_patterns = [
            {
                'keywords': ['é†«ç™‚è²»ç”¨', 'æ²»ç™‚', 'é†«é™¢', 'è¨ºæ‰€', 'å°±è¨º'],
                'name': 'é†«ç™‚è²»ç”¨',
                'template': f'åŸå‘Š{plaintiff}å› æœ¬æ¬¡äº‹æ•…å—å‚·ï¼Œå‰å¾€é†«é™¢æ²»ç™‚æ‰€æ”¯å‡ºä¹‹é†«ç™‚è²»ç”¨'
            },
            {
                'keywords': ['äº¤é€šè²»'],
                'name': 'äº¤é€šè²»ç”¨',
                'template': f'åŸå‘Š{plaintiff}å› æœ¬æ¬¡äº‹æ•…æ‰€ç”Ÿä¹‹äº¤é€šè²»ç”¨'
            },
            {
                'keywords': ['å·¥è³‡æå¤±', 'ä¸èƒ½å·¥ä½œ', 'å·¥ä½œæå¤±', 'ç„¡æ³•å·¥ä½œ', 'ä¼‘é¤Š'],
                'name': 'å·¥ä½œæå¤±',
                'template': f'åŸå‘Š{plaintiff}å› æœ¬æ¬¡äº‹æ•…ç„¡æ³•å·¥ä½œä¹‹æ”¶å…¥æå¤±'
            },
            {
                'keywords': ['æ…°æ’«é‡‘', 'ç²¾ç¥'],
                'name': 'ç²¾ç¥æ…°æ’«é‡‘',
                'template': f'åŸå‘Š{plaintiff}å› æœ¬æ¬¡äº‹æ•…æ‰€å—ç²¾ç¥ä¸Šç—›è‹¦ä¹‹æ…°æ’«é‡‘'
            },
            {
                'keywords': ['è»Šè¼›è²¶å€¼', 'è²¶æ', 'åƒ¹å€¼æ¸›æ', 'äº¤æ˜“åƒ¹å€¼'],
                'name': 'è»Šè¼›è²¶å€¼æå¤±',
                'template': f'ç³»çˆ­è»Šè¼›å› æœ¬æ¬¡äº‹æ•…è²¶å€¼ä¹‹æå¤±'
            },
            {
                'keywords': ['é‘‘å®šè²»'],
                'name': 'é‘‘å®šè²»ç”¨',
                'template': f'ç‚ºè©•ä¼°è»Šè¼›æå¤±æ‰€æ”¯å‡ºä¹‹é‘‘å®šè²»ç”¨'
            }
        ]
        
        # æ›´ç²¾ç¢ºçš„é‡‘é¡æå–ï¼ˆè€ƒæ…®å‰å¾Œæ–‡ï¼‰
        # ä¸è¦åªçœ‹åˆ°é‡‘é¡å°±æå–ï¼Œè¦çœ‹é‡‘é¡å‰å¾Œçš„æè¿°
        
        return damages
    
    def generate_smart_compensation(self, injuries: str, comp_facts: str, parties: dict) -> str:
        """æ™ºèƒ½ç”Ÿæˆæå®³é …ç›®ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰"""
        print("ğŸ’° ç”Ÿæˆæå®³è³ å„Ÿ...")
        
        # ç›´æ¥ä½¿ç”¨LLMè™•ç†ï¼Œé¿å…è¤‡é›œçš„çµæ§‹åŒ–è™•ç†
        return self._generate_llm_based_compensation(comp_facts, parties)

    def _preprocess_chinese_numbers(self, text: str) -> str:
        """é è™•ç†ä¸­æ–‡æ•¸å­—ï¼Œè½‰æ›ç‚ºé˜¿æ‹‰ä¼¯æ•¸å­—"""
        import re
        
        # è™•ç† Xè¬Y,YYYå…ƒ æ ¼å¼ (å¦‚ï¼š26è¬4,379å…ƒ)
        pattern1 = r'(\d+)è¬(\d+,?\d+)å…ƒ'
        def replace1(match):
            wan = int(match.group(1))
            rest = int(match.group(2).replace(',', ''))
            total = wan * 10000 + rest
            return f"{total:,}å…ƒ"
        text = re.sub(pattern1, replace1, text)
        
        # è™•ç† Xè¬Yåƒå…ƒ æ ¼å¼ (å¦‚ï¼š30è¬5åƒå…ƒ)
        pattern2 = r'(\d+)è¬(\d+)åƒå…ƒ'
        def replace2(match):
            wan = int(match.group(1))
            qian = int(match.group(2))
            total = wan * 10000 + qian * 1000
            return f"{total:,}å…ƒ"
        text = re.sub(pattern2, replace2, text)
        
        # è™•ç† Xè¬å…ƒ æ ¼å¼ (å¦‚ï¼š20è¬å…ƒ)
        pattern3 = r'(\d+)è¬å…ƒ'
        def replace3(match):
            wan = int(match.group(1))
            total = wan * 10000
            return f"{total:,}å…ƒ"
        text = re.sub(pattern3, replace3, text)
        
        # è™•ç† Xåƒå…ƒ æ ¼å¼ (å¦‚ï¼š5åƒå…ƒ)
        pattern4 = r'(\d+)åƒå…ƒ'
        def replace4(match):
            qian = int(match.group(1))
            total = qian * 1000
            return f"{total:,}å…ƒ"
        text = re.sub(pattern4, replace4, text)
        
        return text
    
    def _detect_structure_type(self, text: str) -> str:
        """æª¢æ¸¬æ–‡æœ¬çµæ§‹é¡å‹"""
        # æª¢æŸ¥çµæ§‹åŒ–æ¨¡å¼
        structured_patterns = [
            r'\d+\.\s*[^ï¼š:]+[ï¼š:]\s*[0-9,]+å…ƒ',  # 1. é …ç›®ï¼šé‡‘é¡
            r'ï¼ˆ[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ï¼‰.*?[ï¼š:]\s*[0-9,]+å…ƒ',  # ï¼ˆä¸€ï¼‰é …ç›®ï¼šé‡‘é¡
            r'èªªæ˜ï¼š.*?[0-9,]+å…ƒ'  # èªªæ˜ï¼š...é‡‘é¡
        ]
        
        structured_matches = sum(1 for pattern in structured_patterns 
                               if re.search(pattern, text))
        
        if structured_matches >= 2:
            return "structured"
        elif "å…ƒ" in text:
            return "semi_structured"
        else:
            return "unstructured"
    
    def _generate_structured_compensation(self, comp_facts: str, parties: dict) -> str:
        """ä½¿ç”¨çµæ§‹åŒ–è™•ç†å™¨ç”Ÿæˆæå®³é …ç›®"""
        print("ğŸ—ï¸ ä½¿ç”¨çµæ§‹åŒ–è™•ç†å™¨åˆ†ææå®³é …ç›®...")
        
        # ä½¿ç”¨çµæ§‹åŒ–è™•ç†å™¨
        result = self.structured_processor.process_structured_document(comp_facts)
        
        # æª¢æŸ¥æ˜¯å¦æœ‰è¨ˆç®—éŒ¯èª¤
        validation = result.get('validation', {})
        if validation.get('claimed_total') and not validation.get('match', True):
            print(f"âš ï¸ ç™¼ç¾è¨ˆç®—éŒ¯èª¤ï¼šåŸèµ·è¨´ç‹€è²ç¨±{validation['claimed_total']:,}å…ƒï¼Œå¯¦éš›æ‡‰ç‚º{validation['calculated_total']:,}å…ƒ")
            print(f"ğŸ“Š å·®é¡ï¼š{abs(validation['difference']):,}å…ƒ")
        
        # ç”Ÿæˆä¿®æ­£å¾Œçš„æå®³é …ç›®
        structured_text = "ä¸‰ã€æå®³é …ç›®ï¼š\n\n"
        
        # æŒ‰åŸå‘Šåˆ†çµ„é¡¯ç¤º
        current_plaintiff = None
        plaintiff_index = 0
        item_counter = 1
        
        for item in result['structured_items']:
            # åˆ¤æ–·æ˜¯å¦ç‚ºæ–°çš„åŸå‘Š
            item_title = item['item_title']
            if 'åŸå‘Š' in item_title:
                # æå–åŸå‘Šå§“å
                plaintiff_match = re.search(r'åŸå‘Š([^ä¹‹çš„]+)', item_title)
                if plaintiff_match:
                    plaintiff_name = plaintiff_match.group(1).strip()
                    if plaintiff_name != current_plaintiff:
                        current_plaintiff = plaintiff_name
                        plaintiff_index += 1
                        chinese_num = self._chinese_num(plaintiff_index)
                        structured_text += f"ï¼ˆ{chinese_num}ï¼‰åŸå‘Š{current_plaintiff}ä¹‹æå®³ï¼š\n"
                        item_counter = 1
            
            # æ·»åŠ æå®³é …ç›®
            structured_text += f"{item_counter}. {item['item_title']}ï¼š{item['formatted_amount']}\n"
            if item['description']:
                structured_text += f"   {item['description']}\n"
            item_counter += 1
            structured_text += "\n"
        
        # æ·»åŠ æˆ‘èªªæ˜
        total_amount = result['calculation']['total']
        structured_text += f"\nğŸ’° æå®³ç¸½è¨ˆï¼šæ–°å°å¹£{total_amount:,}å…ƒæ•´\n"
        
        # å¦‚æœæœ‰è¨ˆç®—éŒ¯èª¤ï¼Œæ·»åŠ èªªæ˜
        if validation.get('claimed_total') and not validation.get('match', True):
            structured_text += f"\nï¼ˆè¨»ï¼šç¶“é‡æ–°è¨ˆç®—ï¼Œæ­£ç¢ºç¸½é¡ç‚º{total_amount:,}å…ƒï¼Œ"
            if validation['difference'] > 0:
                structured_text += f"åŸèµ·è¨´ç‹€å°‘ç®—{validation['difference']:,}å…ƒï¼‰"
            else:
                structured_text += f"åŸèµ·è¨´ç‹€å¤šç®—{abs(validation['difference']):,}å…ƒï¼‰"
        
        return structured_text
    
    def generate_cot_conclusion_with_smart_amount_calculation(self, accident_facts: str, compensation_text: str, parties: dict) -> str:
        """ä½¿ç”¨æ™ºèƒ½é‡‘é¡è¨ˆç®—ç”ŸæˆCoTçµè«–ï¼ˆé˜²æ­¢é‡è¤‡å’ŒéŒ¯èª¤è¨ˆç®—ï¼‰"""
        print("ğŸ§  ç”ŸæˆCoTçµè«–ï¼ˆå«ç¸½é‡‘é¡è¨ˆç®—ï¼‰...")
        
        # æå–æ‰€æœ‰é‡‘é¡ç”¨æ–¼è¨ˆç®—
        amounts = self._extract_valid_claim_amounts(compensation_text)
        total_amount = sum(amounts) if amounts else 0
        
        # æ§‹å»ºåŒ…å«é‡‘é¡è¨ˆç®—çš„CoTæç¤ºè©
        plaintiff = parties.get("åŸå‘Š", "åŸå‘Š")
        defendant = parties.get("è¢«å‘Š", "è¢«å‘Š")
        
        prompt = f"""ä½ æ˜¯å°ç£è³‡æ·±å¾‹å¸«ï¼Œè«‹é‹ç”¨Chain of Thoughtæ¨ç†æ–¹å¼ç”Ÿæˆå°ˆæ¥­çš„èµ·è¨´ç‹€çµè«–æ®µè½ã€‚

ğŸ‘¥ ç•¶äº‹äººè³‡è¨Šï¼š
åŸå‘Šï¼š{plaintiff}
è¢«å‘Šï¼š{defendant}

ğŸ“„ æ¡ˆä»¶äº‹å¯¦ï¼š
{accident_facts}

ğŸ“„ æå®³è³ å„Ÿå…§å®¹ï¼š
{compensation_text}

ğŸ’° æ™ºèƒ½é‡‘é¡åˆ†æçµæœï¼š
æå–åˆ°çš„æœ‰æ•ˆæ±‚å„Ÿé‡‘é¡ï¼š{amounts}
æ­£ç¢ºç¸½è¨ˆï¼š{total_amount:,}å…ƒ

ğŸ§  è«‹ä½¿ç”¨Chain of Thoughtæ–¹å¼åˆ†æï¼š

æ­¥é©Ÿ1: åˆ†ææ¡ˆä»¶æ€§è³ªå’Œç•¶äº‹äººè²¬ä»»
æ­¥é©Ÿ2: å¾æå®³è³ å„Ÿå…§å®¹ä¸­è­˜åˆ¥å„é …æå®³é …ç›®
æ­¥é©Ÿ3: é©—è­‰å„é …é‡‘é¡çš„åˆç†æ€§ï¼ˆä½¿ç”¨æ™ºèƒ½åˆ†æçµæœï¼‰
æ­¥é©Ÿ4: å½¢æˆç°¡æ½”ç²¾ç¢ºçš„çµè«–

ğŸ›ï¸ æœ€å¾Œè«‹ç”Ÿæˆå°ˆæ¥­çš„çµè«–æ®µè½ï¼Œæ ¼å¼è¦æ±‚ï¼š
- é–‹é ­ï¼šã€Œå››ã€çµè«–ï¼šã€
- ä¸­é–“ï¼šç°¡æ½”åˆ—å‡ºæå®³é …ç›®å’Œé‡‘é¡
- çµå°¾ï¼šç¸½è¨ˆé‡‘é¡{total_amount:,}å…ƒå’Œåˆ©æ¯è«‹æ±‚
- âš ï¸ é‡è¦ï¼šé¿å…é‡è¤‡èªªæ˜åŒä¸€é …ç›®ï¼Œæ¯é …æå®³åªèªªæ˜ä¸€æ¬¡
- âš ï¸ é‡è¦ï¼šå¿…é ˆä½¿ç”¨æä¾›çš„æ­£ç¢ºç¸½è¨ˆ{total_amount:,}å…ƒ

è«‹ç›´æ¥è¼¸å‡ºçµè«–æ®µè½ï¼š"""

        result = self.call_llm(prompt, timeout=180)
        
        return result if result else "å››ã€çµè«–ï¼š\nï¼ˆLLMç”Ÿæˆå¤±æ•—ï¼Œè«‹æª¢æŸ¥è¼¸å…¥å…§å®¹ï¼‰"

    def generate_cot_conclusion_with_structured_analysis(self, accident_facts: str, compensation_text: str, parties: dict) -> str:
        """ä½¿ç”¨æ™ºèƒ½é‡‘é¡è¨ˆç®—ç”ŸæˆCoTçµè«–ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰"""
        print("ğŸ§  ç”ŸæˆCoTçµè«–ï¼ˆä½¿ç”¨æ™ºèƒ½é‡‘é¡è¨ˆç®—ï¼‰...")
        
        # ç›´æ¥ä½¿ç”¨æ™ºèƒ½é‡‘é¡è¨ˆç®—æ–¹å¼ï¼Œé¿å…è¤‡é›œçš„çµæ§‹åŒ–è™•ç†
        return self.generate_cot_conclusion_with_smart_amount_calculation(
            accident_facts, compensation_text, parties
        )
    
    def _build_structured_cot_prompt(self, accident_facts: str, compensation_text: str, parties: dict, analysis_result: dict) -> str:
        """æ§‹å»ºåŸºæ–¼çµæ§‹åŒ–åˆ†æçš„CoTæç¤ºè©"""
        
        plaintiff = parties.get("åŸå‘Š", "åŸå‘Š")
        defendant = parties.get("è¢«å‘Š", "è¢«å‘Š")
        
        # æå–çµæ§‹åŒ–åˆ†æçµæœ
        structured_items = analysis_result.get('structured_items', [])
        calculation = analysis_result.get('calculation', {})
        validation = analysis_result.get('validation', {})
        
        # æ§‹å»ºé …ç›®æ‘˜è¦
        items_summary = "\nğŸ“‹ å·²è­˜åˆ¥çš„æå®³é …ç›®ï¼š"
        for item in structured_items:
            items_summary += f"\nâ€¢ {item['item_title']}: {item['formatted_amount']}"
        
        items_summary += f"\n\nğŸ’° è¨ˆç®—åˆ†æï¼š"
        items_summary += f"\nâ€¢ æ­£ç¢ºç¸½è¨ˆ: {calculation.get('total', 0):,}å…ƒ"
        
        if validation.get('claimed_total'):
            items_summary += f"\nâ€¢ åŸèµ·è¨´ç‹€è²ç¨±: {validation['claimed_total']:,}å…ƒ"
            if validation.get('difference', 0) != 0:
                if validation['difference'] < 0:
                    items_summary += f"\nâ€¢ âŒ åŸèµ·è¨´ç‹€å°‘ç®—äº†: {abs(validation['difference']):,}å…ƒ"
                else:
                    items_summary += f"\nâ€¢ âŒ åŸèµ·è¨´ç‹€å¤šç®—äº†: {validation['difference']:,}å…ƒ"
                items_summary += f"\nâ€¢ âœ… è«‹ä½¿ç”¨æ­£ç¢ºé‡‘é¡: {calculation.get('total', 0):,}å…ƒ"
        
        prompt = f"""ä½ æ˜¯å°ç£è³‡æ·±å¾‹å¸«ï¼Œè«‹é‹ç”¨Chain of Thoughtæ¨ç†æ–¹å¼ï¼Œæ ¹æ“šçµæ§‹åŒ–åˆ†æçµæœç”Ÿæˆå°ˆæ¥­çš„èµ·è¨´ç‹€çµè«–æ®µè½ã€‚

ğŸ¯ é‡è¦æŒ‡ç¤ºï¼š
1. å¿…é ˆä½¿ç”¨æ­£ç¢ºçš„é‡‘é¡è¨ˆç®—ï¼Œé¿å…é‡è¤‡è¨ˆç®—èªªæ˜æ–‡å­—ä¸­çš„é‡‘é¡
2. ä½¿ç”¨é€æ­¥æ¨ç†æ–¹å¼åˆ†ææå®³é …ç›®
3. çµè«–å¿…é ˆåŒ…å«å®Œæ•´çš„é …ç›®æ˜ç´°
4. ç¸½é‡‘é¡å¿…é ˆæº–ç¢ºç„¡èª¤
5. æ¡ç”¨æ¨™æº–çš„æ³•å¾‹æ–‡æ›¸æ ¼å¼

ğŸ‘¥ ç•¶äº‹äººè³‡è¨Šï¼š
åŸå‘Šï¼š{plaintiff}
è¢«å‘Šï¼š{defendant}

ğŸ“„ æ¡ˆä»¶äº‹å¯¦ï¼š
{accident_facts}

ğŸ“„ æå®³è³ å„ŸåŸå§‹å…§å®¹ï¼š
{compensation_text}

{items_summary}

ğŸ§  è«‹ä½¿ç”¨Chain of Thoughtæ–¹å¼åˆ†æï¼š

æ­¥é©Ÿ1: åˆ†ææ¡ˆä»¶æ€§è³ªå’Œç•¶äº‹äººè²¬ä»»
æ­¥é©Ÿ2: æª¢è¦–å„é …æå®³çš„åˆç†æ€§å’Œæ³•å¾‹ä¾æ“š
æ­¥é©Ÿ3: é©—è­‰é‡‘é¡è¨ˆç®—çš„æº–ç¢ºæ€§
æ­¥é©Ÿ4: ç¶œåˆåˆ†æä¸¦å½¢æˆçµè«–

ğŸ›ï¸ æœ€å¾Œè«‹ç”Ÿæˆå°ˆæ¥­çš„çµè«–æ®µè½ï¼ŒåŒ…æ‹¬ï¼š
1. æå®³é …ç›®æ˜ç´°åˆ—è¡¨
2. æ­£ç¢ºçš„ç¸½é‡‘é¡è¨ˆç®—  
3. æ¨™æº–çš„çµè«–æ ¼å¼
4. åˆ©æ¯è¨ˆç®—æ¢æ¬¾

æ ¼å¼è¦æ±‚ï¼š
- é–‹é ­ï¼šã€Œå››ã€çµè«–ï¼šã€
- ä¸­é–“ï¼šåˆ—èˆ‰å„é …æå®³æ˜ç´°
- çµå°¾ï¼šç¸½è¨ˆé‡‘é¡å’Œåˆ©æ¯è«‹æ±‚
- âš ï¸ é‡è¦ï¼šé¿å…é‡è¤‡èªªæ˜åŒä¸€é …ç›®ï¼Œæ¯é …æå®³åªèªªæ˜ä¸€æ¬¡
- âš ï¸ é‡è¦ï¼šä¸è¦é‡è¤‡åˆ—èˆ‰é‡‘é¡ï¼Œä½¿ç”¨çµæ§‹åŒ–åˆ†æçš„æ­£ç¢ºç¸½é¡

é‡è¦ï¼šè«‹ç¢ºä¿é‡‘é¡è¨ˆç®—çµ•å°æ­£ç¢ºï¼Œä½¿ç”¨çµæ§‹åŒ–åˆ†æçš„æ­£ç¢ºç¸½é¡ï¼"""

        return prompt
    
    def _build_traditional_cot_prompt(self, accident_facts: str, compensation_text: str, parties: dict) -> str:
        """æ§‹å»ºå‚³çµ±CoTæç¤ºè©"""
        
        plaintiff = parties.get("åŸå‘Š", "åŸå‘Š")
        defendant = parties.get("è¢«å‘Š", "è¢«å‘Š")
        
        prompt = f"""ä½ æ˜¯å°ç£è³‡æ·±å¾‹å¸«ï¼Œè«‹é‹ç”¨Chain of Thoughtæ¨ç†æ–¹å¼ç”Ÿæˆå°ˆæ¥­çš„èµ·è¨´ç‹€çµè«–æ®µè½ã€‚

ğŸ‘¥ ç•¶äº‹äººè³‡è¨Šï¼š
åŸå‘Šï¼š{plaintiff}
è¢«å‘Šï¼š{defendant}

ğŸ“„ æ¡ˆä»¶äº‹å¯¦ï¼š
{accident_facts}

ğŸ“„ æå®³è³ å„Ÿå…§å®¹ï¼š
{compensation_text}

ğŸ§  è«‹ä½¿ç”¨Chain of Thoughtæ–¹å¼åˆ†æï¼š

æ­¥é©Ÿ1: åˆ†ææ¡ˆä»¶æ€§è³ªå’Œç•¶äº‹äººè²¬ä»»
æ­¥é©Ÿ2: æª¢è¦–å„é …æå®³çš„åˆç†æ€§å’Œæ³•å¾‹ä¾æ“š  
æ­¥é©Ÿ3: è¨ˆç®—ç¸½æå®³é‡‘é¡
æ­¥é©Ÿ4: ç¶œåˆåˆ†æä¸¦å½¢æˆçµè«–

ğŸ›ï¸ æœ€å¾Œè«‹ç”Ÿæˆå°ˆæ¥­çš„çµè«–æ®µè½ï¼Œæ ¼å¼è¦æ±‚ï¼š
- é–‹é ­ï¼šã€Œå››ã€çµè«–ï¼šã€
- ä¸­é–“ï¼šåˆ—èˆ‰å„é …æå®³æ˜ç´°
- çµå°¾ï¼šç¸½è¨ˆé‡‘é¡å’Œåˆ©æ¯è«‹æ±‚
- âš ï¸ é‡è¦ï¼šé¿å…é‡è¤‡èªªæ˜åŒä¸€é …ç›®ï¼Œæ¯é …æå®³åªèªªæ˜ä¸€æ¬¡
- âš ï¸ é‡è¦ï¼šä¸è¦é‡è¤‡åˆ—èˆ‰ç›¸åŒçš„é‡‘é¡å’Œé …ç›®"""

        return prompt
    
    def _post_process_structured_conclusion(self, conclusion: str, analysis_result: dict) -> str:
        """å¾Œè™•ç†çµæ§‹åŒ–çµè«–"""
        
        # æ·»åŠ è™•ç†è³‡è¨Š
        processing_info = f"\n\nğŸ’¡ è™•ç†è³‡è¨Šï¼š\n"
        processing_info += f"è™•ç†æ–¹æ³•ï¼šçµæ§‹åŒ–åˆ†æ\n"
        
        calculation = analysis_result.get('calculation', {})
        validation = analysis_result.get('validation', {})
        
        processing_info += f"æ­£ç¢ºç¸½é¡ï¼š{calculation.get('total', 0):,}å…ƒ\n"
        if validation.get('claimed_total'):
            processing_info += f"åŸè²ç¨±é¡ï¼š{validation['claimed_total']:,}å…ƒ\n"
            if validation.get('difference', 0) != 0:
                processing_info += f"å·®é¡ä¿®æ­£ï¼š{abs(validation['difference']):,}å…ƒ\n"
        
        return conclusion + processing_info

    def _generate_complex_compensation(self, comp_facts: str, parties: dict) -> str:
        """è™•ç†è¤‡é›œæå®³é …ç›®æ–‡æœ¬çš„åˆ†æ­¥æ–¹æ³•"""
        print("ğŸ”§ ä½¿ç”¨åˆ†æ­¥è™•ç†è¤‡é›œæå®³æ–‡æœ¬...")
        
        # æ­¥é©Ÿ1ï¼šé è™•ç†ä¸­æ–‡æ•¸å­—
        preprocessed_facts = self._preprocess_chinese_numbers(comp_facts)
        
        # ç›´æ¥æ ¼å¼åŒ–ç‚ºæ¨™æº–æå®³é …ç›®
        format_prompt = f"""è«‹å°‡ä»¥ä¸‹æå®³è³ å„Ÿå…§å®¹é‡æ–°æ•´ç†ç‚ºæ¨™æº–çš„æ³•å¾‹æ–‡æ›¸æ ¼å¼ï¼š

ã€ç•¶äº‹äººã€‘
åŸå‘Šï¼š{parties.get('åŸå‘Š', 'æœªæåŠ')}ï¼ˆå…±{parties.get('åŸå‘Šæ•¸é‡', 1)}åï¼‰

ã€æå®³æè¿°ã€‘
{preprocessed_facts}

ã€æ¨™æº–æ ¼å¼è¦æ±‚ã€‘
ä¸‰ã€æå®³é …ç›®ï¼š

ï¼ˆä¸€ï¼‰åŸå‘Š[å§“å]ä¹‹æå®³ï¼š
1. [é …ç›®åç¨±]ï¼š[é‡‘é¡]å…ƒ
   èªªæ˜ï¼šåŸå‘Š[å§“å]å› æœ¬æ¬¡è»Šç¦[æå®³æ€§è³ª]
2. [é …ç›®åç¨±]ï¼š[é‡‘é¡]å…ƒ
   èªªæ˜ï¼šåŸå‘Š[å§“å]å› æœ¬æ¬¡è»Šç¦[æå®³æ€§è³ª]

ï¼ˆäºŒï¼‰åŸå‘Š[å§“å]ä¹‹æå®³ï¼š
1. [é …ç›®åç¨±]ï¼š[é‡‘é¡]å…ƒ
   èªªæ˜ï¼šåŸå‘Š[å§“å]å› æœ¬æ¬¡è»Šç¦[æå®³æ€§è³ª]

ã€é‡è¦è¦æ±‚ã€‘
- ç›´æ¥æ•´ç†æå®³é …ç›®ï¼Œä¸è¦é¡¯ç¤ºåˆ†ææˆ–è¨ˆç®—éç¨‹
- å…±åŒè²»ç”¨è¦å¹³å‡åˆ†æ”¤çµ¦ç›¸é—œåŸå‘Š
- æ‰€æœ‰é‡‘é¡ä½¿ç”¨åƒåˆ†ä½é€—è™Ÿæ ¼å¼
- æ¯é …æå®³éƒ½è¦æœ‰å…·é«”èªªæ˜
- ç¢ºä¿æ ¼å¼æ•´é½Šçµ±ä¸€

è«‹ç›´æ¥è¼¸å‡ºæ¨™æº–æ ¼å¼çš„æå®³é …ç›®ï¼š"""

        return self.call_llm(format_prompt, timeout=120)

    def _verify_calculation(self, result_text: str) -> dict:
        """å¾çµæœä¸­æå–ä¸¦é©—è­‰è¨ˆç®—æº–ç¢ºæ€§"""
        import re
        
        verification = {
            "correct": True,
            "errors": [],
            "corrected_total": None
        }
        
        # æå–æ‰€æœ‰é‡‘é¡æ•¸å­—
        amounts = re.findall(r'(\d{1,3}(?:,\d{3})*)', result_text)
        amounts = [int(amt.replace(',', '')) for amt in amounts if amt]
        
        if len(amounts) >= 10:  # å¦‚æœæœ‰è¶³å¤ çš„é‡‘é¡é€²è¡Œé©—è­‰
            # å˜—è©¦æ‰¾åˆ°å…©å€‹å°è¨ˆå’Œç¸½è¨ˆ
            try:
                # å‡è¨­æœ€å¾Œä¸‰å€‹å¤§æ•¸å­—æ˜¯ï¼šå°è¨ˆ1ã€å°è¨ˆ2ã€ç¸½è¨ˆ
                if len(amounts) >= 3:
                    subtotal1 = amounts[-3]
                    subtotal2 = amounts[-2] 
                    reported_total = amounts[-1]
                    
                    # é©—è­‰ç¸½è¨ˆ
                    actual_total = subtotal1 + subtotal2
                    if actual_total != reported_total:
                        verification["correct"] = False
                        verification["errors"].append(f"ç¸½è¨ˆéŒ¯èª¤ï¼š{subtotal1} + {subtotal2} = {actual_total}ï¼Œä½†å ±å‘Šç‚º{reported_total}")
                        verification["corrected_total"] = actual_total
                        
            except Exception as e:
                verification["errors"].append(f"é©—è­‰éç¨‹å‡ºéŒ¯ï¼š{e}")
        
        return verification

    def _generate_llm_based_compensation(self, comp_facts: str, parties: dict) -> str:
        """ä½¿ç”¨LLMå®Œå…¨è™•ç†æå®³é …ç›®ç”Ÿæˆ"""
        
        # å…ˆé è™•ç†ä¸­æ–‡æ•¸å­—
        preprocessed_facts = self._preprocess_chinese_numbers(comp_facts)
        
        prompt = f"""ä½ æ˜¯å°ç£å¾‹å¸«ï¼Œè«‹åˆ†æè»Šç¦æ¡ˆä»¶çš„æå®³è³ å„Ÿä¸¦æ•´ç†æˆç°¡æ½”çš„æ¨™æº–æ ¼å¼ï¼š

ã€ç•¶äº‹äººè³‡è¨Šã€‘
åŸå‘Šï¼š{parties.get('åŸå‘Š', 'æœªæåŠ')}ï¼ˆå…±{parties.get('åŸå‘Šæ•¸é‡', 1)}åï¼‰
è¢«å‘Šï¼š{parties.get('è¢«å‘Š', 'æœªæåŠ')}ï¼ˆå…±{parties.get('è¢«å‘Šæ•¸é‡', 1)}åï¼‰

ã€æå®³æè¿°ã€‘
{preprocessed_facts}

ã€è¼¸å‡ºæ ¼å¼è¦æ±‚ã€‘
ä¸‰ã€æå®³é …ç›®ï¼š

ï¼ˆä¸€ï¼‰åŸå‘Š[å§“å]ä¹‹æå®³ï¼š
1. [æå®³é …ç›®åç¨±]ï¼š[é‡‘é¡]å…ƒ
   èªªæ˜ï¼šåŸå‘Š[å§“å]å› æœ¬æ¬¡è»Šç¦[å…·é«”æå®³èªªæ˜]
2. [æå®³é …ç›®åç¨±]ï¼š[é‡‘é¡]å…ƒ  
   èªªæ˜ï¼šåŸå‘Š[å§“å]å› æœ¬æ¬¡è»Šç¦[å…·é«”æå®³èªªæ˜]

ï¼ˆäºŒï¼‰åŸå‘Š[å§“å]ä¹‹æå®³ï¼š
1. [æå®³é …ç›®åç¨±]ï¼š[é‡‘é¡]å…ƒ
   èªªæ˜ï¼šåŸå‘Š[å§“å]å› æœ¬æ¬¡è»Šç¦[å…·é«”æå®³èªªæ˜]

ã€é‡è¦è¦æ±‚ã€‘
- ç›´æ¥æå–ä¸¦æ•´ç†æå®³é …ç›®ï¼Œä¸è¦é¡¯ç¤ºè¨ˆç®—éç¨‹
- å°‡ä¸­æ–‡æ•¸å­—è½‰æ›ç‚ºé˜¿æ‹‰ä¼¯æ•¸å­—ï¼Œä½¿ç”¨åƒåˆ†ä½é€—è™Ÿæ ¼å¼
- æ¯é …æå®³éƒ½è¦æ˜ç¢ºæ­¸å±¬åˆ°å…·é«”åŸå‘Š
- å…±åŒè²»ç”¨è¦å¹³å‡åˆ†æ”¤ä¸¦èªªæ˜
- åªè¼¸å‡ºæå®³é …ç›®æ¸…å–®ï¼Œä¸è¦é¡å¤–çš„è¨ˆç®—æ­¥é©Ÿ

è«‹ç›´æ¥è¼¸å‡ºæ•´ç†å¾Œçš„æå®³é …ç›®ï¼š"""

        result = self.call_llm(prompt, timeout=120)
        
        # æª¢æŸ¥çµæœæ˜¯å¦åŒ…å«é æœŸæ ¼å¼
        if "ï¼ˆä¸€ï¼‰" in result and "åŸå‘Š" in result:
            # æ¸…ç†çµæœï¼Œç¢ºä¿æ ¼å¼æ­£ç¢º
            if not result.startswith("ä¸‰ã€æå®³é …ç›®ï¼š"):
                result = "ä¸‰ã€æå®³é …ç›®ï¼š\n" + result
            return result
        else:
            # Fallbackï¼šåŸºæœ¬æ ¼å¼åŒ–
            return f"ä¸‰ã€æå®³é …ç›®ï¼š\n{comp_facts}"

    def _comprehensive_number_preprocessing(self, text: str) -> str:
        """å…¨é¢é è™•ç†ä¸­æ–‡æ•¸å­—å’Œç‰¹æ®Šæ ¼å¼"""
        import re
        
        # è™•ç† Xè¬Y,YYYå…ƒ æ ¼å¼ (å¦‚ï¼š26è¬4,379å…ƒ)
        pattern1 = r'(\d+)è¬(\d+,?\d+)å…ƒ'
        def replace1(match):
            wan = int(match.group(1))
            rest = int(match.group(2).replace(',', ''))
            total = wan * 10000 + rest
            return f"{total}å…ƒ"
        text = re.sub(pattern1, replace1, text)
        
        # è™•ç†å…¶ä»–ä¸­æ–‡æ•¸å­—æ ¼å¼
        text = re.sub(r'(\d+)è¬(\d+)åƒå…ƒ', lambda m: f"{int(m.group(1))*10000 + int(m.group(2))*1000}å…ƒ", text)
        text = re.sub(r'(\d+)è¬å…ƒ', lambda m: f"{int(m.group(1))*10000}å…ƒ", text)
        text = re.sub(r'(\d+)åƒå…ƒ', lambda m: f"{int(m.group(1))*1000}å…ƒ", text)
        
        return text

    def _is_same_damage_type(self, context1: str, context2: str) -> bool:
        """åˆ¤æ–·å…©å€‹ä¸Šä¸‹æ–‡æ˜¯å¦ç‚ºç›¸åŒçš„æå®³é¡å‹"""
        damage_types = [
            ['é†«ç™‚', 'æ²»ç™‚', 'å°±è¨º'],
            ['çœ‹è­·', 'ç…§é¡§'],
            ['ç‰™é½’', 'å‡ç‰™'],
            ['æ…°æ’«', 'ç²¾ç¥', 'ç—›è‹¦'],
            ['äº¤é€š', 'è»Šè³‡'],
            ['å·¥ä½œ', 'æ”¶å…¥', 'è–ªè³‡'],
            ['ä¿®å¾©', 'ä¿®ç†', 'ç¶­ä¿®']
        ]
        
        # æ‰¾å‡ºæ¯å€‹ä¸Šä¸‹æ–‡çš„æå®³é¡å‹
        type1 = None
        type2 = None
        
        for i, keywords in enumerate(damage_types):
            if any(keyword in context1 for keyword in keywords):
                type1 = i
            if any(keyword in context2 for keyword in keywords):
                type2 = i
        
        return type1 is not None and type1 == type2

    def _extract_valid_claim_amounts(self, text: str) -> list:
        """æ™ºèƒ½æå–æœ‰æ•ˆçš„æ±‚å„Ÿé‡‘é¡ï¼ˆåŸºæ–¼ä¸Šä¸‹æ–‡èªå¢ƒï¼‰"""
        import re

        print(f"ğŸ” ã€æ™ºèƒ½é‡‘é¡æå–ã€‘åŸå§‹æ–‡æœ¬: {text[:200]}...")

        # 1. å…ˆé è™•ç†ä¸­æ–‡æ•¸å­—
        processed_text = self._comprehensive_number_preprocessing(text)
        clean_text = processed_text.replace(',', '')

        # 2. å®šç¾©æœ‰æ•ˆçš„æ±‚å„Ÿé—œéµè©
        valid_claim_keywords = [
            'è²»ç”¨', 'æå¤±', 'æ…°æ’«é‡‘', 'è³ å„Ÿ', 'æ”¯å‡º', 'èŠ±è²»',
            'é†«ç™‚', 'ä¿®å¾©', 'ä¿®ç†', 'äº¤é€š', 'çœ‹è­·', 'æ‰‹è¡“',
            'å‡ç‰™', 'å¾©å¥', 'æ²»ç™‚', 'å·¥ä½œæ”¶å…¥'
        ]

        # 3. å®šç¾©æ’é™¤çš„é—œéµè©ï¼ˆéæ±‚å„Ÿé …ç›®ï¼‰
        exclude_keywords = [
            'æ—¥è–ª', 'å¹´åº¦æ‰€å¾—', 'æœˆæ”¶å…¥', 'æ™‚è–ª', 'å­¸æ­·', 'ç•¢æ¥­',
            'åä¸‹', 'å‹•ç”¢', 'ç¸½è¨ˆ', 'åˆè¨ˆ', 'å…±è¨ˆ', 'å°è¨ˆ',
            'åŒ…æ‹¬', 'å…¶ä¸­', 'åŒ…å«',  # æ·»åŠ ç´°é …åˆ†è§£é—œéµè©
            '1æ—¥', 'æ—¥1', 'æ¯æ—¥', 'ä¸€æ—¥'  # æ·»åŠ æ—¥è–ªç›¸é—œé—œéµè©
        ]

        amounts = []
        lines = clean_text.split('\n')

        for line in lines:
            # æ‰¾å‡ºè©²è¡Œä¸­çš„æ‰€æœ‰é‡‘é¡
            line_amounts = re.findall(r'(\d+)\s*å…ƒ', line)

            for amt_str in line_amounts:
                try:
                    amount = int(amt_str)
                    if amount < 100:  # è·³éå°é¡ï¼ˆå¯èƒ½æ˜¯ç·¨è™Ÿç­‰ï¼‰
                        continue

                    # æª¢æŸ¥é‡‘é¡å‘¨åœçš„ä¸Šä¸‹æ–‡
                    # æ‰¾åˆ°é‡‘é¡åœ¨åŸæ–‡ä¸­çš„ä½ç½®
                    amount_pos = line.find(amt_str + 'å…ƒ')
                    if amount_pos == -1:
                        continue

                    # æå–é‡‘é¡å‰å¾Œ50å€‹å­—ç¬¦çš„ä¸Šä¸‹æ–‡
                    start = max(0, amount_pos - 50)
                    end = min(len(line), amount_pos + 50)
                    context = line[start:end]

                    # å…ˆæª¢æŸ¥æ˜¯å¦åŒ…å«æœ‰æ•ˆæ±‚å„Ÿé—œéµè©
                    is_valid_claim = any(keyword in context for keyword in valid_claim_keywords)
                    
                    if is_valid_claim:
                        # å¦‚æœæ˜¯æœ‰æ•ˆæ±‚å„Ÿé …ç›®ï¼Œå†æª¢æŸ¥æ˜¯å¦éœ€è¦æ’é™¤
                        should_exclude = any(keyword in context for keyword in exclude_keywords)
                        # ç‰¹æ®Šè™•ç†ï¼šå¦‚æœæ˜¯ã€Œçœ‹è­·è²»ç”¨ã€ç›¸é—œï¼Œå³ä½¿åŒ…å«ã€Œ1æ—¥ã€ä¹Ÿä¸æ’é™¤
                        if should_exclude and 'çœ‹è­·' in context:
                            should_exclude = False
                        
                        if should_exclude:
                            print(f"ğŸ” ã€æ’é™¤ã€‘{amount:,}å…ƒ - åŒ…å«æ’é™¤é—œéµè©: {context[:50]}...")
                        else:
                            print(f"ğŸ” ã€æœ‰æ•ˆã€‘{amount:,}å…ƒ - ä¸Šä¸‹æ–‡: {context[:50]}...")
                            amounts.append(amount)
                    else:
                        print(f"ğŸ” ã€è·³éã€‘{amount:,}å…ƒ - ç„¡æ˜ç¢ºæ±‚å„Ÿé—œéµè©: {context[:50]}...")

                except ValueError:
                    continue

        # 4. æ”¹é€²çš„å»é‡é‚è¼¯ï¼ˆæŒ‰é …ç›®é¡å‹åˆ†çµ„ï¼‰
        damage_items = {}  # æŒ‰é¡å‹åˆ†çµ„ï¼š{é¡å‹: [é‡‘é¡åˆ—è¡¨]}
        
        for line in clean_text.split('\n'):
            # è­˜åˆ¥æå®³é …ç›®æ¨™é¡Œè¡Œï¼ˆå¦‚ï¼šãˆ é†«ç™‚è²»ç”¨38,073å…ƒ æˆ– 1. é†«ç™‚è²»ç”¨38,073å…ƒï¼‰
            if (re.match(r'^[ãˆ ãˆ¡ãˆ¢ãˆ£ãˆ¤ãˆ¥ãˆ¦ãˆ§ãˆ¨ãˆ©]', line.strip()) or 
                re.match(r'^\d+\.\s*[^\d]*\d+å…ƒ', line.strip())):
                line_amounts = re.findall(r'(\d+)\s*å…ƒ', line)
                for amt_str in line_amounts:
                    try:
                        amount = int(amt_str)
                        if amount >= 100:  # æ’é™¤å°é¡
                            # åˆ¤æ–·æå®³é¡å‹
                            damage_type = "å…¶ä»–"
                            if 'é†«ç™‚' in line:
                                damage_type = "é†«ç™‚è²»ç”¨"
                            elif 'çœ‹è­·' in line:
                                damage_type = "çœ‹è­·è²»ç”¨"
                            elif 'ç‰™é½’' in line:
                                damage_type = "ç‰™é½’æå®³"
                            elif 'æ…°æ’«' in line or 'ç²¾ç¥' in line:
                                damage_type = "ç²¾ç¥æ…°æ’«é‡‘"
                            elif 'äº¤é€š' in line:
                                damage_type = "äº¤é€šè²»ç”¨"
                            elif 'è»Šè¼›' in line or 'ä¿®å¾©' in line or 'ä¿®ç†' in line:
                                damage_type = "è»Šè¼›ä¿®å¾©è²»ç”¨"
                            elif 'å·¥ä½œ' in line or 'æ”¶å…¥' in line:
                                damage_type = "å·¥ä½œæå¤±"
                            
                            if damage_type not in damage_items:
                                damage_items[damage_type] = []
                            damage_items[damage_type].append(amount)
                            print(f"ğŸ” ã€ç¢ºèªé …ç›®ã€‘{damage_type}: {amount:,}å…ƒ")
                    except ValueError:
                        continue
        
        # æ¯ç¨®æå®³é¡å‹åªå–ä¸€å€‹é‡‘é¡ï¼ˆé€šå¸¸æ¨™é¡Œè¡Œçš„é‡‘é¡æ˜¯æ­£ç¢ºçš„ï¼‰
        final_amounts = []
        for damage_type, amounts_list in damage_items.items():
            if amounts_list:
                # å–è©²é¡å‹çš„ç¬¬ä¸€å€‹é‡‘é¡ï¼ˆæ¨™é¡Œè¡Œï¼‰
                final_amounts.append(amounts_list[0])
                print(f"âœ… ã€æ¡ç”¨ã€‘{damage_type}: {amounts_list[0]:,}å…ƒ")

        print(f"ğŸ” ã€æ™ºèƒ½é‡‘é¡æå–ã€‘å»é‡å¾Œæœ‰æ•ˆé‡‘é¡: {final_amounts}")
        print(f"ğŸ” ã€æ™ºèƒ½é‡‘é¡æå–ã€‘æœ€çµ‚ç¸½è¨ˆ: {sum(final_amounts):,}å…ƒ")

        return final_amounts

    def _extract_damage_items_from_text(self, text: str) -> Dict[str, List[Dict]]:
        """å¾æ–‡æœ¬ä¸­ç²¾ç¢ºæå–æå®³é …ç›®"""
        # æŒ‰åŸå‘Šåˆ†çµ„
        plaintiff_damages = {}
        
        # åˆ†å¥è™•ç†
        sentences = re.split(r'[ã€‚]', text)
        
        for sentence in sentences:
            # è­˜åˆ¥åŸå‘Š
            plaintiff_match = re.search(r'åŸå‘Š([^ï¼Œã€‚ï¼›ã€\s]{2,4})', sentence)
            if not plaintiff_match:
                continue
                
            plaintiff = plaintiff_match.group(1)
            if plaintiff not in plaintiff_damages:
                plaintiff_damages[plaintiff] = []
            
            # ç²¾ç¢ºåŒ¹é…å„ç¨®æå®³é¡å‹
            # é†«ç™‚è²»ç”¨
            if 'é†«ç™‚è²»ç”¨' in sentence:
                amount_match = re.search(r'é†«ç™‚è²»ç”¨\s*(\d+(?:,\d{3})*)\s*å…ƒ', sentence)
                if amount_match:
                    plaintiff_damages[plaintiff].append({
                        'name': 'é†«ç™‚è²»ç”¨',
                        'amount': int(amount_match.group(1).replace(',', '')),
                        'description': f'åŸå‘Š{plaintiff}å› æœ¬æ¬¡äº‹æ•…å—å‚·å°±é†«ä¹‹é†«ç™‚è²»ç”¨'
                    })
            
            # äº¤é€šè²»
            if 'äº¤é€šè²»' in sentence:
                amount_match = re.search(r'äº¤é€šè²»\s*(\d+(?:,\d{3})*)\s*å…ƒ', sentence)
                if amount_match:
                    plaintiff_damages[plaintiff].append({
                        'name': 'äº¤é€šè²»ç”¨',
                        'amount': int(amount_match.group(1).replace(',', '')),
                        'description': f'åŸå‘Š{plaintiff}å› æœ¬æ¬¡äº‹æ•…æ‰€ç”Ÿä¹‹äº¤é€šè²»ç”¨'
                    })
            
            # å·¥ä½œæå¤±
            if any(keyword in sentence for keyword in ['å·¥è³‡æå¤±', 'ä¸èƒ½å·¥ä½œ', 'ç„¡æ³•å·¥ä½œ']):
                amount_match = re.search(r'(?:æå¤±|è«‹æ±‚)\s*(\d+(?:,\d{3})*)\s*å…ƒ', sentence)
                if amount_match:
                    plaintiff_damages[plaintiff].append({
                        'name': 'å·¥ä½œæå¤±',
                        'amount': int(amount_match.group(1).replace(',', '')),
                        'description': f'åŸå‘Š{plaintiff}å› æœ¬æ¬¡äº‹æ•…ç„¡æ³•å·¥ä½œä¹‹æ”¶å…¥æå¤±'
                    })
            
            # ç²¾ç¥æ…°æ’«é‡‘
            if 'æ…°æ’«é‡‘' in sentence:
                amount_match = re.search(r'æ…°æ’«é‡‘\s*(\d+(?:,\d{3})*)\s*å…ƒ', sentence)
                if amount_match:
                    plaintiff_damages[plaintiff].append({
                        'name': 'ç²¾ç¥æ…°æ’«é‡‘',
                        'amount': int(amount_match.group(1).replace(',', '')),
                        'description': f'åŸå‘Š{plaintiff}å› æœ¬æ¬¡äº‹æ•…æ‰€å—ç²¾ç¥ç—›è‹¦ä¹‹æ…°æ’«é‡‘'
                    })
            
            # è»Šè¼›è²¶å€¼
            if any(keyword in sentence for keyword in ['è²¶å€¼', 'è²¶æ', 'åƒ¹å€¼æ¸›æ']):
                amount_match = re.search(r'(?:è²¶æ|æ¸›æ)\s*(\d+(?:,\d{3})*)\s*å…ƒ', sentence)
                if amount_match:
                    plaintiff_damages[plaintiff].append({
                        'name': 'è»Šè¼›è²¶å€¼æå¤±',
                        'amount': int(amount_match.group(1).replace(',', '')),
                        'description': 'ç³»çˆ­è»Šè¼›å› æœ¬æ¬¡äº‹æ•…ä¹‹åƒ¹å€¼æ¸›æ'
                    })
            
            # é‘‘å®šè²»
            if 'é‘‘å®šè²»' in sentence:
                amount_match = re.search(r'é‘‘å®šè²»\s*(\d+(?:,\d{3})*)\s*å…ƒ', sentence)
                if amount_match:
                    plaintiff_damages[plaintiff].append({
                        'name': 'é‘‘å®šè²»ç”¨',
                        'amount': int(amount_match.group(1).replace(',', '')),
                        'description': 'è»Šè¼›æå®³é‘‘å®šè²»ç”¨'
                    })
        
        return plaintiff_damages
    
    def _format_damage_items(self, damage_items: Dict[str, List[Dict]]) -> str:
        """æ ¼å¼åŒ–æå®³é …ç›®"""
        if not damage_items:
            return ""
        
        result = "ä¸‰ã€æå®³é …ç›®ï¼š\n"
        for idx, (plaintiff, damages) in enumerate(damage_items.items()):
            chinese_num = self._chinese_num(idx + 1)
            result += f"\nï¼ˆ{chinese_num}ï¼‰åŸå‘Š{plaintiff}ä¹‹æå®³ï¼š\n"
            
            for i, damage in enumerate(damages, 1):
                result += f"{i}. {damage['name']}ï¼š{damage['amount']:,}å…ƒ\n"
                result += f"   èªªæ˜ï¼š{damage['description']}\n"
            
            # å°è¨ˆ
            subtotal = sum(d['amount'] for d in damages)
            result += f"\nå°è¨ˆï¼š{subtotal:,}å…ƒ\n"
        
        # ç¸½è¨ˆ
        total = sum(sum(d['amount'] for d in damages) for damages in damage_items.values())
        result += f"\næå®³ç¸½è¨ˆï¼šæ–°å°å¹£{total:,}å…ƒæ•´"
        
        return result

# ===== ä¸»è¦äº’å‹•åŠŸèƒ½ =====

def interactive_generate_lawsuit():
    """äº’å‹•å¼èµ·è¨´ç‹€ç”Ÿæˆï¼ˆä¿æŒHybridç‰ˆæœ¬çš„ä½¿ç”¨è€…äº’å‹•æ–¹å¼ï¼‰"""
    print("=" * 80)
    print("ğŸ›ï¸  è»Šç¦èµ·è¨´ç‹€ç”Ÿæˆå™¨ - æ··åˆç‰ˆæœ¬ï¼ˆæ•´åˆçµæ§‹åŒ–é‡‘é¡è™•ç†ï¼‰")
    print("=" * 80)
    print("ğŸ‘‹ æ­¡è¿ä½¿ç”¨ï¼è«‹è¼¸å…¥å®Œæ•´çš„user queryï¼Œæˆ‘æœƒç‚ºæ‚¨ç”Ÿæˆå°ˆæ¥­çš„èµ·è¨´ç‹€")
    print("ğŸ’¡ æ”¯æ´çµæ§‹åŒ–é‡‘é¡è™•ç†ï¼Œè‡ªå‹•ä¿®æ­£è¨ˆç®—éŒ¯èª¤")
    print()
    
    # åˆå§‹åŒ–ç”Ÿæˆå™¨
    generator = HybridCoTGenerator()
    
    while True:
        print("ğŸ“ è«‹è¼¸å…¥æ‚¨çš„å®Œæ•´user queryï¼ˆæˆ–è¼¸å…¥ 'quit' é€€å‡ºï¼‰ï¼š")
        print("-" * 40)
        
        user_query = input("ğŸ¯ è«‹è¼¸å…¥ï¼š").strip()
        
        if user_query.lower() in ['quit', 'exit', 'é€€å‡º', 'q']:
            print("ğŸ‘‹ æ„Ÿè¬ä½¿ç”¨ï¼Œå†è¦‹ï¼")
            break
            
        if not user_query:
            print("âš ï¸ è«‹è¼¸å…¥æœ‰æ•ˆçš„query")
            continue
        
        print("ğŸ”„ æ­£åœ¨åˆ†ææ‚¨çš„è¼¸å…¥...")
        
        try:
            # åˆ†æ®µæå–è³‡è¨Š
            sections = extract_sections(user_query)
            
            # æå–ç•¶äº‹äºº
            parties = extract_parties(user_query)
            
            # æ¡ˆä»¶åˆ†é¡å’Œæª¢ç´¢ç›¸ä¼¼æ¡ˆä¾‹
            accident_facts = sections.get("accident_facts", user_query)
            case_type = determine_case_type(accident_facts, parties)
            
            # ç›¸ä¼¼æ¡ˆä¾‹æª¢ç´¢ï¼ˆå®‰éœæ¨¡å¼ï¼‰
            similar_cases = []
            if FULL_MODE:
                # è©¢å•ç”¨æˆ¶æ˜¯å¦ä½¿ç”¨å¤šéšæ®µæª¢ç´¢
                use_advanced = input("ğŸ¤” æ˜¯å¦ä½¿ç”¨å¤šéšæ®µç²¾ç¢ºæª¢ç´¢ï¼Ÿ(y/Nï¼Œç›´æ¥æŒ‰Enterä½¿ç”¨ç°¡å–®æ¨¡å¼)ï¼š").lower().strip()
                
                if use_advanced in ['y', 'yes', 'æ˜¯']:
                    # ç”¨æˆ¶è‡ªå®šç¾©åƒæ•¸
                    k_final = int(input("ğŸ”¢ è«‹è¼¸å…¥éœ€è¦çš„ç›¸ä¼¼æ¡ˆä¾‹æ•¸é‡ K (å»ºè­°2-5)ï¼š") or "3")
                    multiplier = int(input("ğŸ”¢ è«‹è¼¸å…¥åˆå§‹æª¢ç´¢å€æ•¸ (å»ºè­°3-5å€)ï¼š") or "3")
                    initial_retrieve_count = k_final * multiplier
                    use_multi_stage = True
                else:
                    # ç°¡å–®æ¨¡å¼
                    k_final = 2
                    initial_retrieve_count = 5
                    use_multi_stage = False
                
                try:
                    # å®‰éœåŸ·è¡Œæª¢ç´¢
                    query_vector = embed(accident_facts)
                    if query_vector:
                        hits = es_search(query_vector, case_type, top_k=initial_retrieve_count, label="Facts", quiet=True)
                        if hits:
                            if use_multi_stage:
                                # å¤šéšæ®µæ¨¡å¼
                                candidate_case_ids = []
                                for hit in hits:
                                    case_id = hit['_source'].get('case_id')
                                    if case_id and case_id not in candidate_case_ids:
                                        candidate_case_ids.append(case_id)
                                
                                if candidate_case_ids:
                                    reranked_case_ids = rerank_case_ids_by_paragraphs(
                                        accident_facts, 
                                        candidate_case_ids[:k_final*2],
                                        label="Facts",
                                        quiet=True
                                    )
                                    final_case_ids = reranked_case_ids[:k_final]
                                    similar_cases = get_complete_cases_content(final_case_ids)
                            else:
                                # ç°¡å–®æ¨¡å¼
                                if hits:
                                    first_hit = hits[0]['_source']
                                    content_fields = ['original_text', 'content', 'text', 'facts_content', 'chunk_content', 'body']
                                    content_field = None
                                    for field in content_fields:
                                        if field in first_hit and first_hit[field]:
                                            content_field = field
                                            break
                                    
                                    if content_field:
                                        similar_cases = [hit['_source'].get(content_field, '') for hit in hits[:k_final] if hit['_source'].get(content_field)]
                                    else:
                                        # Fallback
                                        similar_cases = []
                                        for hit in hits[:k_final]:
                                            all_text = " ".join([str(v) for k, v in hit['_source'].items() 
                                                               if isinstance(v, str) and len(v) > 50])
                                            if all_text:
                                                similar_cases.append(all_text)
                except Exception as e:
                    similar_cases = []
            
            # ç”Ÿæˆèµ·è¨´æ›¸å…§å®¹
            facts = generator.generate_standard_facts(accident_facts, similar_cases)
            laws = generator.generate_standard_laws(
                sections.get("accident_facts", user_query),
                sections.get("injuries", ""),
                parties,
                sections.get("compensation_facts", "")
            )
            compensation_text = sections.get("compensation_facts", user_query)
            damages = generator.generate_smart_compensation(
                sections.get("injuries", ""),
                compensation_text, 
                parties
            )
            conclusion = generator.generate_cot_conclusion_with_structured_analysis(
                sections.get("accident_facts", user_query),
                compensation_text,
                parties
            )
            
            # æå–é©ç”¨æ³•æ¢
            applicable_laws = determine_applicable_laws(
                sections.get("accident_facts", user_query),
                sections.get("injuries", ""),
                sections.get("compensation_facts", ""),
                parties
            )
            
            # ===== è¼¸å‡ºæ ¸å¿ƒçµæœ =====
            print("\n" + "=" * 60)
            print("ğŸ“„ ç›¸ä¼¼æ¡ˆä¾‹")
            print("=" * 60)
            
            if similar_cases:
                for i, case in enumerate(similar_cases, 1):
                    print(f"\nã€æ¡ˆä¾‹ {i}ã€‘")
                    print(case[:500] + "..." if len(case) > 500 else case)
                    if i < len(similar_cases):
                        print("-" * 40)
            else:
                print("æœªæ‰¾åˆ°ç›¸ä¼¼æ¡ˆä¾‹")
            
            print("\n" + "=" * 60)
            print("âš–ï¸ é©ç”¨æ³•æ¢")
            print("=" * 60)
            
            for i, law in enumerate(applicable_laws, 1):
                print(f"{i}. {law}")
            
            print("\n" + "=" * 60)
            print("ğŸ“‹ ç”Ÿæˆçš„èµ·è¨´ç‹€")
            print("=" * 60)
            
            print(f"\n{facts}")
            print(f"\n{laws}")
            print(f"\n{damages}")
            print(f"\n{conclusion}")
            
            print("\n" + "=" * 60)
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
            print("è«‹æª¢æŸ¥è¼¸å…¥æ ¼å¼æˆ–è¯ç¹«ç³»çµ±ç®¡ç†å“¡")
        
        print("\n" + "-" * 80)
        print("ğŸ’¡ æç¤ºï¼šæ‚¨å¯ä»¥ç¹¼çºŒè¼¸å…¥æ–°çš„queryï¼Œæˆ–è¼¸å…¥ 'quit' é€€å‡º")
        print()

def main():
    """ä¸»ç¨‹åºå…¥å£"""
    try:
        # æª¢æŸ¥ä¾è³´
        print("ğŸ”§ æª¢æŸ¥ç³»çµ±ä¾è³´...")
        print(f"ğŸ“Š æª¢ç´¢æ¨¡å¼ï¼š{'å®Œæ•´æ¨¡å¼' if FULL_MODE else 'ç°¡åŒ–æ¨¡å¼'}")
        print(f"ğŸ—ï¸ çµæ§‹åŒ–è™•ç†å™¨ï¼š{'å¯ç”¨' if STRUCTURED_PROCESSOR_AVAILABLE else 'ä¸å¯ç”¨'}")
        print(f"ğŸ“ åŸºæœ¬æ¨™æº–åŒ–å™¨ï¼š{'å¯ç”¨' if BASIC_STANDARDIZER_AVAILABLE else 'ä¸å¯ç”¨'}")
        
        # å•Ÿå‹•äº’å‹•ç•Œé¢
        interactive_generate_lawsuit()
        
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç”¨æˆ¶ä¸­æ–·ï¼Œç¨‹åºé€€å‡º")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºåŸ·è¡ŒéŒ¯èª¤ï¼š{str(e)}")

if __name__ == "__main__":
    main()