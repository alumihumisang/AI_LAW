#!/usr/bin/env python3
"""
KG_700_CoT_Hybrid.py
æ··åˆæ¨¡å¼ï¼šäº‹å¯¦ã€æ³•æ¢å’Œæå®³ç”¨æ¨™æº–æ–¹æ³•ï¼Œçµè«–ç”¨CoT
å„ªåŒ–ç‰ˆæœ¬ï¼šä¿®æ­£æœªæˆå¹´èª¤åˆ¤ã€æ”¹é€²æå®³é …ç›®è™•ç†ã€å¢å¼·ç©©å®šæ€§
"""

import os
import re
import json
import requests
import time
import sys
from typing import List, Dict, Any, Optional
from collections import Counter

# å°å…¥å¿…è¦æ¨¡çµ„
try:
    import torch
    from transformers import AutoTokenizer, AutoModel
    from elasticsearch import Elasticsearch
    from neo4j import GraphDatabase
    from dotenv import load_dotenv
    FULL_MODE = True
    print("âœ… å®Œæ•´æ¨¡å¼ï¼šæ‰€æœ‰æª¢ç´¢åŠŸèƒ½å¯ç”¨")
except ImportError as e:
    print(f"âš ï¸ éƒ¨åˆ†æ¨¡çµ„æœªå®‰è£ï¼š{e}")
    print("âš ï¸ ä½¿ç”¨ç°¡åŒ–æ¨¡å¼ï¼ˆåƒ…LLMç”ŸæˆåŠŸèƒ½ï¼‰")
    FULL_MODE = False

# å°å…¥çµæ§‹åŒ–é‡‘é¡è™•ç†å™¨
try:
    from structured_legal_amount_processor import StructuredLegalAmountProcessor
    STRUCTURED_PROCESSOR_AVAILABLE = True
    print("âœ… çµæ§‹åŒ–é‡‘é¡è™•ç†å™¨è¼‰å…¥æˆåŠŸ")
except ImportError:
    STRUCTURED_PROCESSOR_AVAILABLE = False
    print("âš ï¸ çµæ§‹åŒ–é‡‘é¡è™•ç†å™¨æœªæ‰¾åˆ°")

# å°å…¥åŸºæœ¬é‡‘é¡æ¨™æº–åŒ–å™¨
try:
    from legal_amount_standardizer import LegalAmountStandardizer
    BASIC_STANDARDIZER_AVAILABLE = True
    print("âœ… åŸºæœ¬é‡‘é¡æ¨™æº–åŒ–å™¨è¼‰å…¥æˆåŠŸ")
except ImportError:
    BASIC_STANDARDIZER_AVAILABLE = False
    print("âš ï¸ åŸºæœ¬é‡‘é¡æ¨™æº–åŒ–å™¨æœªæ‰¾åˆ°")

# å°å…¥é€šç”¨æ ¼å¼è™•ç†å™¨
try:
    from universal_format_handler import UniversalFormatHandler
    UNIVERSAL_FORMAT_HANDLER_AVAILABLE = True
    print("âœ… é€šç”¨æ ¼å¼è™•ç†å™¨è¼‰å…¥æˆåŠŸ")
except ImportError:
    UNIVERSAL_FORMAT_HANDLER_AVAILABLE = False
    print("âš ï¸ é€šç”¨æ ¼å¼è™•ç†å™¨æœªæ‰¾åˆ°")

# ===== åŸºæœ¬è¨­å®š =====
LLM_URL = "http://localhost:11434/api/generate"
DEFAULT_MODEL = "gemma3:27b"

# ===== æª¢ç´¢ç³»çµ±è¨­å®š =====
if FULL_MODE:
    # è¼‰å…¥ç’°å¢ƒè®Šæ•¸
    env_path = os.path.join(os.path.dirname(__file__), '..', '01_è¨­å®šèˆ‡é…ç½®', '.env')
    load_dotenv(dotenv_path=env_path)
    
    # åµŒå…¥æ¨¡å‹è¨­å®š
    BERT_MODEL = "shibing624/text2vec-base-chinese"
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    try:
        TOKENIZER = AutoTokenizer.from_pretrained(BERT_MODEL)
        MODEL = AutoModel.from_pretrained(BERT_MODEL, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32).to(device)
        print("âœ… åµŒå…¥æ¨¡å‹è¼‰å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ åµŒå…¥æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        FULL_MODE = False
    
    # ES å’Œ Neo4j é€£æ¥
    try:
        # ä½¿ç”¨ requests ç›´æ¥èª¿ç”¨ ES API é¿å…ç‰ˆæœ¬å…¼å®¹æ€§å•é¡Œ
        ES_HOST = os.getenv("ELASTIC_HOST")
        ES_USER = os.getenv("ELASTIC_USER")
        ES_PASSWORD = os.getenv("ELASTIC_PASSWORD")
        ES_AUTH = (ES_USER, ES_PASSWORD)
        
        # æ¸¬è©¦ ES é€£æ¥
        response = requests.get(f"{ES_HOST}/_cluster/health", auth=ES_AUTH, verify=False)
        if response.status_code != 200:
            raise Exception(f"ESé€£æ¥å¤±æ•—: {response.status_code}")
        
        NEO4J_DRIVER = GraphDatabase.driver(
            os.getenv("NEO4J_URI"),
            auth=(os.getenv("NEO4J_USERNAME"), os.getenv("NEO4J_PASSWORD")),
        )
        CHUNK_INDEX = "legal_kg_chunks"
        print("âœ… è³‡æ–™åº«é€£æ¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ è³‡æ–™åº«é€£æ¥å¤±æ•—: {e}")
        FULL_MODE = False
else:
    ES_HOST = None
    ES_AUTH = None
    NEO4J_DRIVER = None
    CHUNK_INDEX = None

# æ¡ˆä»¶é¡å‹å°ç…§è¡¨ï¼ˆESæª¢ç´¢fallbackç”¨ï¼‰
CASE_TYPE_MAP = {
    # ç‰¹æ®Šæ¡ˆå‹å¦‚æœæ‰¾ä¸åˆ°ï¼Œfallbackåˆ°ç›¸é—œåŸºç¤é¡å‹
    "Â§190å‹•ç‰©æ¡ˆå‹": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    "Â§188åƒ±ç”¨äººæ¡ˆå‹": "å–®ç´”åŸè¢«å‘Šå„ä¸€", 
    "Â§187æœªæˆå¹´æ¡ˆå‹": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    
    # è¤‡åˆæ¡ˆå‹çš„fallback
    "åŸè¢«å‘Šçš†æ•¸å+Â§188åƒ±ç”¨äººæ¡ˆå‹": "Â§188åƒ±ç”¨äººæ¡ˆå‹",
    "æ•¸ååŸå‘Š+Â§188åƒ±ç”¨äººæ¡ˆå‹": "Â§188åƒ±ç”¨äººæ¡ˆå‹",
    "æ•¸åè¢«å‘Š+Â§188åƒ±ç”¨äººæ¡ˆå‹": "Â§188åƒ±ç”¨äººæ¡ˆå‹",
    "æ•¸åè¢«å‘Š+Â§187æœªæˆå¹´æ¡ˆå‹": "Â§187æœªæˆå¹´æ¡ˆå‹", 
    "åŸè¢«å‘Šçš†æ•¸å+Â§187æœªæˆå¹´æ¡ˆå‹": "Â§187æœªæˆå¹´æ¡ˆå‹",
    "åŸè¢«å‘Šçš†æ•¸å+Â§190å‹•ç‰©æ¡ˆå‹": "Â§190å‹•ç‰©æ¡ˆå‹",
    
    # åŸºç¤ç•¶äº‹äººæ•¸é‡é¡å‹ï¼ˆé€šå¸¸ä¸éœ€è¦fallbackï¼‰
    "æ•¸ååŸå‘Š": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    "æ•¸åè¢«å‘Š": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    "åŸè¢«å‘Šçš†æ•¸å": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
}

# ===== è¼”åŠ©å‡½æ•¸ =====
def extract_sections(text: str) -> dict:
    """æå–æ–‡æœ¬æ®µè½"""
    result = {
        "accident_facts": "",
        "injuries": "",
        "compensation_facts": ""
    }
    
    # äº‹æ•…ç™¼ç”Ÿç·£ç”±
    fact_match = re.search(r"ä¸€[ã€ï¼.\s]*äº‹æ•…ç™¼ç”Ÿç·£ç”±[:ï¼š]?\s*(.*?)(?=äºŒ[ã€ï¼.]|$)", text, re.S)
    if fact_match:
        result["accident_facts"] = fact_match.group(1).strip()
    
    # å—å‚·æƒ…å½¢
    injury_match = re.search(r"äºŒ[ã€ï¼.\s]*(?:åŸå‘Š)?å—å‚·æƒ…å½¢[:ï¼š]?\s*(.*?)(?=ä¸‰[ã€ï¼.]|$)", text, re.S)
    if injury_match:
        result["injuries"] = injury_match.group(1).strip()
    
    # è³ å„Ÿäº‹å¯¦æ ¹æ“š
    comp_match = re.search(r"ä¸‰[ã€ï¼.\s]*è«‹æ±‚è³ å„Ÿçš„äº‹å¯¦æ ¹æ“š[:ï¼š]?\s*(.*?)$", text, re.S)
    if comp_match:
        result["compensation_facts"] = comp_match.group(1).strip()
    
    return result

def extract_parties_with_llm(text: str) -> dict:
    """ä½¿ç”¨LLMæå–ç•¶äº‹äººï¼ˆå¢å¼·ç‰ˆ - æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›ï¼‰"""
    print("ğŸ¤– ä½¿ç”¨å¢å¼·ç‰ˆLLMæ™ºèƒ½æå–ç•¶äº‹äºº...")
    
    # å˜—è©¦ä½¿ç”¨èªç¾©è¼”åŠ©è™•ç†å™¨
    try:
        from KG_700_Semantic_Universal import SemanticLegalProcessor
        semantic_processor = SemanticLegalProcessor()
        semantic_result = semantic_processor.extract_parties_semantically(text)
        
        # è½‰æ›æ ¼å¼ä»¥ç¬¦åˆåŸæœ‰æ¥å£
        result = {}
        
        # è™•ç†åŸå‘Š - åŠ å…¥åš´æ ¼é©—è­‰é‚è¼¯
        plaintiffs = []
        for p in semantic_result.get("åŸå‘Š", []):
            if p.confidence > 0.3 and p.name != "åŸå‘Š":
                # åš´æ ¼é©—è­‰ï¼šå¿…é ˆç¢ºå¯¦æ˜¯ã€ŒåŸå‘Šâ—‹â—‹â—‹ã€çš„æ ¼å¼
                if f"åŸå‘Š{p.name}" in text:
                    plaintiffs.append(p.name)
                else:
                    print(f"âš ï¸ èªç¾©æå–çš„åŸå‘Šå§“å '{p.name}' åœ¨åŸæ–‡ä¸­ä¸å­˜åœ¨æˆ–ä¸æ˜¯åŸå‘Šï¼Œè·³é")
        
        result["åŸå‘Š"] = "ã€".join(plaintiffs) if plaintiffs else "åŸå‘Š"
        
        # è™•ç†è¢«å‘Š - åŠ å…¥åš´æ ¼é©—è­‰é‚è¼¯
        defendants = []
        for p in semantic_result.get("è¢«å‘Š", []):
            if p.confidence > 0.3 and p.name != "è¢«å‘Š":
                # åš´æ ¼é©—è­‰ï¼šå¿…é ˆç¢ºå¯¦æ˜¯ã€Œè¢«å‘Šâ—‹â—‹â—‹ã€çš„æ ¼å¼
                if f"è¢«å‘Š{p.name}" in text:
                    defendants.append(p.name)
                else:
                    print(f"âš ï¸ èªç¾©æå–çš„è¢«å‘Šå§“å '{p.name}' åœ¨åŸæ–‡ä¸­ä¸å­˜åœ¨æˆ–ä¸æ˜¯è¢«å‘Šï¼Œè·³é")
        
        result["è¢«å‘Š"] = "ã€".join(defendants) if defendants else "è¢«å‘Š"
        
        # é¡å¤–æª¢æŸ¥ï¼šç¢ºä¿æ²’æœ‰æŠŠè¨´å¤–äººèª¤èªç‚ºç•¶äº‹äºº
        for outsider in semantic_result.get("è¨´å¤–äºº", []):
            if outsider.name in result["åŸå‘Š"]:
                print(f"âš ï¸ æª¢æ¸¬åˆ°è¨´å¤–äºº '{outsider.name}' è¢«èª¤èªç‚ºåŸå‘Šï¼Œç§»é™¤")
                result["åŸå‘Š"] = result["åŸå‘Š"].replace(outsider.name, "").replace("ã€ã€", "ã€").strip("ã€") or "åŸå‘Š"
            if outsider.name in result["è¢«å‘Š"]:
                print(f"âš ï¸ æª¢æ¸¬åˆ°è¨´å¤–äºº '{outsider.name}' è¢«èª¤èªç‚ºè¢«å‘Šï¼Œç§»é™¤")
                result["è¢«å‘Š"] = result["è¢«å‘Š"].replace(outsider.name, "").replace("ã€ã€", "ã€").strip("ã€") or "è¢«å‘Š"
        
        print(f"âœ… èªç¾©è¼”åŠ©æå–æˆåŠŸ: åŸå‘Š={result['åŸå‘Š']}, è¢«å‘Š={result['è¢«å‘Š']}")
        
        # å¦‚æœèªç¾©æå–æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆå§“åï¼Œå‰‡fallbackåˆ°LLMæ–¹æ³•
        if result["åŸå‘Š"] == "åŸå‘Š" and result["è¢«å‘Š"] == "è¢«å‘Š":
            print("âš ï¸ èªç¾©æå–æœªæ‰¾åˆ°å…·é«”å§“åï¼Œfallbackåˆ°LLMæ–¹æ³•")
            # ä¸è¦è¿”å›ï¼Œç¹¼çºŒåŸ·è¡ŒLLMæ–¹æ³•
        else:
            return result
        
    except Exception as e:
        print(f"âš ï¸ èªç¾©è¼”åŠ©ä¸å¯ç”¨ï¼Œä½¿ç”¨åŸLLMæ–¹æ³•: {e}")
    
    # å‚™ç”¨ï¼šå¢å¼·çš„LLMæç¤ºè©
    prompt = f"""è«‹ä½ å¹«æˆ‘å¾ä»¥ä¸‹è»Šç¦æ¡ˆä»¶çš„æ³•å¾‹æ–‡ä»¶ä¸­æå–åŸå‘Šå’Œè¢«å‘Šçš„å§“åã€‚

ä»¥ä¸‹æ˜¯æ¡ˆä»¶å…§å®¹ï¼š
{text}

ã€é—œéµæå–è¦å‰‡ã€‘
1. **åªæå–**ã€ŒåŸå‘Šâ—‹â—‹â—‹ã€å’Œã€Œè¢«å‘Šâ—‹â—‹â—‹ã€ä¸­çš„å§“å
2. **çµ•å°ä¸è¦æå–**ã€Œè¨´å¤–äººâ—‹â—‹â—‹ã€çš„å§“å - è¨´å¤–äººä¸æ˜¯ç•¶äº‹äººï¼
3. **é‡è¦æª¢æŸ¥**ï¼šç¢ºä¿æå–çš„å§“åå‰é¢æœ‰ã€ŒåŸå‘Šã€æˆ–ã€Œè¢«å‘Šã€å­—æ¨£
4. å¦‚æœæ–‡ä¸­åªæœ‰ã€ŒåŸå‘Šã€ã€ã€Œè¢«å‘Šã€è€Œæ²’æœ‰å…·é«”å§“åï¼Œå°±è¼¸å‡ºã€ŒåŸå‘Šã€ã€ã€Œè¢«å‘Šã€
5. **ä¸è¦ææ··è§’è‰²**ï¼šåŸå‘Šå°±æ˜¯åŸå‘Šï¼Œè¢«å‘Šå°±æ˜¯è¢«å‘Š

ã€éŒ¯èª¤ç¯„ä¾‹ - çµ•å°ä¸è¦é€™æ¨£åšã€‘
âŒ æŠŠã€Œè¨´å¤–äººç‹å…¶èƒ½ã€ç•¶æˆåŸå‘Š - éŒ¯èª¤ï¼
âŒ æŠŠã€ŒåŸå‘Šã€é€™å€‹è©ç•¶æˆè¢«å‘Šå§“å - éŒ¯èª¤ï¼ 
âŒ æŠŠè§’è‰²æå - éŒ¯èª¤ï¼

ã€æ­£ç¢ºç¯„ä¾‹ã€‘
âœ… ã€ŒåŸå‘Šç¾…é–å´´ä¸»å¼µ...ã€â†’ åŸå‘Š:ç¾…é–å´´
âœ… ã€Œè¢«å‘Šå¼µä¸‰é§•é§›...ã€â†’ è¢«å‘Š:å¼µä¸‰
âœ… ã€Œè¨´å¤–äººç‹å…¶èƒ½è³¼è²·...ã€â†’ å¿½ç•¥ï¼ˆä¸æ˜¯ç•¶äº‹äººï¼‰
âœ… ã€ŒåŸå‘Šä¸»å¼µè³ å„Ÿ...ã€â†’ åŸå‘Š:åŸå‘Šï¼ˆæ²’æœ‰å…·é«”å§“åï¼‰

ã€ç‰¹æ®Šæ³¨æ„ã€‘
åœ¨é€™å€‹æ¡ˆä¾‹ä¸­ï¼Œæ–‡ä¸­æåˆ°ã€Œè¨´å¤–äººç‹å…¶èƒ½ã€ï¼Œé€™æ˜¯ç¬¬ä¸‰æ–¹ï¼Œä¸æ˜¯åŸå‘Šæˆ–è¢«å‘Šï¼
åŸå‘Šå’Œè¢«å‘Šåœ¨æ–‡ä¸­éƒ½æ²’æœ‰å…·é«”å§“åï¼Œåªç”¨ã€ŒåŸå‘Šã€ã€ã€Œè¢«å‘Šã€ç¨±å‘¼ã€‚

è«‹ä»”ç´°åˆ†æä¸¦è¼¸å‡ºï¼ˆåªè¼¸å‡ºé€™å…©è¡Œï¼‰ï¼š
åŸå‘Š:
è¢«å‘Š:"""

    try:
        # èª¿ç”¨LLM
        response = requests.post(
            LLM_URL,
            json={
                "model": DEFAULT_MODEL,
                "prompt": prompt,
                "stream": False
            },
            timeout=60
        )
        
        if response.status_code == 200:
            llm_result = response.json()["response"].strip()
            print(f"ğŸ¤– LLMæå–çµæœ: {llm_result}")
            return parse_llm_parties_result(llm_result)
        else:
            print(f"âŒ LLMèª¿ç”¨å¤±æ•—: {response.status_code}")
            return extract_parties_fallback(text)
            
    except Exception as e:
        print(f"âŒ LLMæå–ç•°å¸¸: {e}")
        return extract_parties_fallback(text)

def _is_valid_name(name: str) -> bool:
    """æª¢æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„å§“å"""
    import re
    
    # æ’é™¤åŒ…å«æ•¸å­—ã€è·æ¥­æè¿°ã€å¹´é½¡ç­‰çš„æ–‡å­—
    invalid_patterns = [
        r'\d+',  # åŒ…å«æ•¸å­—
        r'æ­²',   # å¹´é½¡
        r'å…ˆç”Ÿ|å¥³å£«|å°å§',  # ç¨±è¬‚
        r'ç¶“ç†|ä¸»ä»»|å¸æ©Ÿ|é ˜ç­|å“¡å·¥',  # è·æ¥­
        r'ä¼æ¥­|å…¬å¸|è¡Œè™Ÿ|åº—',  # å…¬å¸åç¨±
        r'ä¿‚|å³|ä¹‹|ç­‰|åŠ|æˆ–',  # é€£æ¥è©
        r'å—åƒ±äºº|åƒ±ç”¨äºº|æ³•å®šä»£ç†äºº',  # æ³•å¾‹ç”¨èª
    ]
    
    for pattern in invalid_patterns:
        if re.search(pattern, name):
            return False
    
    # æª¢æŸ¥æ˜¯å¦æ˜¯åˆç†çš„ä¸­æ–‡å§“åé•·åº¦ï¼ˆ2-4å€‹å­—ï¼‰
    if len(name) < 2 or len(name) > 6:
        return False
    
    # æª¢æŸ¥æ˜¯å¦ä¸»è¦ç”±ä¸­æ–‡å­—çµ„æˆ
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', name))
    if chinese_chars < len(name) * 0.7:  # è‡³å°‘70%æ˜¯ä¸­æ–‡å­—
        return False
    
    return True

def _clean_name_text(text: str) -> str:
    """æ¸…ç†å§“åæ–‡å­—ï¼Œç§»é™¤éå§“åå…§å®¹"""
    import re
    
    # ç§»é™¤å¹´é½¡ã€è·æ¥­ç­‰æè¿°
    cleaned = re.sub(r'\d+æ­²', '', text)
    cleaned = re.sub(r'å…ˆç”Ÿ|å¥³å£«|å°å§', '', cleaned)
    cleaned = re.sub(r'ï¼ˆ.*?ï¼‰|\(.*?\)', '', cleaned)  # ç§»é™¤æ‹¬è™Ÿå…§å®¹
    cleaned = re.sub(r'[ï¼Œ,]\s*\d+.*', '', cleaned)  # ç§»é™¤é€—è™Ÿå¾Œçš„æ•¸å­—å’Œæè¿°
    
    # æå–å¯èƒ½çš„å§“åï¼ˆ2-4å€‹ä¸­æ–‡å­—çš„çµ„åˆï¼‰
    name_matches = re.findall(r'[\u4e00-\u9fff]{2,4}', cleaned)
    if name_matches:
        return name_matches[0]  # è¿”å›ç¬¬ä¸€å€‹åŒ¹é…çš„å§“å
    
    return text.strip()

def parse_llm_parties_result(llm_result: str) -> dict:
    """è§£æLLMçš„ç•¶äº‹äººæå–çµæœ - å¢å¼·ç‰ˆæœ¬é˜²æ­¢è§’è‰²æ··æ·†"""
    result = {"åŸå‘Š": "åŸå‘Š", "è¢«å‘Š": "è¢«å‘Š", "è¢«å‘Šæ•¸é‡": 1, "åŸå‘Šæ•¸é‡": 1}
    
    # æª¢æŸ¥LLMæ˜¯å¦è¿”å›äº†ç„¡æ•ˆçš„å›æ‡‰
    invalid_responses = ["è«‹æä¾›", "ç„¡æ³•æå–", "æ²’æœ‰æä¾›", "ç”±æ–¼æ‚¨æ²’æœ‰"]
    if any(invalid in llm_result for invalid in invalid_responses):
        print("âš ï¸ LLMè¿”å›ç„¡æ•ˆå›æ‡‰ï¼Œä½¿ç”¨fallback")
        return result
    
    # æª¢æŸ¥æ˜¯å¦å‡ºç¾äº†è§’è‰²æ··æ·†çš„éŒ¯èª¤ï¼ˆå¸¸è¦‹bugï¼‰
    if "åŸå‘Š:ç‹å…¶èƒ½" in llm_result and "è¢«å‘Š:åŸå‘Š" in llm_result:
        print("âš ï¸ æª¢æ¸¬åˆ°æ˜é¡¯çš„è§’è‰²æ··æ·†éŒ¯èª¤ï¼Œä½¿ç”¨å®‰å…¨é»˜èªå€¼")
        return result
    
    # æª¢æŸ¥æ˜¯å¦æŠŠè¨´å¤–äººç•¶æˆäº†ç•¶äº‹äºº
    if "ç‹å…¶èƒ½" in llm_result:
        print("âš ï¸ æª¢æ¸¬åˆ°è¨´å¤–äººè¢«èª¤èªç‚ºç•¶äº‹äººï¼Œä½¿ç”¨å®‰å…¨é»˜èªå€¼")
        return result
    
    lines = llm_result.split('\n')
    
    for line in lines:
        line = line.strip()
        if line.startswith('åŸå‘Š:') or line.startswith('åŸå‘Šï¼š'):
            plaintiff_text = line.split(':', 1)[1].strip() if ':' in line else line.split('ï¼š', 1)[1].strip()
            if plaintiff_text:
                # æ”¹é€²çš„åˆ†å‰²é‚è¼¯ï¼šåªæœ‰ç•¶ç¢ºå¯¦æœ‰å¤šå€‹æœ‰æ•ˆå§“åæ™‚æ‰åˆ†å‰²
                potential_plaintiffs = [p.strip() for p in plaintiff_text.split(',') if p.strip()]
                # é©—è­‰æ˜¯å¦çœŸçš„æ˜¯å¤šå€‹å§“å
                valid_plaintiffs = []
                for p in potential_plaintiffs:
                    # æª¢æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„å§“åï¼ˆä¸åŒ…å«æ•¸å­—ã€ä¸æ˜¯æè¿°æ€§æ–‡å­—ï¼‰
                    if _is_valid_name(p):
                        valid_plaintiffs.append(p)
                
                if len(valid_plaintiffs) > 1:
                    result["åŸå‘Š"] = "ã€".join(valid_plaintiffs)
                    result["åŸå‘Šæ•¸é‡"] = len(valid_plaintiffs)
                else:
                    # å–®ä¸€åŸå‘Šæˆ–ç„¡æ³•ç¢ºå®šå¤šå€‹ï¼Œä½¿ç”¨åŸå§‹æ–‡å­—ä½†æ¸…ç†éå§“åå…§å®¹
                    cleaned_name = _clean_name_text(plaintiff_text)
                    result["åŸå‘Š"] = cleaned_name if cleaned_name else "åŸå‘Š"
                    result["åŸå‘Šæ•¸é‡"] = 1
        
        elif line.startswith('è¢«å‘Š:') or line.startswith('è¢«å‘Šï¼š'):
            defendant_text = line.split(':', 1)[1].strip() if ':' in line else line.split('ï¼š', 1)[1].strip()
            if defendant_text:
                # æ”¹é€²çš„åˆ†å‰²é‚è¼¯ï¼šåªæœ‰ç•¶ç¢ºå¯¦æœ‰å¤šå€‹æœ‰æ•ˆå§“åæ™‚æ‰åˆ†å‰²
                potential_defendants = [d.strip() for d in defendant_text.split(',') if d.strip()]
                # é©—è­‰æ˜¯å¦çœŸçš„æ˜¯å¤šå€‹å§“å
                valid_defendants = []
                for d in potential_defendants:
                    # æª¢æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„å§“å
                    if _is_valid_name(d):
                        valid_defendants.append(d)
                
                if len(valid_defendants) > 1:
                    result["è¢«å‘Š"] = "ã€".join(valid_defendants)
                    result["è¢«å‘Šæ•¸é‡"] = len(valid_defendants)
                else:
                    # å–®ä¸€è¢«å‘Šæˆ–ç„¡æ³•ç¢ºå®šå¤šå€‹ï¼Œä½¿ç”¨åŸå§‹æ–‡å­—ä½†æ¸…ç†éå§“åå…§å®¹
                    cleaned_name = _clean_name_text(defendant_text)
                    result["è¢«å‘Š"] = cleaned_name if cleaned_name else "è¢«å‘Š"
                    result["è¢«å‘Šæ•¸é‡"] = 1
    
    return result

def extract_parties_fallback(text: str) -> dict:
    """ç•¶LLMæå–å¤±æ•—æ™‚çš„fallbackæ–¹æ³•ï¼ˆç°¡åŒ–ç‰ˆæ­£å‰‡ï¼‰"""
    print("âš ï¸ ä½¿ç”¨fallbackæ–¹æ³•æå–ç•¶äº‹äºº...")
    result = {"åŸå‘Š": "åŸå‘Š", "è¢«å‘Š": "è¢«å‘Š", "è¢«å‘Šæ•¸é‡": 1, "åŸå‘Šæ•¸é‡": 1}
    
    # ç°¡åŒ–çš„æ­£å‰‡è¡¨é”å¼æå–
    plaintiffs = set()
    defendants = set()
    
    # åŸºæœ¬æ¨¡å¼
    plaintiff_patterns = [
        r'åŸå‘Š([\u4e00-\u9fff]{2,4})',
        r'åŸå‘Š([ç”²ä¹™ä¸™ä¸æˆŠ])'
    ]
    
    defendant_patterns = [
        r'è¢«å‘Š([\u4e00-\u9fff]{2,4})',
        r'è¢«å‘Š([ç”²ä¹™ä¸™ä¸æˆŠ])'
    ]
    
    for pattern in plaintiff_patterns:
        matches = re.findall(pattern, text)
        plaintiffs.update(matches)
    
    for pattern in defendant_patterns:
        matches = re.findall(pattern, text)
        defendants.update(matches)
    
    # æ¸…ç†å’Œçµ„åˆçµæœ
    if plaintiffs:
        result["åŸå‘Š"] = "ã€".join(sorted(plaintiffs))
        result["åŸå‘Šæ•¸é‡"] = len(plaintiffs)
    elif "åŸå‘Š" in text:
        result["åŸå‘Š"] = "åŸå‘Š"
    
    if defendants:
        result["è¢«å‘Š"] = "ã€".join(sorted(defendants))
        result["è¢«å‘Šæ•¸é‡"] = len(defendants)
    elif "è¢«å‘Š" in text:
        result["è¢«å‘Š"] = "è¢«å‘Š"
    
    return result

def extract_parties(text: str) -> dict:
    """ä¸»è¦çš„ç•¶äº‹äººæå–å‡½æ•¸ï¼ˆå„ªå…ˆä½¿ç”¨LLMï¼‰"""
    return extract_parties_with_llm(text)

# ===== æª¢ç´¢ç›¸é—œå‡½æ•¸ =====
def embed(text: str):
    """æ–‡å­—å‘é‡åŒ–"""
    if not FULL_MODE:
        return []
    
    t = TOKENIZER(text, truncation=True, padding="max_length", max_length=512, return_tensors="pt")
    t = {k: v.to(device) for k, v in t.items()}
    with torch.no_grad():
        vec = MODEL(**t).last_hidden_state.mean(dim=1).squeeze()
    return vec.cpu().numpy().tolist()

def es_search(query_vector, case_type: str, top_k: int = 3, label: str = "Facts", quiet: bool = False):
    """ES æœå°‹ï¼ˆå«fallbackæ©Ÿåˆ¶ï¼‰"""
    if not FULL_MODE or not ES_HOST:
        return []
    
    def _search(label_filter, case_type_filter):
        must_clause = [{"match": {"label": label_filter}}]
        
        if case_type_filter:
            # å˜—è©¦å¤šç¨®å¯èƒ½çš„ case_type æ¬„ä½æ ¼å¼
            case_type_options = [
                {"term": {"case_type.keyword": case_type_filter}},
                {"term": {"case_type": case_type_filter}},
                {"match": {"case_type": case_type_filter}}
            ]
            
            # ä½¿ç”¨ should æŸ¥è©¢ï¼Œä»»ä¸€ç¬¦åˆå³å¯
            must_clause.append({
                "bool": {
                    "should": case_type_options,
                    "minimum_should_match": 1
                }
            })
            
        body = {
            "size": top_k,
            "query": {
                "script_score": {
                    "query": {"bool": {"must": must_clause}},
                    "script": {
                        "source": "cosineSimilarity(params.qv,'embedding')+1.0",
                        "params": {"qv": query_vector},
                    },
                }
            },
        }
        
        # èª¿è©¦ä¿¡æ¯ï¼šåªåœ¨éå®‰éœæ¨¡å¼ä¸‹è¼¸å‡º
        if not quiet:
            print(f"ğŸ” ESæŸ¥è©¢æ¢ä»¶: index={CHUNK_INDEX}, label={label_filter}, case_type={case_type_filter}")
        
        try:
            url = f"{ES_HOST}/{CHUNK_INDEX}/_search"
            response = requests.post(url, auth=ES_AUTH, json=body, verify=False)
            if response.status_code == 200:
                result = response.json()
                hits = result["hits"]["hits"]
                total_docs = result["hits"]["total"]["value"] if isinstance(result["hits"]["total"], dict) else result["hits"]["total"]
                if not quiet:
                    print(f"ğŸ“Š ESæŸ¥è©¢çµæœ: æ‰¾åˆ° {len(hits)} å€‹åŒ¹é…çµæœï¼Œç¸½æ–‡æª”æ•¸: {total_docs}")
                return hits
            else:
                if not quiet:
                    print(f"âŒ ESæŸ¥è©¢å¤±æ•—: {response.status_code} - {response.text}")
                return []
        except Exception as e:
            if not quiet:
                print(f"âŒ ESæŸ¥è©¢å¤±æ•—: {e}")
            return []

    if not quiet:
        print(f"ğŸ” ä½¿ç”¨ case_type='{case_type}' æœç´¢ç›¸ä¼¼æ¡ˆä¾‹...")
    hits = _search(label, case_type)
    
    if not hits:
        # å…ˆæª¢æŸ¥ç´¢å¼•æ˜ å°„å’Œå¯ç”¨çš„æ¡ˆä»¶é¡å‹
        try:
            # æª¢æŸ¥ mapping
            mapping_url = f"{ES_HOST}/{CHUNK_INDEX}/_mapping"
            mapping_response = requests.get(mapping_url, auth=ES_AUTH, verify=False)
            if mapping_response.status_code == 200:
                mapping = mapping_response.json()
                properties = mapping[CHUNK_INDEX]["mappings"]["properties"]
                has_case_type = "case_type" in properties
                print(f"ğŸ—ºï¸ case_typeæ¬„ä½å­˜åœ¨: {has_case_type}")
                
                if has_case_type:
                    # å˜—è©¦ä¸åŒçš„æ¬„ä½åç¨±
                    for field_name in ["case_type", "case_type.keyword"]:
                        try:
                            check_body = {
                                "size": 0,
                                "aggs": {
                                    "case_type_count": {"terms": {"field": field_name, "size": 20}}
                                }
                            }
                            check_url = f"{ES_HOST}/{CHUNK_INDEX}/_search"
                            check_response = requests.post(check_url, auth=ES_AUTH, json=check_body, verify=False)
                            if check_response.status_code == 200:
                                check_result = check_response.json()
                                available_types = [bucket["key"] for bucket in check_result["aggregations"]["case_type_count"]["buckets"]]
                                print(f"ğŸ“‹ ä½¿ç”¨æ¬„ä½ {field_name} æ‰¾åˆ°çš„æ¡ˆä»¶é¡å‹: {available_types}")
                                break
                        except Exception as field_e:
                            print(f"âš ï¸ æ¬„ä½ {field_name} æŸ¥è©¢å¤±æ•—: {field_e}")
                else:
                    print("âŒ case_typeæ¬„ä½ä¸å­˜åœ¨æ–¼ç´¢å¼•æ˜ å°„ä¸­")
            else:
                print(f"âŒ ç²å–æ˜ å°„å¤±æ•—: {mapping_response.status_code}")
                
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•æª¢æŸ¥ç´¢å¼•æ˜ å°„æˆ–æ¡ˆä»¶é¡å‹: {e}")
        
        fallback = CASE_TYPE_MAP.get(case_type, "å–®ç´”åŸè¢«å‘Šå„ä¸€")
        if fallback != case_type:
            print(f"âš ï¸ ä½¿ç”¨ fallback='{fallback}' é‡æ–°æœå°‹...")
            hits = _search(label, fallback)
    
    if not hits:
        print("âš ï¸ ä¸é™æ¡ˆä»¶é¡å‹é€²è¡Œæœå°‹...")
        hits = _search(label, None)
    
    return hits

def rerank_case_ids_by_paragraphs(query_text: str, case_ids: List[str], label: str = "Facts", quiet: bool = False) -> List[str]:
    """æ ¹æ“šæ®µè½ç´šè³‡æ–™é‡æ–°æ’åºæ¡ˆä¾‹"""
    if not FULL_MODE or not ES_HOST:
        return case_ids
    
    if not quiet:
        print("ğŸ“˜ å•Ÿå‹•æ®µè½ç´š rerank...")
    try:
        from sklearn.metrics.pairwise import cosine_similarity
        import numpy as np
    except ImportError:
        if not quiet:
            print("âš ï¸ sklearnæœªå®‰è£ï¼Œè·³érerank")
        return case_ids

    query_vec = embed(query_text)
    if not query_vec:
        return case_ids
    
    query_vec_np = np.array(query_vec).reshape(1, -1)
    scored_cases = []
    
    for cid in case_ids:
        try:
            # ä½¿ç”¨ requests ç›´æ¥èª¿ç”¨ ES API
            search_body = {
                "query": {
                    "bool": {
                        "must": [
                            {"term": {"case_id": cid}},
                            {"term": {"label": label}}
                        ]
                    }
                },
                "size": 1
            }
            url = f"{ES_HOST}/legal_kg_paragraphs/_search"
            response = requests.post(url, auth=ES_AUTH, json=search_body, verify=False)
            if response.status_code != 200:
                continue
            res = response.json()
            hits = res.get("hits", {}).get("hits", [])
            if hits:
                vec = hits[0]['_source']['embedding']
                para_vec_np = np.array(vec).reshape(1, -1)
                score = cosine_similarity(query_vec_np, para_vec_np)[0][0]
                scored_cases.append((cid, score))
        except Exception as e:
            print(f"âš ï¸ Case {cid} rerankå¤±æ•—: {e}")
            scored_cases.append((cid, 0.0))

    scored_cases.sort(key=lambda x: x[1], reverse=True)
    return [cid for cid, _ in scored_cases]

def get_complete_cases_content(case_ids: List[str]) -> List[str]:
    """ç²å–å®Œæ•´æ¡ˆä¾‹å…§å®¹"""
    if not FULL_MODE or not NEO4J_DRIVER:
        return []
    
    complete_cases = []
    
    try:
        with NEO4J_DRIVER.session() as session:
            for case_id in case_ids:
                # æŸ¥è©¢å®Œæ•´æ¡ˆä¾‹çš„äº‹å¯¦æ®µè½
                result = session.run("""
                    MATCH (c:Case {case_id: $case_id})-[:åŒ…å«]->(f:Facts)
                    RETURN f.description AS facts_content
                """, case_id=case_id).data()
                
                if result:
                    # çµ„åˆæ¡ˆä¾‹çš„æ‰€æœ‰äº‹å¯¦æ®µè½
                    case_content = "\n".join([record["facts_content"] for record in result if record["facts_content"]])
                    if case_content:
                        complete_cases.append(case_content)
                else:
                    print(f"âš ï¸ æ¡ˆä¾‹ {case_id} ç„¡æ³•ç²å–å®Œæ•´å…§å®¹")
                    
    except Exception as e:
        print(f"âš ï¸ ç²å–å®Œæ•´æ¡ˆä¾‹å…§å®¹å¤±æ•—: {e}")
    
    return complete_cases

def query_laws(case_ids):
    """å¾Neo4jæŸ¥è©¢æ³•æ¢è³‡è¨Š"""
    if not FULL_MODE or not NEO4J_DRIVER:
        return Counter(), {}
    
    counter = Counter()
    law_text_map = {}
    
    try:
        with NEO4J_DRIVER.session() as session:
            for cid in case_ids:
                result = session.run("""
                    MATCH (c:Case {case_id: $cid})-[:åŒ…å«]->(:Facts)-[:é©ç”¨]->(l:Laws)-[:åŒ…å«]->(ld:LawDetail)
                    RETURN collect(distinct ld.name) AS law_names, collect(distinct ld.text) AS law_texts
                """, cid=cid).single()
                if result:
                    names = result["law_names"]
                    texts = result["law_texts"]
                    counter.update(names)
                    for n, t in zip(names, texts):
                        if n not in law_text_map:
                            law_text_map[n] = t
    except Exception as e:
        print(f"âš ï¸ Neo4jæŸ¥è©¢å¤±æ•—: {e}")
    
    return counter, law_text_map

def get_similar_cases_laws_stats(case_ids):
    """ç²å–ç›¸ä¼¼æ¡ˆä¾‹çš„æ³•æ¢çµ±è¨ˆè³‡è¨Š"""
    counter, _ = query_laws(case_ids)
    return counter.most_common()

def normalize_article_number(article: str) -> str:
    """æ¢è™Ÿæ ¼å¼æ¨™æº–åŒ–ï¼šç¬¬191-2æ¢ â†’ ç¬¬191æ¢ä¹‹2"""
    # è™•ç†ç‰¹æ®Šæ ¼å¼çš„æ¢è™Ÿ
    article = re.sub(r'ç¬¬(\d+)-(\d+)æ¢', r'ç¬¬\1æ¢ä¹‹\2', article)
    return article

def detect_special_relationships(text: str, parties: dict) -> dict:
    """åµæ¸¬ç‰¹æ®Šæ³•å¾‹é—œä¿‚ï¼ˆå„ªåŒ–ç‰ˆ - ä¿®å¾©ç•¶äº‹äººæ•¸é‡æª¢æ¸¬ï¼‰"""
    
    # æ­£ç¢ºè¨ˆç®—ç•¶äº‹äººæ•¸é‡
    plaintiff_count = len([name.strip() for name in parties.get('åŸå‘Š', 'åŸå‘Š').split('ã€') if name.strip() and name.strip() != 'åŸå‘Š'])
    defendant_count = len([name.strip() for name in parties.get('è¢«å‘Š', 'è¢«å‘Š').split('ã€') if name.strip() and name.strip() != 'è¢«å‘Š'])
    
    # å¦‚æœæ²’æœ‰å…·é«”å§“åï¼Œç•¶äº‹äººæ•¸é‡ç‚º1
    if parties.get('åŸå‘Š', '') in ['åŸå‘Š', '']:
        plaintiff_count = 1
    if parties.get('è¢«å‘Š', '') in ['è¢«å‘Š', '']:
        defendant_count = 1
        
    print(f"ğŸ” ç•¶äº‹äººæ•¸é‡æª¢æ¸¬: åŸå‘Š={plaintiff_count}äºº, è¢«å‘Š={defendant_count}äºº")
    
    relationships = {
        "æœªæˆå¹´": False,
        "é›‡å‚­é—œä¿‚": False,
        "å‹•ç‰©æå®³": False,
        "å¤šè¢«å‘Š": defendant_count > 1,
        "å¤šåŸå‘Š": plaintiff_count > 1
    }
    
    # æ›´ç²¾ç¢ºçš„æœªæˆå¹´æª¢æ¸¬
    # 1. æ˜ç¢ºæåˆ°æœªæˆå¹´ç›¸é—œè©å½™
    explicit_minor_keywords = ["æœªæˆå¹´", "æ³•å®šä»£ç†äºº", "ç›£è­·äºº", "æœªæ»¿åå…«æ­²", "æœªæ»¿18æ­²"]
    if any(keyword in text for keyword in explicit_minor_keywords):
        relationships["æœªæˆå¹´"] = True
    
    # 2. æª¢æŸ¥å…·é«”å¹´é½¡ï¼ˆ18æ­²ä»¥ä¸‹ï¼‰
    age_pattern = r'(\d+)\s*æ­²'
    age_matches = re.findall(age_pattern, text)
    for age_str in age_matches:
        age = int(age_str)
        if age < 18:
            relationships["æœªæˆå¹´"] = True
            break
    
    # 3. å­¸æ ¡é—œéµå­—éœ€è¦æ›´è¬¹æ…
    school_keywords = ["åœ‹ä¸­ç”Ÿ", "åœ‹å°ç”Ÿ", "é«˜ä¸­ç”Ÿ"]  # ä¸æ˜¯å–®ç´”çš„"åœ‹ä¸­"ã€"é«˜ä¸­"
    if any(keyword in text for keyword in school_keywords):
        relationships["æœªæˆå¹´"] = True
    
    # æª¢æŸ¥é›‡å‚­é—œä¿‚
    employment_keywords = ["å—åƒ±", "åƒ±ç”¨", "é›‡ä¸»", "å“¡å·¥", "è·å‹™", "å·¥ä½œæ™‚é–“", "å…¬å¸è»Š", "åŸ·è¡Œè·å‹™"]
    relationships["é›‡å‚­é—œä¿‚"] = any(keyword in text for keyword in employment_keywords)
    
    # æª¢æŸ¥å‹•ç‰©æå®³
    animal_keywords = ["ç‹—", "è²“", "çŠ¬", "å‹•ç‰©", "å¯µç‰©", "å’¬å‚·", "æŠ“å‚·"]
    relationships["å‹•ç‰©æå®³"] = any(keyword in text for keyword in animal_keywords)
    
    return relationships

def determine_case_type(accident_facts: str, parties: dict) -> str:
    """åˆ¤æ–·æ¡ˆä»¶é¡å‹ï¼ˆä¸ƒå¤§åŸºæœ¬é¡å‹ï¼‰"""
    relationships = detect_special_relationships(accident_facts, parties)
    
    # å„ªå…ˆåˆ¤æ–·ç‰¹æ®Šæ³•æ¢é¡å‹ï¼ˆäº’æ–¥å„ªå…ˆç´šï¼‰
    if relationships["å‹•ç‰©æå®³"]:
        return "Â§190å‹•ç‰©æ¡ˆå‹"
    elif relationships["é›‡å‚­é—œä¿‚"]: 
        return "Â§188åƒ±ç”¨äººæ¡ˆå‹"
    elif relationships["æœªæˆå¹´"]:
        return "Â§187æœªæˆå¹´æ¡ˆå‹"
    
    # å…¶æ¬¡åˆ¤æ–·ç•¶äº‹äººæ•¸é‡é¡å‹
    elif relationships["å¤šåŸå‘Š"] and relationships["å¤šè¢«å‘Š"]:
        return "åŸè¢«å‘Šçš†æ•¸å"
    elif relationships["å¤šåŸå‘Š"]:
        return "æ•¸ååŸå‘Š"
    elif relationships["å¤šè¢«å‘Š"]:
        return "æ•¸åè¢«å‘Š"
    else:
        return "å–®ç´”åŸè¢«å‘Šå„ä¸€"

def determine_applicable_laws(accident_facts: str, injuries: str, comp_facts: str, parties: dict) -> List[str]:
    """æ ¹æ“šæ¡ˆä»¶äº‹å¯¦æ™ºèƒ½åˆ¤æ–·é©ç”¨æ³•æ¢"""
    applicable_laws = []
    
    # åµæ¸¬ç‰¹æ®Šé—œä¿‚
    relationships = detect_special_relationships(accident_facts + injuries + comp_facts, parties)
    
    # 1. ç¬¬184æ¢ç¬¬1é …å‰æ®µ - åŸºæœ¬ä¾µæ¬Šè²¬ä»»ï¼ˆå¿…é ˆï¼‰
    applicable_laws.append("æ°‘æ³•ç¬¬184æ¢ç¬¬1é …å‰æ®µ")
    
    # 2. è»Šç¦æ¡ˆä»¶ - ç¬¬191æ¢ä¹‹2ï¼ˆäº¤é€šå·¥å…·ï¼‰
    traffic_keywords = ["æ±½è»Š", "æ©Ÿè»Š", "è»Šè¼›", "é§•é§›", "äº¤é€š", "æ’", "ç¢°æ’"]
    if any(keyword in accident_facts for keyword in traffic_keywords):
        applicable_laws.append("æ°‘æ³•ç¬¬191æ¢ä¹‹2")
    
    # 3. èº«é«”å¥åº·æå®³ - ç¬¬193æ¢ç¬¬1é …
    health_damage_keywords = ["é†«ç™‚", "çœ‹è­·", "å·¥ä½œæå¤±", "è–ªè³‡", "æ”¶å…¥", "å‹å‹•èƒ½åŠ›"]
    if injuries or any(keyword in comp_facts for keyword in health_damage_keywords):
        applicable_laws.append("æ°‘æ³•ç¬¬193æ¢ç¬¬1é …")
    
    # 4. ç²¾ç¥æ…°æ’«é‡‘ - ç¬¬195æ¢ç¬¬1é …å‰æ®µ
    mental_damage_keywords = ["ç²¾ç¥", "æ…°æ’«", "ç—›è‹¦", "åè­½", "äººæ ¼"]
    if any(keyword in comp_facts for keyword in mental_damage_keywords):
        applicable_laws.append("æ°‘æ³•ç¬¬195æ¢ç¬¬1é …å‰æ®µ")
    
    # 5. ç‰¹æ®Šæƒ…æ³è™•ç†ï¼ˆäº’æ–¥è¦å‰‡ï¼‰
    if relationships["é›‡å‚­é—œä¿‚"]:
        # é›‡å‚­é—œä¿‚å„ªå…ˆé©ç”¨ç¬¬188æ¢ç¬¬1é …æœ¬æ–‡
        applicable_laws.append("æ°‘æ³•ç¬¬188æ¢ç¬¬1é …æœ¬æ–‡")
    elif relationships["å¤šè¢«å‘Š"]:
        # å¤šè¢«å‘Šä½†ç„¡é›‡å‚­é—œä¿‚æ™‚é©ç”¨ç¬¬185æ¢ç¬¬1é …
        applicable_laws.append("æ°‘æ³•ç¬¬185æ¢ç¬¬1é …")
    
    # 6. æœªæˆå¹´æ¡ˆä»¶ - ç¬¬187æ¢ç¬¬1é …
    if relationships["æœªæˆå¹´"]:
        applicable_laws.append("æ°‘æ³•ç¬¬187æ¢ç¬¬1é …")
    
    # 7. å‹•ç‰©æå®³ - ç¬¬190æ¢ç¬¬1é …
    if relationships["å‹•ç‰©æå®³"]:
        applicable_laws.append("æ°‘æ³•ç¬¬190æ¢ç¬¬1é …")
    
    # æ¨™æº–åŒ–æ¢è™Ÿæ ¼å¼
    applicable_laws = [normalize_article_number(law) for law in applicable_laws]
    
    return list(dict.fromkeys(applicable_laws))  # å»é‡ä½†ä¿æŒé †åº


    

# ===== æ··åˆæ¨¡å¼ç”Ÿæˆå™¨ =====
class HybridCoTGenerator:
    """æ··åˆæ¨¡å¼ç”Ÿæˆå™¨ï¼šäº‹å¯¦æ³•æ¢æå®³ç”¨æ¨™æº–ï¼Œçµè«–ç”¨CoT"""
    
    def __init__(self, model_name: str = DEFAULT_MODEL):
        self.model_name = model_name
        self.llm_url = LLM_URL
        
        # åˆå§‹åŒ–é‡‘é¡è™•ç†å™¨
        if STRUCTURED_PROCESSOR_AVAILABLE:
            self.structured_processor = StructuredLegalAmountProcessor()
        else:
            self.structured_processor = None
        
        if BASIC_STANDARDIZER_AVAILABLE:
            self.basic_standardizer = LegalAmountStandardizer()
        else:
            self.basic_standardizer = None
        
        # åˆå§‹åŒ–é€šç”¨æ ¼å¼è™•ç†å™¨
        if UNIVERSAL_FORMAT_HANDLER_AVAILABLE:
            self.format_handler = UniversalFormatHandler()
        else:
            self.format_handler = None
        
        # æª¢æŸ¥LLMé€£æ¥
        self.llm_available = self._check_llm_connection()
    
    def _check_llm_connection(self) -> bool:
        """æª¢æŸ¥LLMé€£æ¥"""
        try:
            response = requests.get("http://localhost:11434/api/version", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def call_llm(self, prompt: str, timeout: int = 180) -> str:
        """èª¿ç”¨LLM"""
        if not self.llm_available:
            return "âŒ LLMæœå‹™ä¸å¯ç”¨"
        
        try:
            response = requests.post(
                self.llm_url,
                json={
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False
                },
                timeout=timeout
            )
            
            if response.status_code == 200:
                return response.json()["response"].strip()
            else:
                return f"âŒ LLM APIéŒ¯èª¤: {response.status_code}"
                
        except Exception as e:
            return f"âŒ LLMèª¿ç”¨å¤±æ•—: {str(e)}"
    
    def _chinese_num(self, num: int) -> str:
        """æ•¸å­—è½‰ä¸­æ–‡"""
        chinese = ["", "ä¸€", "äºŒ", "ä¸‰", "å››", "äº”", "å…­", "ä¸ƒ", "å…«", "ä¹", "å"]
        if num <= 10:
            return chinese[num]
        return str(num)
    
    def _extract_all_plaintiffs(self, text: str) -> List[str]:
        """æå–æ‰€æœ‰åŸå‘Šå§“å"""
        plaintiffs = []
        
        # æ–¹æ³•1ï¼šå¾æ–‡æœ¬ä¸­æ‰¾ã€ŒåŸå‘Šâ—‹â—‹â—‹ã€çš„æ¨¡å¼
        pattern = r'åŸå‘Š([^ï¼Œã€‚ï¼›ã€\s]{2,4})(?:ç‚º|å› |å—|å‰å¾€|æ”¯å‡º)'
        matches = re.findall(pattern, text)
        plaintiffs.extend(matches)
        
        # å»é‡ä¸¦ä¿æŒé †åº
        seen = set()
        unique_plaintiffs = []
        for p in plaintiffs:
            if p not in seen:
                seen.add(p)
                unique_plaintiffs.append(p)
        
        return unique_plaintiffs
    
    def generate_standard_facts(self, accident_facts: str, similar_cases: List[str] = None, parties: dict = None) -> str:
        """æ¨™æº–æ–¹å¼ç”Ÿæˆäº‹å¯¦æ®µè½ï¼ˆå«ç›¸ä¼¼æ¡ˆä¾‹åƒè€ƒï¼‰"""
        print("ğŸ“ ä½¿ç”¨æ¨™æº–æ–¹å¼ç”Ÿæˆäº‹å¯¦æ®µè½...")
        
        # çµ„åˆç›¸ä¼¼æ¡ˆä¾‹åƒè€ƒ
        reference_text = ""
        if similar_cases:
            reference_text = "\n\nåƒè€ƒç›¸ä¼¼æ¡ˆä¾‹ï¼š\n" + "\n".join([f"{i+1}. {case}" for i, case in enumerate(similar_cases[:2])])
        
        prompt = f"""ä½ æ˜¯å°ç£å¾‹å¸«ï¼Œè«‹æ ¹æ“šä»¥ä¸‹äº‹å¯¦ææ–™æ’°å¯«èµ·è¨´ç‹€çš„äº‹å¯¦æ®µè½ï¼š

äº‹å¯¦ææ–™ï¼š
{accident_facts}{reference_text}

è¦æ±‚ï¼š
1. ä»¥ã€Œç·£è¢«å‘Šã€é–‹é ­
2. ä½¿ç”¨ã€ŒåŸå‘Šã€ã€ã€Œè¢«å‘Šã€ç¨±è¬‚ï¼Œä½†å¿…é ˆä¿æŒå§“åçš„å®Œæ•´æ€§å’Œæº–ç¢ºæ€§
3. å®¢è§€æè¿°äº‹æ•…ç¶“éï¼Œç¢ºä¿èªæ³•æ­£ç¢ºæµæš¢
4. åƒè€ƒç›¸ä¼¼æ¡ˆä¾‹çš„æ•˜è¿°æ–¹å¼ï¼Œä½†ä¸å¾—æŠ„è¥²
5. æ ¼å¼ï¼šä¸€ã€äº‹å¯¦æ¦‚è¿°ï¼š[å…§å®¹]
6. **é‡è¦**ï¼šå¦‚æœäº‹å¯¦ææ–™ä¸­æœ‰å…·é«”å§“åï¼Œè«‹å®Œæ•´ä¿ç•™ï¼Œä¸è¦æˆªæ–·æˆ–æ”¹è®Šä»»ä½•å­—å…ƒ
7. **ç¦æ­¢äº‹é …**ï¼šçµ•å°ä¸å¯ä»¥åœ¨è¼¸å‡ºä¸­åŒ…å«ä»»ä½•æ‹¬è™Ÿæé†’æ–‡å­—ï¼Œå¦‚ã€Œï¼ˆå§“åï¼šè«‹å¡«å¯«...ï¼‰ã€ã€ã€Œï¼ˆè«‹å¡«å¯«...ï¼‰ã€ç­‰æç¤ºå…§å®¹
8. **ç›´æ¥è¼¸å‡º**ï¼šåªè¼¸å‡ºå®Œæ•´çš„äº‹å¯¦æ®µè½ï¼Œä¸è¦åŒ…å«ä»»ä½•éœ€è¦ç”¨æˆ¶å¡«å¯«çš„ç©ºç™½æˆ–æé†’
9. **èªæ³•è¦æ±‚**ï¼šè«‹ç‰¹åˆ¥æ³¨æ„å¥å­çµæ§‹çš„å®Œæ•´æ€§ï¼Œé¿å…å‡ºç¾èªæ³•éŒ¯èª¤æˆ–ä¸å®Œæ•´çš„å¥å­

è«‹ç›´æ¥è¼¸å‡ºå®Œæ•´çš„äº‹å¯¦æ®µè½ï¼š"""
        
        result = self.call_llm(prompt)
        
        # æ¸…ç†æ‹¬è™Ÿæé†’æ–‡å­—
        result = self._remove_bracket_reminders(result)
        
        # ä¿®æ­£èªæ³•éŒ¯èª¤
        result = self._fix_grammar_errors(result)
        
        # æå–äº‹å¯¦æ®µè½
        fact_match = re.search(r"ä¸€ã€äº‹å¯¦æ¦‚è¿°ï¼š\s*(.*?)(?:\n\n|$)", result, re.S)
        if fact_match:
            cleaned_content = fact_match.group(1).strip()
            result = f"ä¸€ã€äº‹å¯¦æ¦‚è¿°ï¼š\n{cleaned_content}"
            return self._standardize_names_in_facts(result, parties or {})
        elif "ç·£è¢«å‘Š" in result:
            # æ‰¾åˆ°åŒ…å«"ç·£è¢«å‘Š"çš„è¡Œ
            for line in result.split('\n'):
                if "ç·£è¢«å‘Š" in line:
                    cleaned_line = line.strip()
                    result = f"ä¸€ã€äº‹å¯¦æ¦‚è¿°ï¼š\n{cleaned_line}"
                    return self._standardize_names_in_facts(result, parties or {})
        
        # Fallback
        facts_content = accident_facts.replace('ç·£è¢«å‘Š', '').strip()
        result = f"ä¸€ã€äº‹å¯¦æ¦‚è¿°ï¼š\nç·£è¢«å‘Š{facts_content}"
        
        # çµ±ä¸€å§“åè™•ç†
        return self._standardize_names_in_facts(result, parties or {})
    
    def generate_standard_laws(self, accident_facts: str, injuries: str, parties: dict, compensation_facts: str = "") -> str:
        """æ¨™æº–æ–¹å¼ç”Ÿæˆæ³•å¾‹ä¾æ“šï¼ˆç¬¦åˆæ³•æ¢å¼•ç”¨è¦ç¯„ï¼‰"""
        print("âš–ï¸ ä½¿ç”¨æ¨™æº–æ–¹å¼ç”Ÿæˆæ³•å¾‹ä¾æ“š...")
        
        # æ™ºèƒ½åˆ¤æ–·é©ç”¨æ³•æ¢
        applicable_laws = determine_applicable_laws(accident_facts, injuries, compensation_facts, parties)
        
        # å®Œæ•´çš„æ³•æ¢èªªæ˜å°ç…§è¡¨ï¼ˆç²¾ç¢ºåˆ°é …ã€æ®µã€ä½†æ›¸ï¼‰
        law_descriptions = {
            "æ°‘æ³•ç¬¬184æ¢ç¬¬1é …å‰æ®µ": "å› æ•…æ„æˆ–éå¤±ï¼Œä¸æ³•ä¾µå®³ä»–äººä¹‹æ¬Šåˆ©è€…ï¼Œè² æå®³è³ å„Ÿè²¬ä»»ã€‚",
            "æ°‘æ³•ç¬¬185æ¢ç¬¬1é …": "æ•¸äººå…±åŒä¸æ³•ä¾µå®³ä»–äººä¹‹æ¬Šåˆ©è€…ï¼Œé€£å¸¶è² æå®³è³ å„Ÿè²¬ä»»ã€‚",
            "æ°‘æ³•ç¬¬187æ¢ç¬¬1é …": "ç„¡è¡Œç‚ºèƒ½åŠ›äººæˆ–é™åˆ¶è¡Œç‚ºèƒ½åŠ›äººï¼Œä¸æ³•ä¾µå®³ä»–äººä¹‹æ¬Šåˆ©è€…ï¼Œä»¥è¡Œç‚ºæ™‚æœ‰è­˜åˆ¥èƒ½åŠ›ç‚ºé™ï¼Œèˆ‡å…¶æ³•å®šä»£ç†äººé€£å¸¶è² æå®³è³ å„Ÿè²¬ä»»ã€‚",
            "æ°‘æ³•ç¬¬188æ¢ç¬¬1é …æœ¬æ–‡": "å—åƒ±äººå› åŸ·è¡Œè·å‹™ï¼Œä¸æ³•ä¾µå®³ä»–äººä¹‹æ¬Šåˆ©è€…ï¼Œç”±åƒ±ç”¨äººèˆ‡è¡Œç‚ºäººé€£å¸¶è² æå®³è³ å„Ÿè²¬ä»»ã€‚",
            "æ°‘æ³•ç¬¬190æ¢ç¬¬1é …": "å‹•ç‰©åŠ æå®³æ–¼ä»–äººè€…ï¼Œç”±å…¶å æœ‰äººè² æå®³è³ å„Ÿè²¬ä»»ã€‚",
            "æ°‘æ³•ç¬¬191æ¢ä¹‹2": "æ±½è»Šã€æ©Ÿè»Šæˆ–å…¶ä»–éä¾è»Œé“è¡Œé§›ä¹‹å‹•åŠ›è»Šè¼›ï¼Œåœ¨ä½¿ç”¨ä¸­åŠ æå®³æ–¼ä»–äººè€…ï¼Œé§•é§›äººæ‡‰è³ å„Ÿå› æ­¤æ‰€ç”Ÿä¹‹æå®³ã€‚",
            "æ°‘æ³•ç¬¬193æ¢ç¬¬1é …": "ä¸æ³•ä¾µå®³ä»–äººä¹‹èº«é«”æˆ–å¥åº·è€…ï¼Œå°æ–¼è¢«å®³äººå› æ­¤å–ªå¤±æˆ–æ¸›å°‘å‹å‹•èƒ½åŠ›æˆ–å¢åŠ ç”Ÿæ´»ä¸Šä¹‹éœ€è¦æ™‚ï¼Œæ‡‰è² æå®³è³ å„Ÿè²¬ä»»ã€‚",
            "æ°‘æ³•ç¬¬195æ¢ç¬¬1é …å‰æ®µ": "ä¸æ³•ä¾µå®³ä»–äººä¹‹èº«é«”ã€å¥åº·ã€åè­½ã€è‡ªç”±ã€ä¿¡ç”¨ã€éš±ç§ã€è²æ“ï¼Œæˆ–ä¸æ³•ä¾µå®³å…¶ä»–äººæ ¼æ³•ç›Šè€Œæƒ…ç¯€é‡å¤§è€…ï¼Œè¢«å®³äººé›–éè²¡ç”¢ä¸Šä¹‹æå®³ï¼Œäº¦å¾—è«‹æ±‚è³ å„Ÿç›¸ç•¶ä¹‹é‡‘é¡ã€‚"
        }
        
        # çµ„åˆæ³•æ¢å…§å®¹ï¼ˆå…ˆåˆ—æ³•æ¢å…§å®¹ï¼‰
        law_texts = []
        valid_laws = []
        
        for law in applicable_laws:
            if law in law_descriptions:
                law_texts.append(f"ã€Œ{law_descriptions[law]}ã€")
                valid_laws.append(law)
        
        if not law_texts:
            # Fallbackï¼šè‡³å°‘åŒ…å«åŸºæœ¬ä¾µæ¬Šæ¢æ–‡
            law_texts = ["ã€Œå› æ•…æ„æˆ–éå¤±ï¼Œä¸æ³•ä¾µå®³ä»–äººä¹‹æ¬Šåˆ©è€…ï¼Œè² æå®³è³ å„Ÿè²¬ä»»ã€‚ã€"]
            valid_laws = ["æ°‘æ³•ç¬¬184æ¢ç¬¬1é …å‰æ®µ"]
        
        # æŒ‰æ¢è™Ÿæ•¸å­—æ’åºæ³•æ¢
        sorted_laws_with_content = self._sort_laws_by_article_number(valid_laws, law_descriptions)
        
        # æå–æ’åºå¾Œçš„å…§å®¹å’Œæ¢è™Ÿ
        sorted_law_texts = [item[1] for item in sorted_laws_with_content]
        sorted_article_list = [item[0] for item in sorted_laws_with_content]
        
        # æŒ‰æ­£ç¢ºæ ¼å¼çµ„åˆï¼šå…ˆæ³•æ¢å…§å®¹ï¼Œå¾Œæ¢è™Ÿ
        law_content_block = "ã€".join(sorted_law_texts)
        article_list = "ã€".join(sorted_article_list)
        
        print(f"âœ… é©ç”¨æ³•æ¢: {', '.join(sorted_article_list)}")
        
        return f"""äºŒã€æ³•å¾‹ä¾æ“šï¼š
æŒ‰{law_content_block}ï¼Œ{article_list}åˆ†åˆ¥å®šæœ‰æ˜æ–‡ã€‚æŸ¥è¢«å‘Šå› ä¸Šé–‹ä¾µæ¬Šè¡Œç‚ºï¼Œè‡´åŸå‘Šå—æœ‰ä¸‹åˆ—æå®³ï¼Œä¾å‰æ­è¦å®šï¼Œè¢«å‘Šæ‡‰è² æå®³è³ å„Ÿè²¬ä»»ï¼š"""
    
    def _parse_damage_from_sentence(self, sentence: str, plaintiff: str) -> List[dict]:
        """å¾å¥å­ä¸­è§£ææå®³é …ç›®ï¼ˆæ”¹é€²ç‰ˆï¼‰"""
        damages = []
        
        # æ“´å……æå®³é¡å‹æ¨¡å¼
        damage_patterns = [
            {
                'keywords': ['é†«ç™‚è²»ç”¨', 'æ²»ç™‚', 'é†«é™¢', 'è¨ºæ‰€', 'å°±è¨º'],
                'name': 'é†«ç™‚è²»ç”¨',
                'template': f'åŸå‘Š{plaintiff}å› æœ¬æ¬¡äº‹æ•…å—å‚·ï¼Œå‰å¾€é†«é™¢æ²»ç™‚æ‰€æ”¯å‡ºä¹‹é†«ç™‚è²»ç”¨'
            },
            {
                'keywords': ['äº¤é€šè²»', 'äº¤é€šè²»ç”¨', 'å¾€è¿”è²»ç”¨', 'ä¾†å›è²»ç”¨', 'è»Šè²»', 'æ²¹è²»', 'åœè»Šè²»', 'éè·¯è²»', 'é€šè¡Œè²»'],
                'name': 'äº¤é€šè²»ç”¨',
                'template': f'åŸå‘Š{plaintiff}å› æœ¬æ¬¡äº‹æ•…æ‰€ç”Ÿä¹‹äº¤é€šè²»ç”¨'
            },
            {
                'keywords': ['å·¥è³‡æå¤±', 'ä¸èƒ½å·¥ä½œ', 'å·¥ä½œæå¤±', 'ç„¡æ³•å·¥ä½œ', 'ä¼‘é¤Š'],
                'name': 'å·¥ä½œæå¤±',
                'template': f'åŸå‘Š{plaintiff}å› æœ¬æ¬¡äº‹æ•…ç„¡æ³•å·¥ä½œä¹‹æ”¶å…¥æå¤±'
            },
            {
                'keywords': ['æ…°æ’«é‡‘', 'ç²¾ç¥'],
                'name': 'ç²¾ç¥æ…°æ’«é‡‘',
                'template': f'åŸå‘Š{plaintiff}å› æœ¬æ¬¡äº‹æ•…æ‰€å—ç²¾ç¥ä¸Šç—›è‹¦ä¹‹æ…°æ’«é‡‘'
            },
            {
                'keywords': ['è»Šè¼›è²¶å€¼', 'è²¶æ', 'åƒ¹å€¼æ¸›æ', 'äº¤æ˜“åƒ¹å€¼'],
                'name': 'è»Šè¼›è²¶å€¼æå¤±',
                'template': f'ç³»çˆ­è»Šè¼›å› æœ¬æ¬¡äº‹æ•…è²¶å€¼ä¹‹æå¤±'
            },
            {
                'keywords': ['é‘‘å®šè²»'],
                'name': 'é‘‘å®šè²»ç”¨',
                'template': f'ç‚ºè©•ä¼°è»Šè¼›æå¤±æ‰€æ”¯å‡ºä¹‹é‘‘å®šè²»ç”¨'
            }
        ]
        
        # æ›´ç²¾ç¢ºçš„é‡‘é¡æå–ï¼ˆè€ƒæ…®å‰å¾Œæ–‡ï¼‰
        # ä¸è¦åªçœ‹åˆ°é‡‘é¡å°±æå–ï¼Œè¦çœ‹é‡‘é¡å‰å¾Œçš„æè¿°
        
        return damages
    
    def generate_smart_compensation(self, injuries: str, comp_facts: str, parties: dict) -> str:
        """æ™ºèƒ½ç”Ÿæˆæå®³é …ç›®ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰"""
        print("ğŸ’° ç”Ÿæˆæå®³è³ å„Ÿ...")
        
        # ç›´æ¥ä½¿ç”¨LLMè™•ç†ï¼Œé¿å…è¤‡é›œçš„çµæ§‹åŒ–è™•ç†
        return self._generate_llm_based_compensation(comp_facts, parties)

    def _preprocess_chinese_numbers(self, text: str) -> str:
        """é è™•ç†ä¸­æ–‡æ•¸å­—ï¼Œè½‰æ›ç‚ºé˜¿æ‹‰ä¼¯æ•¸å­—"""
        import re
        
        # è™•ç† Xè¬Y,YYYå…ƒ æ ¼å¼ (å¦‚ï¼š26è¬4,379å…ƒ)
        pattern1 = r'(\d+)è¬(\d+,?\d+)å…ƒ'
        def replace1(match):
            wan = int(match.group(1))
            rest = int(match.group(2).replace(',', ''))
            total = wan * 10000 + rest
            return f"{total:,}å…ƒ"
        text = re.sub(pattern1, replace1, text)
        
        # è™•ç† Xè¬Yåƒå…ƒ æ ¼å¼ (å¦‚ï¼š30è¬5åƒå…ƒ)
        pattern2 = r'(\d+)è¬(\d+)åƒå…ƒ'
        def replace2(match):
            wan = int(match.group(1))
            qian = int(match.group(2))
            total = wan * 10000 + qian * 1000
            return f"{total:,}å…ƒ"
        text = re.sub(pattern2, replace2, text)
        
        # è™•ç† Xè¬å…ƒ æ ¼å¼ (å¦‚ï¼š20è¬å…ƒ)
        pattern3 = r'(\d+)è¬å…ƒ'
        def replace3(match):
            wan = int(match.group(1))
            total = wan * 10000
            return f"{total:,}å…ƒ"
        text = re.sub(pattern3, replace3, text)
        
        # è™•ç† Xåƒå…ƒ æ ¼å¼ (å¦‚ï¼š5åƒå…ƒ)
        pattern4 = r'(\d+)åƒå…ƒ'
        def replace4(match):
            qian = int(match.group(1))
            total = qian * 1000
            return f"{total:,}å…ƒ"
        text = re.sub(pattern4, replace4, text)
        
        return text
    
    def _remove_bracket_reminders(self, text: str) -> str:
        """ç§»é™¤æ–‡æœ¬ä¸­çš„æ‹¬è™Ÿæé†’æ–‡å­—"""
        import re
        
        # ç§»é™¤å„ç¨®æ‹¬è™Ÿæé†’æ¨¡å¼
        patterns = [
            r'\ï¼ˆ[^ï¼‰]*è«‹å¡«å¯«[^ï¼‰]*\ï¼‰',  # ï¼ˆå§“åï¼šè«‹å¡«å¯«...ï¼‰
            r'\ï¼ˆ[^ï¼‰]*è«‹[^ï¼‰]*\ï¼‰',     # ï¼ˆè«‹...ï¼‰
            r'\ï¼ˆ[^ï¼‰]*ï¼š[^ï¼‰]*\ï¼‰',     # ï¼ˆä»»ä½•ï¼šèªªæ˜ï¼‰
            r'\ï¼ˆ[^ï¼‰]*å¡«å¯«[^ï¼‰]*\ï¼‰',   # ï¼ˆ...å¡«å¯«...ï¼‰
            r'\ï¼ˆ[^ï¼‰]*è¼¸å…¥[^ï¼‰]*\ï¼‰',   # ï¼ˆ...è¼¸å…¥...ï¼‰
            r'\ï¼ˆ[^ï¼‰]*è£œå……[^ï¼‰]*\ï¼‰',   # ï¼ˆ...è£œå……...ï¼‰
        ]
        
        cleaned_text = text
        for pattern in patterns:
            cleaned_text = re.sub(pattern, '', cleaned_text)
        
        # æ¸…ç†å¤šé¤˜çš„ç©ºæ ¼
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
        
        return cleaned_text
    
    def _fix_grammar_errors(self, text: str) -> str:
        """ä¿®æ­£æ–‡æœ¬ä¸­çš„èªæ³•éŒ¯èª¤"""
        import re
        
        cleaned_text = text
        
        # ä¿®æ­£å¸¸è¦‹çš„èªæ³•éŒ¯èª¤æ¨¡å¼
        grammar_fixes = [
            # ä¿®æ­£ã€Œè‡´æ’æ“Šè»Šå¾Œä¹‹æ©Ÿè»Šé“ä¸Šï¼Œç”±åŸå‘Šæ‰€é¨ä¹˜...ã€é¡å‹çš„éŒ¯èª¤
            (r'è‡´æ’æ“Šè»Šå¾Œä¹‹æ©Ÿè»Šé“ä¸Šï¼Œç”±([^æ‰€]+)æ‰€é¨ä¹˜([^æ™®]+)æ™®é€šé‡å‹æ©Ÿè»Š', 
             r'è‡´æ’æ“Š\1æ‰€é¨ä¹˜ä¹‹\2æ™®é€šé‡å‹æ©Ÿè»Š'),
            
            # ä¿®æ­£å…¶ä»–å¸¸è¦‹çš„èªæ³•éŒ¯èª¤
            (r'ï¼Œç”±([^æ‰€]+)æ‰€é¨ä¹˜([^ä¹‹]+)ä¹‹([^ï¼Œã€‚]+)ï¼Œ', r'ï¼Œæ’æ“Š\1æ‰€é¨ä¹˜ä¹‹\2\3ï¼Œ'),
            
            # ä¿®æ­£å¥å­çµæ§‹ä¸å®Œæ•´çš„å•é¡Œ
            (r'è‡´æ’æ“Š([^ï¼Œã€‚]+)ï¼Œ$', r'è‡´æ’æ“Š\1ã€‚'),
            
            # ä¿®æ­£é‡è¤‡çš„ä»‹è©æˆ–é€£è©
            (r'ä¹‹ä¹‹', r'ä¹‹'),
            (r'æ–¼æ–¼', r'æ–¼'),
            (r'ï¼Œï¼Œ', r'ï¼Œ'),
            
            # ä¿®æ­£è»Šè¼›æè¿°çš„èªæ³•
            (r'è»Šå¾Œä¹‹æ©Ÿè»Šé“ä¸Šï¼Œç”±', r'è»Šè¼›ï¼Œè©²è»Šè¼›ç”±'),
            
            # ä¿®æ­£ä¸å®Œæ•´çš„å¥å­çµæ§‹
            (r'ç”±([^æ‰€]+)æ‰€é¨ä¹˜([^ï¼Œã€‚]+)ï¼Œ$', r'ç”±\1æ‰€é¨ä¹˜ä¹‹\2ã€‚'),
        ]
        
        for pattern, replacement in grammar_fixes:
            cleaned_text = re.sub(pattern, replacement, cleaned_text)
        
        # æ¸…ç†å¤šé¤˜çš„ç©ºæ ¼å’Œæ¨™é»
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
        cleaned_text = re.sub(r'ï¼Œã€‚', 'ã€‚', cleaned_text)
        cleaned_text = re.sub(r'ã€‚ã€‚', 'ã€‚', cleaned_text)
        
        return cleaned_text
    
    def _remove_conclusion_phrases(self, text: str) -> str:
        """ç§»é™¤æ–‡æœ¬ä¸­çš„çµè«–æ€§æ–‡å­—å’Œç¸½è¨ˆèªªæ˜"""
        import re
        
        # åˆ†è¡Œè™•ç†
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            
            # è·³éåŒ…å«çµè«–æ€§é—œéµè©çš„è¡Œï¼ˆä½†ä¸è¦èª¤åˆªç†ç”±èªªæ˜ï¼‰
            conclusion_keywords = [
                'ç¶œä¸Šæ‰€è¿°', 'ç¸½è¨ˆæ–°å°å¹£', 'åˆè¨ˆæ–°å°å¹£', 'æ³•å®šåˆ©æ¯', 
                'æŒ‰é€±å¹´åˆ©ç‡', 'æŒ‰å¹´æ¯', 'èµ·è¨´ç‹€ç¹•æœ¬é€é”',
                'æ¸…å„Ÿæ—¥æ­¢', 'è‡ª.*èµ·è‡³.*æ­¢', 'å¹´æ¯5%', 'ç¸½é¡ç‚º',
                'æ­¤æœ‰ç›¸é—œæ”¶æ“šå¯è­‰', 'æœ‰æ”¶æ“šç‚ºè­‰', 'æœ‰çµ±ä¸€ç™¼ç¥¨å¯è­‰',
                'ç¶“æŸ¥', 'æŸ¥æ˜', 'ç¶“å¯©ç†'
            ]
            
            should_skip = False
            for keyword in conclusion_keywords:
                if keyword in line:
                    should_skip = True
                    break
            
            if not should_skip and line:  # ä¿ç•™éç©ºä¸”éçµè«–æ€§çš„è¡Œ
                cleaned_lines.append(line)
        
        # é‡æ–°çµ„åˆ
        cleaned_text = '\n'.join(cleaned_lines)
        
        # é¡å¤–æ¸…ç†ï¼šç§»é™¤å¯èƒ½çš„çµè«–æ®µè½å’Œè­‰æ“šæ–‡å­—
        # ç§»é™¤å¾ã€Œç¶œä¸Šã€é–‹å§‹åˆ°çµå°¾çš„æ‰€æœ‰å…§å®¹
        cleaned_text = re.sub(r'ç¶œä¸Š.*$', '', cleaned_text, flags=re.MULTILINE | re.DOTALL)
        
        # ç§»é™¤å¥å­ä¸­çš„è­‰æ“šç›¸é—œæ–‡å­—
        evidence_patterns = [
            r'ï¼Œæ­¤æœ‰[^ã€‚]*å¯è­‰ã€‚?',
            r'ï¼Œæœ‰[^ã€‚]*æ”¶æ“š[^ã€‚]*è­‰ã€‚?',
            r'ï¼Œæœ‰[^ã€‚]*ç™¼ç¥¨[^ã€‚]*è­‰ã€‚?',
            r'ï¼Œ[^ã€‚]*ç‚ºè­‰ã€‚?'
        ]
        
        for pattern in evidence_patterns:
            cleaned_text = re.sub(pattern, 'ã€‚', cleaned_text)
        
        # æ¸…ç†å¤šé¤˜çš„å¥è™Ÿ
        cleaned_text = re.sub(r'ã€‚+', 'ã€‚', cleaned_text)
        
        return cleaned_text.strip()
    
    def _remove_defendant_damage_errors(self, text: str) -> str:
        """ç§»é™¤é—œæ–¼è¢«å‘Šæå®³çš„éŒ¯èª¤å…§å®¹"""
        import re
        
        # åˆ†è¡Œè™•ç†
        lines = text.split('\n')
        cleaned_lines = []
        skip_section = False
        
        for line in lines:
            line_stripped = line.strip()
            
            # æª¢æŸ¥æ˜¯å¦é–‹å§‹è¢«å‘Šæå®³æ®µè½
            if re.search(r'[ï¼ˆ(][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å][ï¼‰)].*è¢«å‘Š.*æå®³', line_stripped):
                print(f"ğŸ” æª¢æ¸¬åˆ°è¢«å‘Šæå®³éŒ¯èª¤æ®µè½ï¼Œé–‹å§‹è·³éï¼š{line_stripped}")
                skip_section = True
                continue
            
            # æª¢æŸ¥æ˜¯å¦é–‹å§‹æ–°çš„æ­£å¸¸æ®µè½ï¼ˆå¦‚ä¸‹ä¸€å€‹åŸå‘Šæˆ–å…¶ä»–æ®µè½ï¼‰
            if skip_section and (
                re.search(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€', line_stripped) or  # æ–°çš„å¤§æ¨™é¡Œ
                re.search(r'[ï¼ˆ(][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å][ï¼‰)](?!.*è¢«å‘Š)', line_stripped) or  # æ–°çš„é …ç›®ä½†ä¸æ˜¯è¢«å‘Š
                line_stripped.startswith('å››ã€') or  # çµè«–æ®µè½
                line_stripped == ''
            ):
                skip_section = False
            
            # å¦‚æœä¸åœ¨è·³éæ¨¡å¼ä¸­ï¼Œä¿ç•™é€™è¡Œ
            if not skip_section:
                # é¡å¤–æª¢æŸ¥ï¼šç§»é™¤ä»»ä½•ç›´æ¥æåˆ°è¢«å‘Šæå®³çš„è¡Œ
                if ('è¢«å‘Š' in line_stripped and 'æå®³' in line_stripped and 
                    ('ä¹‹æå®³' in line_stripped or 'çš„æå®³' in line_stripped)):
                    print(f"ğŸ” ç§»é™¤è¢«å‘Šæå®³ç›¸é—œè¡Œï¼š{line_stripped}")
                    continue
                
                # ç§»é™¤è§£é‡‹è¢«å‘Šç‚ºä»€éº¼æ²’æœ‰æå®³çš„ç„¡ç”¨æ–‡å­—
                if ('æœªæåŠè¢«å‘Š' in line_stripped or 
                    'æ•…æ­¤éƒ¨åˆ†ç„¡æå®³é …ç›®' in line_stripped or
                    'ç”±æ–¼åŸå§‹æè¿°åƒ…æä¾›' in line_stripped):
                    print(f"ğŸ” ç§»é™¤ç„¡ç”¨è§£é‡‹æ–‡å­—ï¼š{line_stripped}")
                    continue
                
                cleaned_lines.append(line)
        
        # é‡æ–°çµ„åˆ
        cleaned_text = '\n'.join(cleaned_lines)
        
        # æ¸…ç†å¤šé¤˜çš„ç©ºè¡Œ
        cleaned_text = re.sub(r'\n\s*\n\s*\n', '\n\n', cleaned_text)
        
        return cleaned_text.strip()
    
    def _final_format_validation(self, text: str, is_single_case: bool) -> str:
        """æœ€çµ‚æ ¼å¼é©—è­‰å’Œä¿®æ­£"""
        import re
        
        if not is_single_case:
            return text  # å¤šåŸå‘Šæ¡ˆä»¶ä¸éœ€è¦ç‰¹æ®Šè™•ç†
        
        # å–®ä¸€åŸå‘Šæ¡ˆä»¶çš„ç‰¹æ®Šæ ¼å¼ä¿®æ­£
        lines = text.split('\n')
        corrected_lines = []
        
        for line in lines:
            line_stripped = line.strip()
            
            # æª¢æ¸¬ä¸¦ä¿®æ­£ã€Œï¼ˆä¸€ï¼‰åŸå‘Šä¹‹æå®³ï¼šã€æ ¼å¼
            if re.match(r'^[ï¼ˆ(][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å][ï¼‰)].*åŸå‘Š.*ä¹‹æå®³ï¼š?$', line_stripped):
                print(f"ğŸ” æª¢æ¸¬åˆ°éŒ¯èª¤æ ¼å¼ï¼Œç§»é™¤ï¼š{line_stripped}")
                continue  # è·³éé€™ç¨®éŒ¯èª¤æ ¼å¼çš„è¡Œ
            
            # æª¢æ¸¬ä¸¦ä¿®æ­£åµŒå¥—çš„æå®³é …ç›®æ ¼å¼
            # å¦‚ï¼šã€Œ1. é†«ç™‚è²»ç”¨ï¼š182,690å…ƒã€â†’ã€Œï¼ˆä¸€ï¼‰é†«ç™‚è²»ç”¨ï¼š182,690å…ƒã€
            if re.match(r'^\d+\.\s*([^ï¼š]+)ï¼š([0-9,]+å…ƒ)', line_stripped):
                match = re.match(r'^\d+\.\s*([^ï¼š]+)ï¼š([0-9,]+å…ƒ)', line_stripped)
                if match:
                    item_name = match.group(1).strip()
                    amount = match.group(2).strip()
                    # è½‰æ›ç‚ºæ­£ç¢ºçš„ä¸­æ–‡ç·¨è™Ÿæ ¼å¼
                    # ç°¡å–®çš„æ•¸å­—è½‰æ›ï¼ˆ1â†’ä¸€ï¼Œ2â†’äºŒç­‰ï¼‰
                    chinese_nums = ['', 'ä¸€', 'äºŒ', 'ä¸‰', 'å››', 'äº”', 'å…­', 'ä¸ƒ', 'å…«', 'ä¹', 'å']
                    item_num = re.match(r'^(\d+)', line_stripped).group(1)
                    try:
                        num = int(item_num)
                        if num <= 10:
                            chinese_num = chinese_nums[num]
                            corrected_line = f"ï¼ˆ{chinese_num}ï¼‰{item_name}ï¼š{amount}"
                            print(f"ğŸ” æ ¼å¼ä¿®æ­£ï¼š{line_stripped} â†’ {corrected_line}")
                            corrected_lines.append(corrected_line)
                            continue
                    except:
                        pass
            
            corrected_lines.append(line)
        
        # é‡æ–°çµ„åˆä¸¦æ¸…ç†
        corrected_text = '\n'.join(corrected_lines)
        
        # ç¢ºä¿æ ¼å¼ä¸€è‡´æ€§
        if not corrected_text.startswith("ä¸‰ã€æå®³é …ç›®ï¼š"):
            corrected_text = "ä¸‰ã€æå®³é …ç›®ï¼š\n" + corrected_text
        
        return corrected_text.strip()
    
    def _clean_evidence_language(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬ä¸­çš„è­‰æ“šèªè¨€"""
        import re
        
        # ç§»é™¤è­‰æ“šç›¸é—œæ–‡å­—
        evidence_patterns = [
            r'ï¼Œæ­¤æœ‰[^ã€‚]*å¯è­‰ã€‚?',
            r'ï¼Œæœ‰[^ã€‚]*æ”¶æ“š[^ã€‚]*è­‰ã€‚?',
            r'ï¼Œæœ‰[^ã€‚]*ç™¼ç¥¨[^ã€‚]*è­‰ã€‚?',
            r'ï¼Œæœ‰[^ã€‚]*è­‰æ˜[^ã€‚]*è­‰ã€‚?',
            r'ï¼Œ[^ã€‚]*ç‚ºè­‰ã€‚?',
            r'æ­¤æœ‰[^ã€‚]*å¯è­‰ã€‚?',
            r'æœ‰[^ã€‚]*æ”¶æ“š[^ã€‚]*è­‰ã€‚?',
            r'æœ‰[^ã€‚]*ç™¼ç¥¨[^ã€‚]*è­‰ã€‚?',
            r'æœ‰[^ã€‚]*è­‰æ˜[^ã€‚]*è­‰ã€‚?',
            r'[^ã€‚]*ç‚ºè­‰ã€‚?'
        ]
        
        cleaned_text = text
        for pattern in evidence_patterns:
            cleaned_text = re.sub(pattern, 'ã€‚', cleaned_text)
        
        # æ¸…ç†å¤šé¤˜çš„å¥è™Ÿå’Œç©ºæ ¼
        cleaned_text = re.sub(r'ã€‚+', 'ã€‚', cleaned_text)
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text)
        cleaned_text = cleaned_text.strip().rstrip('ã€‚')
        
        return cleaned_text

    def _ensure_reason_completeness(self, text: str, original_facts: str) -> str:
        """ç¢ºä¿æ¯å€‹æå®³é …ç›®éƒ½æœ‰ç†ç”±èªªæ˜"""
        import re
        
        lines = text.split('\n')
        enhanced_lines = []
        
        # è§£æåŸå§‹æè¿°ï¼Œå»ºç«‹é …ç›®åˆ°ç†ç”±çš„å°æ‡‰
        reason_map = {}
        original_lines = original_facts.split('\n')
        
        current_item = None
        current_reason = []
        
        for line in original_lines:
            line = line.strip()
            if not line:
                continue
            
            # æª¢æŸ¥æ˜¯å¦æ˜¯æ–°çš„é …ç›®é–‹å§‹
            item_match = re.match(r'^\d+\.\s*([^ï¼š]+)ï¼š', line)
            if item_match:
                # ä¿å­˜å‰ä¸€å€‹é …ç›®çš„ç†ç”±
                if current_item and current_reason:
                    reason_text = ' '.join(current_reason)
                    # æ¸…ç†è­‰æ“šèªè¨€
                    reason_text = self._clean_evidence_language(reason_text)
                    reason_map[current_item] = reason_text
                
                # é–‹å§‹æ–°é …ç›®
                current_item = item_match.group(1).strip()
                current_reason = []
                
                # æª¢æŸ¥åŒä¸€è¡Œæ˜¯å¦æœ‰ç†ç”±
                reason_part = line.split('ï¼š', 1)[1] if 'ï¼š' in line else ''
                if reason_part and not re.match(r'^\d+[,\d]*å…ƒ', reason_part.strip()):
                    current_reason.append(reason_part.strip())
            else:
                # é€™æ˜¯ç†ç”±èªªæ˜è¡Œ
                if current_item and line:
                    current_reason.append(line)
        
        # ä¿å­˜æœ€å¾Œä¸€å€‹é …ç›®
        if current_item and current_reason:
            reason_text = ' '.join(current_reason)
            # æ¸…ç†è­‰æ“šèªè¨€
            reason_text = self._clean_evidence_language(reason_text)
            reason_map[current_item] = reason_text
        
        # è™•ç†è¼¸å‡ºæ–‡æœ¬ï¼Œè£œå……ç¼ºå¤±çš„ç†ç”±
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            enhanced_lines.append(lines[i])
            
            # æª¢æŸ¥æ˜¯å¦æ˜¯æå®³é …ç›®è¡Œ
            item_match = re.match(r'^[ï¼ˆ(][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å][ï¼‰)]([^ï¼š]+)ï¼š([0-9,]+å…ƒ)', line)
            if item_match:
                item_name = item_match.group(1).strip()
                amount = item_match.group(2).strip()
                
                # æª¢æŸ¥ä¸‹ä¸€è¡Œæ˜¯å¦æœ‰ç†ç”±
                has_reason = False
                if i + 1 < len(lines):
                    next_line = lines[i + 1].strip()
                    if (next_line and 
                        not re.match(r'^[ï¼ˆ(][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å][ï¼‰)]', next_line) and
                        len(next_line) > 10):
                        has_reason = True
                
                # å¦‚æœæ²’æœ‰ç†ç”±ï¼Œå¾åŸå§‹æè¿°ä¸­æ‰¾ç†ç”±ä¸¦æ·»åŠ 
                if not has_reason:
                    reason = None
                    
                    # ç²¾ç¢ºåŒ¹é…é …ç›®åç¨±
                    for key, value in reason_map.items():
                        if item_name in key or key in item_name:
                            reason = value
                            break
                    
                    # å¦‚æœæ‰¾ä¸åˆ°ç²¾ç¢ºåŒ¹é…ï¼Œç”¨ç°¡æ½”çš„é€šç”¨ç†ç”±
                    if not reason:
                        if 'é†«ç™‚' in item_name:
                            reason = f"åŸå‘Šå› æœ¬æ¬¡äº‹æ•…å—å‚·ï¼Œæ”¯å‡º{item_name}{amount}ã€‚"
                        elif 'çœ‹è­·' in item_name:
                            reason = f"åŸå‘Šå› æœ¬æ¬¡äº‹æ•…å—æœ‰é‡å‚·ï¼Œéœ€å°ˆäººçœ‹è­·ï¼Œæ”¯å‡º{item_name}{amount}ã€‚"
                        elif 'äº¤é€š' in item_name:
                            reason = f"åŸå‘Šå› æœ¬æ¬¡äº‹æ•…å—å‚·å°±é†«æœŸé–“ï¼Œæ”¯å‡º{item_name}{amount}ã€‚"
                        elif 'å·¥ä½œ' in item_name or 'æå¤±' in item_name:
                            reason = f"åŸå‘Šå› æœ¬æ¬¡äº‹æ•…å—å‚·ç„¡æ³•å·¥ä½œï¼Œå—æœ‰{item_name}{amount}ã€‚"
                        elif 'æ…°æ’«' in item_name:
                            reason = f"åŸå‘Šå› æœ¬æ¬¡äº‹æ•…å—æœ‰å‚·å®³ï¼Œé€ æˆèº«å¿ƒç—›è‹¦ï¼Œè«‹æ±‚{item_name}{amount}ã€‚"
                        else:
                            reason = f"åŸå‘Šå› æœ¬æ¬¡äº‹æ•…æ”¯å‡º{item_name}{amount}ã€‚"
                    
                    if reason:
                        # æ·»åŠ ç†ç”±è¡Œ
                        enhanced_lines.append(reason)
                        print(f"ğŸ” è£œå……ç†ç”±ï¼š{item_name} -> {reason}")
            
            i += 1
        
        return '\n'.join(enhanced_lines)
    
    def _sort_laws_by_article_number(self, laws: List[str], law_descriptions: dict) -> List[tuple]:
        """æŒ‰æ¢è™Ÿæ•¸å­—å¤§å°æ’åºæ³•æ¢ä¸¦è¿”å›(æ¢è™Ÿ, å…§å®¹)å…ƒçµ„åˆ—è¡¨"""
        import re
        
        def extract_article_number(law: str) -> tuple:
            """æå–æ¢è™Ÿæ•¸å­—ç”¨æ–¼æ’åº"""
            # åŒ¹é…å¦‚ï¼šæ°‘æ³•ç¬¬184æ¢ç¬¬1é …å‰æ®µ
            match = re.search(r'ç¬¬(\d+)æ¢(?:ä¹‹(\d+))?', law)
            if match:
                main_num = int(match.group(1))
                sub_num = int(match.group(2)) if match.group(2) else 0
                return (main_num, sub_num)
            return (999, 0)  # ç„¡æ³•è§£æçš„æ”¾åˆ°æœ€å¾Œ
        
        # å‰µå»ºåŒ…å«æ¢è™Ÿã€å…§å®¹å’Œæ’åºéµçš„åˆ—è¡¨
        law_items = []
        for law in laws:
            if law in law_descriptions:
                content = f"ã€Œ{law_descriptions[law]}ã€"
                sort_key = extract_article_number(law)
                law_items.append((law, content, sort_key))
        
        # æŒ‰æ¢è™Ÿæ•¸å­—æ’åº
        law_items.sort(key=lambda x: x[2])
        
        # è¿”å›(æ¢è™Ÿ, å…§å®¹)å…ƒçµ„åˆ—è¡¨
        return [(item[0], item[1]) for item in law_items]
    
    def _detect_structure_type(self, text: str) -> str:
        """æª¢æ¸¬æ–‡æœ¬çµæ§‹é¡å‹"""
        # æª¢æŸ¥çµæ§‹åŒ–æ¨¡å¼
        structured_patterns = [
            r'\d+\.\s*[^ï¼š:]+[ï¼š:]\s*[0-9,]+å…ƒ',  # 1. é …ç›®ï¼šé‡‘é¡
            r'ï¼ˆ[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ï¼‰.*?[ï¼š:]\s*[0-9,]+å…ƒ',  # ï¼ˆä¸€ï¼‰é …ç›®ï¼šé‡‘é¡
            r'èªªæ˜ï¼š.*?[0-9,]+å…ƒ'  # èªªæ˜ï¼š...é‡‘é¡
        ]
        
        structured_matches = sum(1 for pattern in structured_patterns 
                               if re.search(pattern, text))
        
        if structured_matches >= 2:
            return "structured"
        elif "å…ƒ" in text:
            return "semi_structured"
        else:
            return "unstructured"
    
    def _generate_structured_compensation(self, comp_facts: str, parties: dict) -> str:
        """ä½¿ç”¨çµæ§‹åŒ–è™•ç†å™¨ç”Ÿæˆæå®³é …ç›®"""
        print("ğŸ—ï¸ ä½¿ç”¨çµæ§‹åŒ–è™•ç†å™¨åˆ†ææå®³é …ç›®...")
        
        # ä½¿ç”¨çµæ§‹åŒ–è™•ç†å™¨
        result = self.structured_processor.process_structured_document(comp_facts)
        
        # æª¢æŸ¥æ˜¯å¦æœ‰è¨ˆç®—éŒ¯èª¤
        validation = result.get('validation', {})
        if validation.get('claimed_total') and not validation.get('match', True):
            print(f"âš ï¸ ç™¼ç¾è¨ˆç®—éŒ¯èª¤ï¼šåŸèµ·è¨´ç‹€è²ç¨±{validation['claimed_total']:,}å…ƒï¼Œå¯¦éš›æ‡‰ç‚º{validation['calculated_total']:,}å…ƒ")
            print(f"ğŸ“Š å·®é¡ï¼š{abs(validation['difference']):,}å…ƒ")
        
        # ç”Ÿæˆä¿®æ­£å¾Œçš„æå®³é …ç›®
        structured_text = "ä¸‰ã€æå®³é …ç›®ï¼š\n\n"
        
        # æŒ‰åŸå‘Šåˆ†çµ„é¡¯ç¤º
        current_plaintiff = None
        plaintiff_index = 0
        item_counter = 1
        
        for item in result['structured_items']:
            # åˆ¤æ–·æ˜¯å¦ç‚ºæ–°çš„åŸå‘Š
            item_title = item['item_title']
            if 'åŸå‘Š' in item_title:
                # æå–åŸå‘Šå§“å
                plaintiff_match = re.search(r'åŸå‘Š([^ä¹‹çš„]+)', item_title)
                if plaintiff_match:
                    plaintiff_name = plaintiff_match.group(1).strip()
                    if plaintiff_name != current_plaintiff:
                        current_plaintiff = plaintiff_name
                        plaintiff_index += 1
                        chinese_num = self._chinese_num(plaintiff_index)
                        structured_text += f"ï¼ˆ{chinese_num}ï¼‰åŸå‘Š{current_plaintiff}ä¹‹æå®³ï¼š\n"
                        item_counter = 1
            
            # æ·»åŠ æå®³é …ç›®
            structured_text += f"{item_counter}. {item['item_title']}ï¼š{item['formatted_amount']}\n"
            if item['description']:
                structured_text += f"   {item['description']}\n"
            item_counter += 1
            structured_text += "\n"
        
        # æ·»åŠ æˆ‘èªªæ˜
        total_amount = result['calculation']['total']
        structured_text += f"\nğŸ’° æå®³ç¸½è¨ˆï¼šæ–°å°å¹£{total_amount:,}å…ƒæ•´\n"
        
        # å¦‚æœæœ‰è¨ˆç®—éŒ¯èª¤ï¼Œæ·»åŠ èªªæ˜
        if validation.get('claimed_total') and not validation.get('match', True):
            structured_text += f"\nï¼ˆè¨»ï¼šç¶“é‡æ–°è¨ˆç®—ï¼Œæ­£ç¢ºç¸½é¡ç‚º{total_amount:,}å…ƒï¼Œ"
            if validation['difference'] > 0:
                structured_text += f"åŸèµ·è¨´ç‹€å°‘ç®—{validation['difference']:,}å…ƒï¼‰"
            else:
                structured_text += f"åŸèµ·è¨´ç‹€å¤šç®—{abs(validation['difference']):,}å…ƒï¼‰"
        
        return structured_text
    
    def generate_cot_conclusion_with_smart_amount_calculation(self, accident_facts: str, compensation_text: str, parties: dict) -> str:
        """ä½¿ç”¨æ™ºèƒ½é‡‘é¡è¨ˆç®—ç”ŸæˆCoTçµè«–ï¼ˆé˜²æ­¢é‡è¤‡å’ŒéŒ¯èª¤è¨ˆç®—ï¼‰"""
        print("ğŸ§  ç”ŸæˆCoTçµè«–ï¼ˆå«ç¸½é‡‘é¡è¨ˆç®—ï¼‰...")
        
        # çµ±ä¸€ä½¿ç”¨è©³ç´°æå®³é …ç›®æå–æ–¹æ³•ï¼ˆå·²é©—è­‰æº–ç¢ºï¼‰
        plaintiff_damages = self._extract_damage_items_from_text(compensation_text)
        
        # åŸºæ–¼è©³ç´°é …ç›®è¨ˆç®—ç¸½é‡‘é¡ï¼ˆç¢ºä¿ä¸€è‡´æ€§ï¼‰
        total_amount = 0
        amounts = []
        item_details = []
        
        print("ğŸ” ã€çµ±ä¸€é‡‘é¡è¨ˆç®—ã€‘åŸºæ–¼ç¬¬ä¸‰éƒ¨åˆ†è©³ç´°é …ç›®è¨ˆç®—ç¸½é¡...")
        for plaintiff, damages in plaintiff_damages.items():
            for damage in damages:
                total_amount += damage['amount']
                amounts.append(damage['amount'])
                item_details.append(f"{damage['name']}{damage['amount']:,}å…ƒ")
                print(f"   âœ… {plaintiff}: {damage['name']} {damage['amount']:,}å…ƒ")
        
        print(f"ğŸ” ã€çµ±ä¸€é‡‘é¡è¨ˆç®—ã€‘æœ€çµ‚ç¸½è¨ˆ: {total_amount:,}å…ƒ")
        
        # æ§‹å»ºåŒ…å«é‡‘é¡è¨ˆç®—çš„CoTæç¤ºè©
        plaintiff = parties.get("åŸå‘Š", "åŸå‘Š")
        defendant = parties.get("è¢«å‘Š", "è¢«å‘Š")
        plaintiff_count = parties.get('åŸå‘Šæ•¸é‡', 1)
        
        # æ§‹å»ºåŸå‘Šåˆ†çµ„ä¿¡æ¯
        plaintiff_details = ""
        if plaintiff_damages:
            plaintiff_details = "\nğŸ“‹ å„åŸå‘Šæå®³è©³æƒ…ï¼š"
            for plaintiff_name, damages in plaintiff_damages.items():
                plaintiff_details += f"\nâ€¢ åŸå‘Š{plaintiff_name}ï¼š"
                for damage in damages:
                    plaintiff_details += f" {damage['name']}{damage['amount']:,}å…ƒ"
        
        # æ ¹æ“šè¢«å‘Šæ•¸é‡æ±ºå®šè²¬ä»»ç”¨è©
        defendant_list = [name.strip() for name in parties.get('è¢«å‘Š', 'è¢«å‘Š').split('ã€') if name.strip() and name.strip() != 'è¢«å‘Š']
        defendant_count = len(defendant_list) if defendant_list else 1
        
        # å–®ä¸€è¢«å‘Šç”¨ã€Œè³ å„Ÿã€ï¼Œå¤šè¢«å‘Šç”¨ã€Œé€£å¸¶è³ å„Ÿã€
        liability_term = "é€£å¸¶è³ å„Ÿ" if defendant_count > 1 else "è³ å„Ÿ"
        
        prompt = f"""ä½ æ˜¯å°ç£è³‡æ·±å¾‹å¸«ï¼Œè«‹é‹ç”¨Chain of Thoughtæ¨ç†æ–¹å¼ç”Ÿæˆå°ˆæ¥­çš„èµ·è¨´ç‹€çµè«–æ®µè½ã€‚

ğŸ‘¥ ç•¶äº‹äººè³‡è¨Šï¼š
åŸå‘Šï¼š{plaintiff}ï¼ˆå…±{plaintiff_count}åï¼‰
è¢«å‘Šï¼š{defendant}ï¼ˆå…±{defendant_count}åï¼‰

ğŸ“„ æ¡ˆä»¶äº‹å¯¦ï¼š
{accident_facts}

ğŸ“„ æå®³è³ å„Ÿå…§å®¹ï¼š
{compensation_text}
{plaintiff_details}

ğŸ’° æ™ºèƒ½é‡‘é¡åˆ†æçµæœï¼š
æå–åˆ°çš„æœ‰æ•ˆæ±‚å„Ÿé‡‘é¡ï¼š{amounts}
æ­£ç¢ºç¸½è¨ˆï¼š{total_amount:,}å…ƒ

ğŸ§  è«‹ä½¿ç”¨Chain of Thoughtæ–¹å¼åˆ†æï¼š

æ­¥é©Ÿ1: åˆ†ææ¡ˆä»¶æ€§è³ªå’Œç•¶äº‹äººè²¬ä»»
æ­¥é©Ÿ2: å¾æå®³è³ å„Ÿå…§å®¹ä¸­è­˜åˆ¥å„åŸå‘Šçš„å…·é«”æå®³é …ç›®å’Œé‡‘é¡
æ­¥é©Ÿ3: ç¢ºä¿å§“åä¸€è‡´æ€§ï¼ˆèˆ‡äº‹å¯¦æ¦‚è¿°ä¸­çš„å§“åå®Œå…¨ä¸€è‡´ï¼‰
æ­¥é©Ÿ4: æŒ‰åŸå‘Šåˆ†çµ„åˆ—å‡ºå…·é«”æå®³é …ç›®ï¼Œå½¢æˆç²¾ç¢ºçš„çµè«–

ğŸ›ï¸ æœ€å¾Œè«‹ç”Ÿæˆå°ˆæ¥­çš„çµè«–æ®µè½ï¼Œæ ¼å¼è¦æ±‚ï¼š
- é–‹é ­ï¼šã€Œå››ã€çµè«–ï¼šã€
- æ ¼å¼ï¼šä¸€æ®µå¼é€£çºŒæ–‡å­—ï¼Œä¸è¦æ¢åˆ—å¼
- å…§å®¹çµæ§‹ï¼šã€Œç¶œä¸Šæ‰€é™³ï¼Œè¢«å‘Šæ‡‰{liability_term}åŸå‘Šä¹‹æå®³ï¼ŒåŒ…å«åŸå‘Š[å§“å1][é …ç›®1][é‡‘é¡1]å…ƒã€[é …ç›®2][é‡‘é¡2]å…ƒï¼›åŸå‘Š[å§“å2][é …ç›®1][é‡‘é¡1]å…ƒã€[é …ç›®2][é‡‘é¡2]å…ƒï¼Œç¸½è¨ˆ{total_amount:,}å…ƒï¼Œä¸¦è‡ªèµ·è¨´ç‹€å‰¯æœ¬é€é”ç¿Œæ—¥èµ·è‡³æ¸…å„Ÿæ—¥æ­¢ï¼ŒæŒ‰å¹´æ¯5%è¨ˆç®—ä¹‹åˆ©æ¯ã€‚ã€

âš ï¸ é‡è¦è¦æ±‚ï¼š
1. å¿…é ˆæŒ‰åŸå‘Šåˆ†çµ„ï¼Œæ¸…æ¥šåˆ—å‡ºæ¯ä½åŸå‘Šçš„å…·é«”æå®³é …ç›®å’Œé‡‘é¡
2. ç¢ºä¿åŸå‘Šå§“åèˆ‡äº‹å¯¦æ¦‚è¿°å®Œå…¨ä¸€è‡´
3. æ¯ä½åŸå‘Šçš„æå®³é …ç›®è¦æº–ç¢ºå°æ‡‰å…¶å¯¦éš›é‡‘é¡
4. å¿…é ˆä½¿ç”¨æä¾›çš„æ­£ç¢ºç¸½è¨ˆ{total_amount:,}å…ƒ
5. ç”¨ä¸€æ®µé€£çºŒæ–‡å­—ï¼Œä¸è¦åˆ†æ¢åˆ—å‡º
6. å¦‚æœ‰å¤šååŸå‘Šï¼Œç”¨åˆ†è™Ÿå€éš”ä¸åŒåŸå‘Šçš„æå®³é …ç›®
7. **é‡è¦**ï¼šé …ç›®åç¨±å¿…é ˆèˆ‡æå®³è³ å„Ÿæ®µè½å®Œå…¨ä¸€è‡´ï¼ˆå¦‚ï¼šæå®³æ®µè½å¯«ã€Œå·¥ä½œæå¤±ã€ï¼Œçµè«–å°±ç”¨ã€Œå·¥ä½œæå¤±ã€ï¼Œä¸å¯æ”¹ç‚ºã€Œå·¥è³‡æå¤±ã€ï¼‰
8. **é‡è¦**ï¼šè»Šè¼›ç›¸é—œé …ç›®ä½¿ç”¨çµ±ä¸€åç¨±ï¼ˆå¦‚ï¼šã€Œè»Šè¼›æå¤±ã€è€Œéã€Œè»Šè¼›è²¶å€¼æå¤±ã€æˆ–ã€Œé‘‘å®šè²»ç”¨ã€ï¼‰

è«‹ç›´æ¥è¼¸å‡ºçµè«–æ®µè½ï¼š"""

        result = self.call_llm(prompt, timeout=180)
        
        return result if result else "å››ã€çµè«–ï¼š\nï¼ˆLLMç”Ÿæˆå¤±æ•—ï¼Œè«‹æª¢æŸ¥è¼¸å…¥å…§å®¹ï¼‰"

    def generate_cot_conclusion_with_structured_analysis(self, accident_facts: str, compensation_text: str, parties: dict) -> str:
        """ä½¿ç”¨æ™ºèƒ½é‡‘é¡è¨ˆç®—ç”ŸæˆCoTçµè«–ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰"""
        print("ğŸ§  ç”ŸæˆCoTçµè«–ï¼ˆä½¿ç”¨æ™ºèƒ½é‡‘é¡è¨ˆç®—ï¼‰...")
        
        # ç›´æ¥ä½¿ç”¨æ™ºèƒ½é‡‘é¡è¨ˆç®—æ–¹å¼ï¼Œé¿å…è¤‡é›œçš„çµæ§‹åŒ–è™•ç†
        return self.generate_cot_conclusion_with_smart_amount_calculation(
            accident_facts, compensation_text, parties
        )
    
    def _build_structured_cot_prompt(self, accident_facts: str, compensation_text: str, parties: dict, analysis_result: dict) -> str:
        """æ§‹å»ºåŸºæ–¼çµæ§‹åŒ–åˆ†æçš„CoTæç¤ºè©"""
        
        plaintiff = parties.get("åŸå‘Š", "åŸå‘Š")
        defendant = parties.get("è¢«å‘Š", "è¢«å‘Š")
        
        # æå–çµæ§‹åŒ–åˆ†æçµæœ
        structured_items = analysis_result.get('structured_items', [])
        calculation = analysis_result.get('calculation', {})
        validation = analysis_result.get('validation', {})
        
        # æ§‹å»ºé …ç›®æ‘˜è¦
        items_summary = "\nğŸ“‹ å·²è­˜åˆ¥çš„æå®³é …ç›®ï¼š"
        for item in structured_items:
            items_summary += f"\nâ€¢ {item['item_title']}: {item['formatted_amount']}"
        
        items_summary += f"\n\nğŸ’° è¨ˆç®—åˆ†æï¼š"
        items_summary += f"\nâ€¢ æ­£ç¢ºç¸½è¨ˆ: {calculation.get('total', 0):,}å…ƒ"
        
        if validation.get('claimed_total'):
            items_summary += f"\nâ€¢ åŸèµ·è¨´ç‹€è²ç¨±: {validation['claimed_total']:,}å…ƒ"
            if validation.get('difference', 0) != 0:
                if validation['difference'] < 0:
                    items_summary += f"\nâ€¢ âŒ åŸèµ·è¨´ç‹€å°‘ç®—äº†: {abs(validation['difference']):,}å…ƒ"
                else:
                    items_summary += f"\nâ€¢ âŒ åŸèµ·è¨´ç‹€å¤šç®—äº†: {validation['difference']:,}å…ƒ"
                items_summary += f"\nâ€¢ âœ… è«‹ä½¿ç”¨æ­£ç¢ºé‡‘é¡: {calculation.get('total', 0):,}å…ƒ"
        
        prompt = f"""ä½ æ˜¯å°ç£è³‡æ·±å¾‹å¸«ï¼Œè«‹é‹ç”¨Chain of Thoughtæ¨ç†æ–¹å¼ï¼Œæ ¹æ“šçµæ§‹åŒ–åˆ†æçµæœç”Ÿæˆå°ˆæ¥­çš„èµ·è¨´ç‹€çµè«–æ®µè½ã€‚

ğŸ¯ é‡è¦æŒ‡ç¤ºï¼š
1. å¿…é ˆä½¿ç”¨æ­£ç¢ºçš„é‡‘é¡è¨ˆç®—ï¼Œé¿å…é‡è¤‡è¨ˆç®—èªªæ˜æ–‡å­—ä¸­çš„é‡‘é¡
2. ä½¿ç”¨é€æ­¥æ¨ç†æ–¹å¼åˆ†ææå®³é …ç›®
3. çµè«–å¿…é ˆåŒ…å«å®Œæ•´çš„é …ç›®æ˜ç´°
4. ç¸½é‡‘é¡å¿…é ˆæº–ç¢ºç„¡èª¤
5. æ¡ç”¨æ¨™æº–çš„æ³•å¾‹æ–‡æ›¸æ ¼å¼

ğŸ‘¥ ç•¶äº‹äººè³‡è¨Šï¼š
åŸå‘Šï¼š{plaintiff}
è¢«å‘Šï¼š{defendant}

ğŸ“„ æ¡ˆä»¶äº‹å¯¦ï¼š
{accident_facts}

ğŸ“„ æå®³è³ å„ŸåŸå§‹å…§å®¹ï¼š
{compensation_text}

{items_summary}

ğŸ§  è«‹ä½¿ç”¨Chain of Thoughtæ–¹å¼åˆ†æï¼š

æ­¥é©Ÿ1: åˆ†ææ¡ˆä»¶æ€§è³ªå’Œç•¶äº‹äººè²¬ä»»
æ­¥é©Ÿ2: æª¢è¦–å„é …æå®³çš„åˆç†æ€§å’Œæ³•å¾‹ä¾æ“š
æ­¥é©Ÿ3: é©—è­‰é‡‘é¡è¨ˆç®—çš„æº–ç¢ºæ€§
æ­¥é©Ÿ4: ç¶œåˆåˆ†æä¸¦å½¢æˆçµè«–

ğŸ›ï¸ æœ€å¾Œè«‹ç”Ÿæˆå°ˆæ¥­çš„çµè«–æ®µè½ï¼ŒåŒ…æ‹¬ï¼š
1. æå®³é …ç›®æ˜ç´°åˆ—è¡¨
2. æ­£ç¢ºçš„ç¸½é‡‘é¡è¨ˆç®—  
3. æ¨™æº–çš„çµè«–æ ¼å¼
4. åˆ©æ¯è¨ˆç®—æ¢æ¬¾

æ ¼å¼è¦æ±‚ï¼š
- é–‹é ­ï¼šã€Œå››ã€çµè«–ï¼šã€
- ä¸­é–“ï¼šåˆ—èˆ‰å„é …æå®³æ˜ç´°
- çµå°¾ï¼šç¸½è¨ˆé‡‘é¡å’Œåˆ©æ¯è«‹æ±‚
- âš ï¸ é‡è¦ï¼šé¿å…é‡è¤‡èªªæ˜åŒä¸€é …ç›®ï¼Œæ¯é …æå®³åªèªªæ˜ä¸€æ¬¡
- âš ï¸ é‡è¦ï¼šä¸è¦é‡è¤‡åˆ—èˆ‰é‡‘é¡ï¼Œä½¿ç”¨çµæ§‹åŒ–åˆ†æçš„æ­£ç¢ºç¸½é¡

é‡è¦ï¼šè«‹ç¢ºä¿é‡‘é¡è¨ˆç®—çµ•å°æ­£ç¢ºï¼Œä½¿ç”¨çµæ§‹åŒ–åˆ†æçš„æ­£ç¢ºç¸½é¡ï¼"""

        return prompt
    
    def _build_traditional_cot_prompt(self, accident_facts: str, compensation_text: str, parties: dict) -> str:
        """æ§‹å»ºå‚³çµ±CoTæç¤ºè©"""
        
        plaintiff = parties.get("åŸå‘Š", "åŸå‘Š")
        defendant = parties.get("è¢«å‘Š", "è¢«å‘Š")
        
        prompt = f"""ä½ æ˜¯å°ç£è³‡æ·±å¾‹å¸«ï¼Œè«‹é‹ç”¨Chain of Thoughtæ¨ç†æ–¹å¼ç”Ÿæˆå°ˆæ¥­çš„èµ·è¨´ç‹€çµè«–æ®µè½ã€‚

ğŸ‘¥ ç•¶äº‹äººè³‡è¨Šï¼š
åŸå‘Šï¼š{plaintiff}
è¢«å‘Šï¼š{defendant}

ğŸ“„ æ¡ˆä»¶äº‹å¯¦ï¼š
{accident_facts}

ğŸ“„ æå®³è³ å„Ÿå…§å®¹ï¼š
{compensation_text}

ğŸ§  è«‹ä½¿ç”¨Chain of Thoughtæ–¹å¼åˆ†æï¼š

æ­¥é©Ÿ1: åˆ†ææ¡ˆä»¶æ€§è³ªå’Œç•¶äº‹äººè²¬ä»»
æ­¥é©Ÿ2: æª¢è¦–å„é …æå®³çš„åˆç†æ€§å’Œæ³•å¾‹ä¾æ“š  
æ­¥é©Ÿ3: è¨ˆç®—ç¸½æå®³é‡‘é¡
æ­¥é©Ÿ4: ç¶œåˆåˆ†æä¸¦å½¢æˆçµè«–

ğŸ›ï¸ æœ€å¾Œè«‹ç”Ÿæˆå°ˆæ¥­çš„çµè«–æ®µè½ï¼Œæ ¼å¼è¦æ±‚ï¼š
- é–‹é ­ï¼šã€Œå››ã€çµè«–ï¼šã€
- ä¸­é–“ï¼šåˆ—èˆ‰å„é …æå®³æ˜ç´°
- çµå°¾ï¼šç¸½è¨ˆé‡‘é¡å’Œåˆ©æ¯è«‹æ±‚
- âš ï¸ é‡è¦ï¼šé¿å…é‡è¤‡èªªæ˜åŒä¸€é …ç›®ï¼Œæ¯é …æå®³åªèªªæ˜ä¸€æ¬¡
- âš ï¸ é‡è¦ï¼šä¸è¦é‡è¤‡åˆ—èˆ‰ç›¸åŒçš„é‡‘é¡å’Œé …ç›®"""

        return prompt
    
    def _post_process_structured_conclusion(self, conclusion: str, analysis_result: dict) -> str:
        """å¾Œè™•ç†çµæ§‹åŒ–çµè«–"""
        
        # æ·»åŠ è™•ç†è³‡è¨Š
        processing_info = f"\n\nğŸ’¡ è™•ç†è³‡è¨Šï¼š\n"
        processing_info += f"è™•ç†æ–¹æ³•ï¼šçµæ§‹åŒ–åˆ†æ\n"
        
        calculation = analysis_result.get('calculation', {})
        validation = analysis_result.get('validation', {})
        
        processing_info += f"æ­£ç¢ºç¸½é¡ï¼š{calculation.get('total', 0):,}å…ƒ\n"
        if validation.get('claimed_total'):
            processing_info += f"åŸè²ç¨±é¡ï¼š{validation['claimed_total']:,}å…ƒ\n"
            if validation.get('difference', 0) != 0:
                processing_info += f"å·®é¡ä¿®æ­£ï¼š{abs(validation['difference']):,}å…ƒ\n"
        
        return conclusion + processing_info

    def _generate_complex_compensation(self, comp_facts: str, parties: dict) -> str:
        """è™•ç†è¤‡é›œæå®³é …ç›®æ–‡æœ¬çš„åˆ†æ­¥æ–¹æ³•"""
        print("ğŸ”§ ä½¿ç”¨åˆ†æ­¥è™•ç†è¤‡é›œæå®³æ–‡æœ¬...")
        
        # æ­¥é©Ÿ1ï¼šé è™•ç†ä¸­æ–‡æ•¸å­—
        preprocessed_facts = self._preprocess_chinese_numbers(comp_facts)
        
        # ç›´æ¥æ ¼å¼åŒ–ç‚ºæ¨™æº–æå®³é …ç›®
        format_prompt = f"""è«‹å°‡ä»¥ä¸‹æå®³è³ å„Ÿå…§å®¹é‡æ–°æ•´ç†ç‚ºæ¨™æº–çš„æ³•å¾‹æ–‡æ›¸æ ¼å¼ï¼š

ã€ç•¶äº‹äººã€‘
åŸå‘Šï¼š{parties.get('åŸå‘Š', 'æœªæåŠ')}ï¼ˆå…±{parties.get('åŸå‘Šæ•¸é‡', 1)}åï¼‰

ã€æå®³æè¿°ã€‘
{preprocessed_facts}

ã€æ¨™æº–æ ¼å¼è¦æ±‚ã€‘
ä¸‰ã€æå®³é …ç›®ï¼š

ï¼ˆä¸€ï¼‰åŸå‘Š[å§“å]ä¹‹æå®³ï¼š
1. [é …ç›®åç¨±]ï¼š[é‡‘é¡]å…ƒ
   èªªæ˜ï¼šåŸå‘Š[å§“å]å› æœ¬æ¬¡è»Šç¦[æå®³æ€§è³ª]
2. [é …ç›®åç¨±]ï¼š[é‡‘é¡]å…ƒ
   èªªæ˜ï¼šåŸå‘Š[å§“å]å› æœ¬æ¬¡è»Šç¦[æå®³æ€§è³ª]

ï¼ˆäºŒï¼‰åŸå‘Š[å§“å]ä¹‹æå®³ï¼š
1. [é …ç›®åç¨±]ï¼š[é‡‘é¡]å…ƒ
   èªªæ˜ï¼šåŸå‘Š[å§“å]å› æœ¬æ¬¡è»Šç¦[æå®³æ€§è³ª]

ã€é‡è¦è¦æ±‚ã€‘
- ç›´æ¥æ•´ç†æå®³é …ç›®ï¼Œä¸è¦é¡¯ç¤ºåˆ†ææˆ–è¨ˆç®—éç¨‹
- å…±åŒè²»ç”¨è¦å¹³å‡åˆ†æ”¤çµ¦ç›¸é—œåŸå‘Š
- æ‰€æœ‰é‡‘é¡ä½¿ç”¨åƒåˆ†ä½é€—è™Ÿæ ¼å¼
- æ¯é …æå®³éƒ½è¦æœ‰å…·é«”èªªæ˜
- ç¢ºä¿æ ¼å¼æ•´é½Šçµ±ä¸€

è«‹ç›´æ¥è¼¸å‡ºæ¨™æº–æ ¼å¼çš„æå®³é …ç›®ï¼š"""

        return self.call_llm(format_prompt, timeout=120)

    def _verify_calculation(self, result_text: str) -> dict:
        """å¾çµæœä¸­æå–ä¸¦é©—è­‰è¨ˆç®—æº–ç¢ºæ€§"""
        import re
        
        verification = {
            "correct": True,
            "errors": [],
            "corrected_total": None
        }
        
        # æå–æ‰€æœ‰é‡‘é¡æ•¸å­—
        amounts = re.findall(r'(\d{1,3}(?:,\d{3})*)', result_text)
        amounts = [int(amt.replace(',', '')) for amt in amounts if amt]
        
        if len(amounts) >= 10:  # å¦‚æœæœ‰è¶³å¤ çš„é‡‘é¡é€²è¡Œé©—è­‰
            # å˜—è©¦æ‰¾åˆ°å…©å€‹å°è¨ˆå’Œç¸½è¨ˆ
            try:
                # å‡è¨­æœ€å¾Œä¸‰å€‹å¤§æ•¸å­—æ˜¯ï¼šå°è¨ˆ1ã€å°è¨ˆ2ã€ç¸½è¨ˆ
                if len(amounts) >= 3:
                    subtotal1 = amounts[-3]
                    subtotal2 = amounts[-2] 
                    reported_total = amounts[-1]
                    
                    # é©—è­‰ç¸½è¨ˆ
                    actual_total = subtotal1 + subtotal2
                    if actual_total != reported_total:
                        verification["correct"] = False
                        verification["errors"].append(f"ç¸½è¨ˆéŒ¯èª¤ï¼š{subtotal1} + {subtotal2} = {actual_total}ï¼Œä½†å ±å‘Šç‚º{reported_total}")
                        verification["corrected_total"] = actual_total
                        
            except Exception as e:
                verification["errors"].append(f"é©—è­‰éç¨‹å‡ºéŒ¯ï¼š{e}")
        
        return verification

    def _generate_llm_based_compensation(self, comp_facts: str, parties: dict) -> str:
        """ä½¿ç”¨LLMå®Œå…¨è™•ç†æå®³é …ç›®ç”Ÿæˆ - æ”¯æŒæ ¼å¼è‡ªé©æ‡‰"""
        
        # å…ˆé è™•ç†ä¸­æ–‡æ•¸å­—
        preprocessed_facts = self._preprocess_chinese_numbers(comp_facts)
        
        # æª¢æ¸¬è¼¸å…¥æ ¼å¼é¡å‹
        if self.format_handler:
            format_info = self.format_handler.detect_format(preprocessed_facts)
            detected_format = format_info.get('primary_format')
            confidence = format_info.get('confidence', 0.0)
            print(f"ğŸ” ã€æ ¼å¼æª¢æ¸¬ã€‘æª¢æ¸¬åˆ°æ ¼å¼: {detected_format} (ç½®ä¿¡åº¦: {confidence:.2f})")
        else:
            detected_format = None
            confidence = 0.0
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºå–®ä¸€åŸå‘Šæƒ…æ³ï¼ˆè¢«å‘Šæ•¸é‡ä¸å½±éŸ¿æ ¼å¼é¸æ“‡ï¼‰
        # æ­£ç¢ºè¨ˆç®—ç•¶äº‹äººæ•¸é‡
        plaintiff_list = [name.strip() for name in parties.get('åŸå‘Š', 'åŸå‘Š').split('ã€') if name.strip() and name.strip() != 'åŸå‘Š']
        defendant_list = [name.strip() for name in parties.get('è¢«å‘Š', 'è¢«å‘Š').split('ã€') if name.strip() and name.strip() != 'è¢«å‘Š']
        
        # å¦‚æœæ²’æœ‰å…·é«”å§“åï¼Œç•¶äº‹äººæ•¸é‡ç‚º1
        plaintiff_count = len(plaintiff_list) if plaintiff_list else 1
        defendant_count = len(defendant_list) if defendant_list else 1
        is_single_case = plaintiff_count == 1  # åªçœ‹åŸå‘Šæ•¸é‡
        
        # é™¤éŒ¯è¼¸å‡º
        print(f"ğŸ” DEBUG: parties = {parties}")
        print(f"ğŸ” DEBUG: plaintiff_list = {plaintiff_list}")
        print(f"ğŸ” DEBUG: plaintiff_count = {plaintiff_count}, defendant_count = {defendant_count}")
        print(f"ğŸ” DEBUG: is_single_case = {is_single_case}")
        
        # æ ¹æ“šæ ¼å¼æª¢æ¸¬çµæœé¸æ“‡åˆé©çš„æç¤ºè©ç­–ç•¥
        # å„ªå…ˆæª¢æŸ¥æ˜¯å¦ç‚ºå¤šåŸå‘Šæ¡ˆä¾‹
        if plaintiff_count > 1:
            # å¤šåŸå‘Šæ¡ˆä¾‹ - å¼·åˆ¶ä½¿ç”¨å¤šåŸå‘Šå°ˆé–€è™•ç†
            print(f"ğŸ” æª¢æ¸¬åˆ°å¤šåŸå‘Šæ¡ˆä¾‹: {plaintiff_count}ååŸå‘Šï¼Œä½¿ç”¨å¤šåŸå‘Šå°ˆé–€è™•ç†")
            prompt = self._build_adaptive_prompt(preprocessed_facts, parties, 'multi_plaintiff_narrative')
        elif detected_format == 'multi_plaintiff_narrative' or (format_info.get('is_multi_plaintiff', False) and plaintiff_count > 1):
            # å¤šåŸå‘Šæ¡ˆä¾‹ - ä½¿ç”¨å¤šåŸå‘Šå°ˆé–€è™•ç†
            prompt = self._build_adaptive_prompt(preprocessed_facts, parties, 'multi_plaintiff_narrative')
        elif detected_format == 'free_format' or confidence < 0.5:
            # è‡ªç”±æ ¼å¼æˆ–æ ¼å¼ä¸æ˜ç¢º - ä½¿ç”¨é©æ‡‰æ€§æç¤ºè©
            prompt = self._build_adaptive_prompt(preprocessed_facts, parties, detected_format)
        elif is_single_case:
            # å–®ä¸€åŸå‘Šæ™‚ï¼Œä½¿ç”¨ç°¡åŒ–çš„ä¸­æ–‡ç·¨è™Ÿæ ¼å¼ï¼ˆä¸è«–è¢«å‘Šæ•¸é‡ï¼‰
            prompt = f"""è«‹å°‡ä»¥ä¸‹æå®³å…§å®¹æ•´ç†æˆæå®³é …ç›®æ¸…å–®ï¼Œç†ç”±ç°¡æ½”æ˜ç­ã€‚

ã€åŸå§‹å…§å®¹ã€‘
{preprocessed_facts}

ã€è¼¸å‡ºæ ¼å¼ã€‘
ï¼ˆä¸€ï¼‰é …ç›®åç¨±ï¼šé‡‘é¡å…ƒ
åŸå‘Šå› æœ¬æ¬¡äº‹æ•…[ç°¡æ½”ç†ç”±]ï¼Œæ”¯å‡º/å—æœ‰[é …ç›®åç¨±][é‡‘é¡]å…ƒã€‚

ã€ç¯„ä¾‹ã€‘
ï¼ˆä¸€ï¼‰é†«ç™‚è²»ç”¨ï¼š182,690å…ƒ
åŸå‘Šå› æœ¬æ¬¡äº‹æ•…å—å‚·ï¼Œæ–¼å—é–€é†«é™¢ã€é›²æ—åŸºç£æ•™é†«é™¢å°±é†«æ²»ç™‚ï¼Œæ”¯å‡ºé†«ç™‚è²»ç”¨å…±è¨ˆ182,690å…ƒã€‚

ï¼ˆäºŒï¼‰çœ‹è­·è²»ç”¨ï¼š246,000å…ƒ
åŸå‘Šå› æœ¬æ¬¡äº‹æ•…å—æœ‰é‡å‚·ï¼Œéœ€å°ˆäººçœ‹è­·ï¼Œæ”¯å‡ºçœ‹è­·è²»ç”¨246,000å…ƒã€‚

ï¼ˆä¸‰ï¼‰å·¥ä½œæå¤±ï¼š485,000å…ƒ
åŸå‘Šæ–¼è»Šç¦ç™¼ç”Ÿæ™‚ä»»è·æ–¼å‡±æ’’å¤§é£¯åº—æˆ¿å‹™éƒ¨é ˜ç­ï¼Œå› æœ¬æ¬¡äº‹æ•…å—å‚·éœ€ä¼‘é¤Šä¸€å¹´ï¼Œè‡´ç„¡æ³•å·¥ä½œï¼Œå—æœ‰å·¥ä½œæå¤±485,000å…ƒã€‚

ã€è¦æ±‚ã€‘
- ç†ç”±1-2å¥è©±ï¼Œç°¡æ½”æ˜ç­
- ä¿ç•™é‡è¦é†«é™¢åç¨±ã€å·¥ä½œå–®ä½ç­‰é—œéµè³‡è¨Š
- çµ±ä¸€ä½¿ç”¨"æœ¬æ¬¡äº‹æ•…"è€Œé"æœ¬ä»¶è»Šç¦"
- å·¥ä½œæå¤±é …ç›®åç¨±ç›´æ¥ç”¨"å·¥ä½œæå¤±"
- ç²¾ç¥æ…°æ’«é‡‘é …ç›®åç¨±ç›´æ¥ç”¨"æ…°æ’«é‡‘"
- ä¸è¦éåº¦è§£é‡‹æˆ–åˆ†æ

è«‹è¼¸å‡ºï¼š"""
        else:
            # å¤šåŸå‘Šæ™‚ï¼Œä½¿ç”¨å®Œæ•´æ ¼å¼ï¼Œæ¯ä½åŸå‘Šåˆ†åˆ¥åˆ—å‡ºæå®³
            prompt = f"""ä½ æ˜¯å°ç£å¾‹å¸«ï¼Œè«‹æ ¹æ“šè»Šç¦æ¡ˆä»¶çš„æå®³è³ å„Ÿå…§å®¹ï¼Œåˆ†æä¸¦é‡æ–°æ•´ç†æˆæ¨™æº–çš„èµ·è¨´ç‹€æå®³é …ç›®æ ¼å¼ï¼š

ã€ç•¶äº‹äººè³‡è¨Šã€‘
åŸå‘Šï¼š{parties.get('åŸå‘Š', 'æœªæåŠ')}ï¼ˆå…±{plaintiff_count}åï¼‰
è¢«å‘Šï¼š{parties.get('è¢«å‘Š', 'æœªæåŠ')}ï¼ˆå…±{defendant_count}åï¼‰
åŸå‘Šåå–®ï¼š{plaintiff_list}

ã€åŸå§‹æå®³æè¿°ã€‘
{preprocessed_facts}

ã€åˆ†æè¦æ±‚ã€‘
è«‹ä»”ç´°åˆ†æä¸Šè¿°å…§å®¹ï¼Œå¾ä¸­æå–å‡ºï¼š
1. æ¯ä½åŸå‘Šçš„å…·é«”æå®³é …ç›®é¡å‹å’Œç¢ºåˆ‡é‡‘é¡
2. **æ¯é …æå®³çš„è©³ç´°äº‹å¯¦æ ¹æ“šå’Œæ³•å¾‹ç†ç”±**ï¼šé€™æ˜¯æœ€é‡è¦çš„éƒ¨åˆ†ï¼
3. **é‡è¦**ï¼šåªèƒ½ä½¿ç”¨åŸå§‹æè¿°ä¸­å·²æåŠçš„äº‹å¯¦ï¼Œçµ•å°ä¸å¯ä»¥è‡ªè¡Œæ·»åŠ æˆ–ç·¨é€ ä»»ä½•å…§å®¹
4. **å°±é†«ç´€éŒ„æ•´åˆ**ï¼šå¦‚æœåŸå§‹æè¿°ä¸­æœ‰å…·é«”çš„é†«é™¢åç¨±ï¼Œè«‹å°‡é€™äº›å°±é†«ç´€éŒ„æ•´åˆåˆ°é†«ç™‚ç›¸é—œæå®³é …ç›®çš„ç†ç”±èªªæ˜ä¸­
5. **ç†ç”±å®Œæ•´æ€§**ï¼šæ¯å€‹æå®³é …ç›®éƒ½å¿…é ˆæœ‰å®Œæ•´çš„ç†ç”±èªªæ˜ï¼ŒåŒ…æ‹¬ï¼š
   - æå®³ç™¼ç”Ÿçš„åŸå› ï¼ˆå› æœ¬æ¬¡è»Šç¦...ï¼‰
   - å…·é«”çš„äº‹å¯¦ä¾æ“šï¼ˆæ”¯å‡ºè²»ç”¨ã€å—å‚·æƒ…æ³ã€å½±éŸ¿ç­‰ï¼‰
   - ç›¸é—œçš„è©³ç´°èªªæ˜ï¼ˆå°±é†«æƒ…æ³ã€å·¥ä½œå½±éŸ¿ã€ç”Ÿæ´»å½±éŸ¿ç­‰ï¼‰

ã€æ¨™æº–è¼¸å‡ºæ ¼å¼ã€‘
ä¸‰ã€æå®³é …ç›®ï¼š

ï¼ˆä¸€ï¼‰åŸå‘Šå³éº—å¨Ÿä¹‹æå®³ï¼š
1. é†«ç™‚è²»ç”¨ï¼š6,720å…ƒ
åŸå‘Šå³éº—å¨Ÿå› æœ¬æ¬¡è»Šç¦å—å‚·ï¼Œæ–¼è‡ºåŒ—æ¦®æ°‘ç¸½é†«é™¢ã€é¦¬å•ç´€å¿µé†«é™¢ã€å…§æ¹–èè‹±è¨ºæ‰€åŠä¸­é†«è¨ºæ‰€å°±é†«æ²»ç™‚ï¼Œæ”¯å‡ºé†«ç™‚è²»ç”¨è¨ˆ6,720å…ƒã€‚

2. æœªä¾†æ‰‹è¡“è²»ç”¨ï¼š264,379å…ƒ
åŸå‘Šå³éº—å¨Ÿå› æœ¬æ¬¡è»Šç¦ç¶“æ¦®æ°‘ç¸½é†«é™¢ç¢ºè¨ºç™¼ç”Ÿè…°æ¤ç¬¬ä¸€ã€äºŒç¯€è„Šæ¤æ»‘è„«ï¼Œé è¨ˆæœªä¾†æ‰‹è¡“è²»ç”¨ç‚º264,379å…ƒã€‚

3. çœ‹è­·è²»ç”¨ï¼š152,500å…ƒ
åŸå‘Šå³éº—å¨Ÿå› æœ¬æ¬¡è»Šç¦èº«é«”å—çŒ›çƒˆæ’æ“Šéœ‡ç›ªï¼Œé¤Šå‚·æœŸé–“ç„¡ç”Ÿæ´»è‡ªä¸»èƒ½åŠ›ï¼Œè‡ª107å¹´7æœˆ24æ—¥èµ·è‡³107å¹´11æœˆ23æ—¥æ­¢ï¼Œå¹³å‡åˆ†æ”¤çœ‹è­·è²»ç”¨å…±è¨ˆ305,000å…ƒä¹‹åŠæ•¸ã€‚

4. æ…°æ’«é‡‘ï¼š200,000å…ƒ
åŸå‘Šå³éº—å¨Ÿå› æœ¬æ¬¡è»Šç¦é™¤å—å¤–å‚·å¤–ï¼Œå°šå› å—æ’æ“Šæ‹‰æ‰¯ï¼Œé ˆé•·æœŸæ²»ç™‚åŠå¾©å¥ï¼Œä¸”æœªä¾†å°šé ˆè² æ“”æ²‰é‡æ‰‹è¡“è²»ç”¨ï¼Œæ•…è«‹æ±‚æ…°æ’«é‡‘200,000å…ƒã€‚

ï¼ˆäºŒï¼‰åŸå‘Šé™³ç¢§ç¿”ä¹‹æå®³ï¼š
1. é†«ç™‚è²»ç”¨ï¼š12,180å…ƒ
åŸå‘Šé™³ç¢§ç¿”å› æœ¬æ¬¡è»Šç¦å—å‚·ï¼Œæ–¼è‡ºåŒ—æ¦®æ°‘ç¸½é†«é™¢ã€é¦¬å•ç´€å¿µé†«é™¢åŠä¸­é†«è¨ºæ‰€å°±é†«æ²»ç™‚ï¼Œæ”¯å‡ºé†«ç™‚è²»ç”¨è¨ˆ12,180å…ƒã€‚

2. å‡ç‰™è£ç½®è²»ç”¨ï¼š24,000å…ƒ
åŸå‘Šé™³ç¢§ç¿”å› æœ¬æ¬¡è»Šç¦é ­éƒ¨å³å´é­å—é‡æ“Šï¼Œå‡ç‰™è„«è½ï¼Œéœ€é‡æ–°å®‰è£å‡ç‰™è£ç½®ï¼Œè²»ç”¨ç‚º24,000å…ƒã€‚

ã€é—œéµè¦æ±‚ã€‘
- æ¯ä½åŸå‘Šå…ˆç”¨ï¼ˆä¸€ï¼‰ï¼ˆäºŒï¼‰ç­‰ç·¨è™Ÿå€åˆ†
- æ¯ä½åŸå‘Šå…§éƒ¨çš„æå®³é …ç›®ä½¿ç”¨ 1. 2. 3. ç­‰æ•¸å­—ç·¨è™Ÿ
- **æ¯é …å¿…é ˆæ ¼å¼**ï¼šæ•¸å­—ç·¨è™Ÿ. é …ç›®åç¨±ï¼šé‡‘é¡ + è©³ç´°å®Œæ•´çš„æ³•å¾‹ç†ç”±èªªæ˜
- **ç†ç”±å®Œæ•´æ€§è¦æ±‚**ï¼šæ¯å€‹æå®³é …ç›®éƒ½å¿…é ˆåŒ…å«å®Œæ•´çš„ç†ç”±èªªæ˜ï¼Œä¸å¯åªæœ‰é‡‘é¡
- **ç†ç”±å…§å®¹è¦æ±‚**ï¼š
  - æå®³ç™¼ç”Ÿçš„åŸå› ï¼ˆå¦‚ï¼šå› æœ¬æ¬¡è»Šç¦å—å‚·...ï¼‰
  - å…·é«”çš„äº‹å¯¦ä¾æ“šï¼ˆå¦‚ï¼šæ”¯å‡ºè²»ç”¨ã€å—å‚·æƒ…æ³ã€å·¥ä½œå½±éŸ¿ç­‰ï¼‰
  - ç›¸é—œçš„è©³ç´°èªªæ˜ï¼ˆå¦‚ï¼šå°±é†«æƒ…æ³ã€æ²»ç™‚éœ€è¦ã€ç”Ÿæ´»å½±éŸ¿ç­‰ï¼‰
- ç†ç”±èªªæ˜å¿…é ˆåŸºæ–¼åŸå§‹æè¿°ä¸­çš„å…·é«”äº‹å¯¦ï¼Œå……åˆ†æå–åŸå§‹æè¿°çš„è©³ç´°è³‡è¨Š
- ä¸å¯è‡ªè¡Œç·¨é€ ä»»ä½•é†«ç™‚è¨ºæ–·ã€å‚·å‹¢æè¿°æˆ–å…¶ä»–ç´°ç¯€
- **å°±é†«ç´€éŒ„è™•ç†**ï¼šå¦‚æœåŸå§‹æè¿°æåŠå…·é«”é†«é™¢åç¨±ï¼Œè«‹åœ¨é†«ç™‚ç›¸é—œé …ç›®ä¸­åŠ å…¥ã€Œæ–¼[é†«é™¢åç¨±]å°±é†«æ²»ç™‚ã€
- ç†ç”±è¦æ¡ç”¨æ­£å¼çš„æ³•å¾‹æ–‡æ›¸èªè¨€
- ä½¿ç”¨åƒåˆ†ä½é€—è™Ÿæ ¼å¼é¡¯ç¤ºé‡‘é¡

ã€åš´æ ¼ç¦æ­¢äº‹é …ã€‘
- çµ•å°ä¸å¯åœ¨è¼¸å‡ºä¸­åŒ…å«ã€Œç¶œä¸Šæ‰€è¿°ã€ã€ã€Œç¸½è¨ˆã€ã€ã€Œåˆè¨ˆã€ã€ã€Œå…±è¨ˆã€ç­‰çµè«–æ€§æ–‡å­—
- ä¸è¦åŒ…å«ä»»ä½•ç¸½é‡‘é¡è¨ˆç®—æˆ–åŒ¯ç¸½èªªæ˜
- ä¸è¦åŒ…å«ä»»ä½•æ³•å®šåˆ©æ¯çš„èªªæ˜
- ä¸è¦åŒ…å«ä»»ä½•çµè«–æ®µè½æˆ–ç¸½çµæ–‡å­—
- ä¸è¦åŒ…å«è­‰æ“šç›¸é—œæ–‡å­—ï¼šã€Œæ­¤æœ‰ç›¸é—œæ”¶æ“šå¯è­‰ã€ã€ã€Œæœ‰æ”¶æ“šç‚ºè­‰ã€ã€ã€Œæœ‰çµ±ä¸€ç™¼ç¥¨å¯è­‰ã€ã€ã€Œå¯è­‰ã€ç­‰
- ä¸è¦åŒ…å«åˆ¤æ±ºæ›¸ç”¨èªï¼šã€Œç¶“æŸ¥ã€ã€ã€ŒæŸ¥æ˜ã€ã€ã€Œç¶“å¯©ç†ã€ç­‰
- **çµ•å°ç¦æ­¢è¢«å‘Šæå®³**ï¼šä¸å¯ä»¥åŒ…å«ä»»ä½•é—œæ–¼è¢«å‘Šæå®³çš„å…§å®¹ï¼Œè¢«å‘Šæ˜¯è³ å„Ÿç¾©å‹™äººä¸æœƒæœ‰æå®³
- **çµ•å°ç¦æ­¢**ï¼šä¸å¯ä»¥å‡ºç¾ã€Œï¼ˆäºŒï¼‰è¢«å‘Š...ä¹‹æå®³ã€æˆ–ä»»ä½•è¢«å‘Šæå®³çš„æè¿°
- åªè¼¸å‡ºç´”ç²¹çš„æå®³é …ç›®æ¢åˆ—ï¼Œæ¯é …åŒ…å«ç·¨è™Ÿã€åç¨±ã€é‡‘é¡ã€ç†ç”±èªªæ˜
- **æ˜ç¢ºåŸå‰‡**ï¼šèµ·è¨´ç‹€ä¸­åªèƒ½æœ‰åŸå‘Šçš„æå®³ï¼Œçµ•å°ä¸èƒ½å‰µé€ è™›å‡çš„è¢«å‘Šæå®³é …ç›®

è«‹åš´æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼å’Œè¦æ±‚ï¼ŒåŸºæ–¼åŸå§‹æè¿°çš„äº‹å¯¦åˆ†æä¸¦è¼¸å‡ºæå®³é …ç›®ï¼š"""

        result = self.call_llm(prompt, timeout=120)
        
        # æ¸…ç†çµè«–æ€§æ–‡å­—
        result = self._remove_conclusion_phrases(result)
        
        # ç§»é™¤ä»»ä½•é—œæ–¼è¢«å‘Šæå®³çš„éŒ¯èª¤å…§å®¹
        result = self._remove_defendant_damage_errors(result)
        
        # æª¢æŸ¥ä¸¦è£œå……ç¼ºå¤±çš„ç†ç”±
        result = self._ensure_reason_completeness(result, preprocessed_facts)
        
        # æœ€çµ‚æ¸…ç†è­‰æ“šèªè¨€ï¼ˆä½†ä¿ç•™ç†ç”±èªªæ˜ï¼‰
        result = self._remove_conclusion_phrases(result)
        
        # æœ€çµ‚æ ¼å¼é©—è­‰å’Œä¿®æ­£ï¼ˆä½†ä¸è¦ç ´å£ç†ç”±ï¼‰
        # result = self._final_format_validation(result, is_single_case)
        
        # æª¢æŸ¥çµæœæ˜¯å¦åŒ…å«é æœŸæ ¼å¼
        if "ï¼ˆä¸€ï¼‰" in result:
            # ç¢ºä¿åŒ…å«æ¨™é¡Œã€Œä¸‰ã€æå®³é …ç›®ï¼šã€
            if not result.startswith("ä¸‰ã€æå®³é …ç›®ï¼š"):
                result = "ä¸‰ã€æå®³é …ç›®ï¼š\n\n" + result
            return result
        else:
            # Fallbackï¼šè¿”å›åŸºæœ¬æ ¼å¼
            return comp_facts

    def _build_adaptive_prompt(self, preprocessed_facts: str, parties: dict, detected_format: str) -> str:
        """æ ¹æ“šæª¢æ¸¬åˆ°çš„æ ¼å¼æ§‹å»ºé©æ‡‰æ€§æç¤ºè©"""
        plaintiff = parties.get("åŸå‘Š", "åŸå‘Š")
        plaintiff_count = parties.get('åŸå‘Šæ•¸é‡', 1)
        
        if detected_format == 'multi_plaintiff_narrative':
            # ä¿®æ­£åŸå‘Šæ•¸é‡è¨ˆç®—
            plaintiff_list = [name.strip() for name in parties.get('åŸå‘Š', 'åŸå‘Š').split('ã€') if name.strip() and name.strip() != 'åŸå‘Š']
            actual_plaintiff_count = len(plaintiff_list) if plaintiff_list else 1
            
            # å¤šåŸå‘Šæ•˜è¿°æ ¼å¼ - å°ˆé–€è™•ç†å¤šååŸå‘Šå„è‡ªçš„æå®³
            return f"""ä½ æ˜¯å°ç£è³‡æ·±å¾‹å¸«ï¼Œè«‹å¾ä»¥ä¸‹å¤šåŸå‘Šæå®³æè¿°ä¸­åˆ†åˆ¥æå–æ¯ä½åŸå‘Šçš„æå®³é …ç›®ã€‚

ã€å¤šåŸå‘Šæå®³æè¿°ã€‘
{preprocessed_facts}

ã€ç•¶äº‹äººä¿¡æ¯ã€‘
åŸå‘Šï¼š{plaintiff}ï¼ˆå…±{actual_plaintiff_count}åï¼‰
åŸå‘Šåå–®ï¼š{plaintiff_list}

ã€é‡è¦åˆ†ææŒ‡å°ã€‘
é€™æ˜¯å¤šåŸå‘Šæ¡ˆä¾‹ï¼Œéœ€è¦ï¼š
1. åˆ†åˆ¥è­˜åˆ¥æ¯ä½åŸå‘Šçš„æå®³é …ç›®å’Œé‡‘é¡
2. å€åˆ†è¨ˆç®—åŸºæº–vsæœ€çµ‚æ±‚å„Ÿé‡‘é¡
3. æŒ‰åŸå‘Šåˆ†çµ„æ•´ç†æå®³é …ç›®

ã€è¼¸å‡ºæ ¼å¼ã€‘
ï¼ˆä¸€ï¼‰åŸå‘Š[å§“å]ä¹‹æå®³ï¼š
1. æå®³é¡å‹ï¼šé‡‘é¡å…ƒ
åŸå‘Š[å§“å]å› æœ¬æ¬¡äº‹æ•…[ç°¡æ½”ç†ç”±]ï¼Œæ”¯å‡º/å—æœ‰[æå®³é¡å‹]é‡‘é¡å…ƒã€‚

ï¼ˆäºŒï¼‰åŸå‘Š[å§“å]ä¹‹æå®³ï¼š
1. æå®³é¡å‹ï¼šé‡‘é¡å…ƒ
åŸå‘Š[å§“å]å› æœ¬æ¬¡äº‹æ•…[ç°¡æ½”ç†ç”±]ï¼Œæ”¯å‡º/å—æœ‰[æå®³é¡å‹]é‡‘é¡å…ƒã€‚

ã€ç¯„ä¾‹ã€‘
ï¼ˆä¸€ï¼‰åŸå‘Šç¾…é–å´´ä¹‹æå®³ï¼š
1. é†«ç™‚è²»ç”¨ï¼š2,443å…ƒ
åŸå‘Šç¾…é–å´´å› æœ¬æ¬¡äº‹æ•…å—å‚·å°±é†«ï¼Œæ”¯å‡ºé†«ç™‚è²»ç”¨2,443å…ƒã€‚
2. äº¤é€šè²»ï¼š1,235å…ƒ
åŸå‘Šç¾…é–å´´å› æœ¬æ¬¡äº‹æ•…å°±é†«ï¼Œæ”¯å‡ºäº¤é€šè²»ç”¨1,235å…ƒã€‚
3. å·¥ä½œæå¤±ï¼š20,148å…ƒ
åŸå‘Šç¾…é–å´´å› æœ¬æ¬¡äº‹æ•…ç„¡æ³•å·¥ä½œ16æ—¥ï¼Œå—æœ‰å·¥ä½œæå¤±20,148å…ƒã€‚
4. æ…°æ’«é‡‘ï¼š10,000å…ƒ
åŸå‘Šç¾…é–å´´å› æœ¬æ¬¡äº‹æ•…å—æœ‰èº«å¿ƒç—›è‹¦ï¼Œè«‹æ±‚ç²¾ç¥æ…°æ’«é‡‘10,000å…ƒã€‚

ï¼ˆäºŒï¼‰åŸå‘Šé‚±å“å¦ä¹‹æå®³ï¼š
1. é†«ç™‚è²»ç”¨ï¼š57,550å…ƒ
åŸå‘Šé‚±å“å¦å› æœ¬æ¬¡äº‹æ•…å—å‚·å°±é†«ï¼Œæ”¯å‡ºé†«ç™‚è²»ç”¨57,550å…ƒã€‚
2. äº¤é€šè²»ï¼š22,195å…ƒ
åŸå‘Šé‚±å“å¦å› æœ¬æ¬¡äº‹æ•…å°±é†«ï¼Œæ”¯å‡ºäº¤é€šè²»ç”¨22,195å…ƒã€‚
3. å·¥ä½œæå¤±ï¼š66,768å…ƒ
åŸå‘Šé‚±å“å¦å› æœ¬æ¬¡äº‹æ•…ç„¡æ³•å·¥ä½œ1æœˆ28æ—¥ï¼Œå—æœ‰å·¥ä½œæå¤±66,768å…ƒã€‚
4. è»Šè¼›æå¤±ï¼š36,000å…ƒ
åŸå‘Šé‚±å“å¦å› æœ¬æ¬¡äº‹æ•…è»Šè¼›å—æï¼Œæ”¯å‡ºä¿®å¾©åŠé‘‘å®šè²»ç”¨36,000å…ƒã€‚
5. æ…°æ’«é‡‘ï¼š60,000å…ƒ
åŸå‘Šé‚±å“å¦å› æœ¬æ¬¡äº‹æ•…å—æœ‰èº«å¿ƒç—›è‹¦ï¼Œè«‹æ±‚ç²¾ç¥æ…°æ’«é‡‘60,000å…ƒã€‚

ã€åš´æ ¼è¦æ±‚ã€‘
- å¿…é ˆæŒ‰åŸå‘Šåˆ†çµ„ï¼Œæ¯ä½åŸå‘Šå–®ç¨åˆ—å‡º
- æ¯ä½åŸå‘Šå…ˆç”¨ï¼ˆä¸€ï¼‰ï¼ˆäºŒï¼‰ç­‰ä¸­æ–‡ç·¨è™Ÿå€åˆ†
- æ¯ä½åŸå‘Šå…§éƒ¨ç”¨1. 2. 3.ç­‰æ•¸å­—ç·¨è™Ÿ
- åªæå–æœ€çµ‚æ±‚å„Ÿé‡‘é¡ï¼Œæ’é™¤è¨ˆç®—åŸºæº–ï¼ˆå¦‚æœˆè–ªã€æ—¥è–ªç­‰ï¼‰
- æ¯é …ç†ç”±1-2å¥è©±ï¼Œç°¡æ½”æ˜ç­
- ä½¿ç”¨åƒåˆ†ä½é€—è™Ÿæ ¼å¼
- ä¸è¦åŒ…å«ç¸½è¨ˆã€ç¶œä¸Šæ‰€è¿°ç­‰çµè«–æ€§æ–‡å­—

è«‹åˆ†æä¸¦è¼¸å‡ºï¼š"""
            
        elif detected_format == 'free_format':
            # è‡ªç”±æ–‡æœ¬æ ¼å¼ - å°ˆé–€è™•ç†éçµæ§‹åŒ–æè¿°
            return f"""ä½ æ˜¯å°ç£è³‡æ·±å¾‹å¸«ï¼Œè«‹å¾ä»¥ä¸‹è‡ªç”±æ–‡æœ¬ä¸­æå–ä¸¦æ•´ç†æå®³è³ å„Ÿé …ç›®ã€‚

ã€åŸå§‹è‡ªç”±æ–‡æœ¬ã€‘
{preprocessed_facts}

ã€é‡è¦åˆ†ææŒ‡å°ã€‘
ä»¥ä¸‹æ–‡æœ¬å¯èƒ½åŒ…å«ï¼š
1. æ··åˆæè¿°çš„æå®³é …ç›®å’Œé‡‘é¡
2. è¨ˆç®—éç¨‹ä¸­çš„åŸºæº–æ•¸æ“šï¼ˆå¦‚åŸºæœ¬å·¥è³‡ã€æ—¥è–ªç­‰ï¼‰- é€™äº›ä¸æ˜¯æœ€çµ‚æ±‚å„Ÿé‡‘é¡
3. æœ€çµ‚çš„æ±‚å„Ÿé‡‘é¡

ã€æå–åŸå‰‡ã€‘
1. è­˜åˆ¥çœŸæ­£çš„æå®³é¡å‹ï¼šé†«ç™‚è²»ç”¨ã€äº¤é€šè²»ã€çœ‹è­·è²»ã€å·¥ä½œæå¤±ã€ç²¾ç¥æ…°æ’«é‡‘ç­‰
2. å€åˆ†è¨ˆç®—åŸºæº–vsæœ€çµ‚æ±‚å„Ÿï¼š
   - è¨ˆç®—åŸºæº–ï¼šã€Œä»¥æ¯æ—¥XXå…ƒä½œç‚ºè¨ˆç®—åŸºæº–ã€ã€ã€Œæ¯æœˆåŸºæœ¬å·¥è³‡XXå…ƒè¨ˆç®—ã€
   - æœ€çµ‚æ±‚å„Ÿï¼šã€Œå…±è«‹æ±‚XXå…ƒã€ã€ã€Œè³ å„ŸXXå…ƒã€ã€ã€Œæå¤±ç‚ºXXå…ƒã€
3. åªæ¡ç”¨æœ€çµ‚æ±‚å„Ÿé‡‘é¡ï¼Œæ’é™¤è¨ˆç®—éç¨‹ä¸­çš„åŸºæº–æ•¸æ“š

ã€è¼¸å‡ºæ ¼å¼ã€‘
ï¼ˆä¸€ï¼‰æå®³é¡å‹ï¼šé‡‘é¡å…ƒ
åŸå‘Šå› æœ¬æ¬¡äº‹æ•…[ç°¡æ½”ç†ç”±èªªæ˜]ã€‚

ã€ç¯„ä¾‹ã€‘
ï¼ˆä¸€ï¼‰é†«ç™‚è²»ç”¨ï¼š255,830å…ƒ
åŸå‘Šå› æœ¬æ¬¡äº‹æ•…å—å‚·å°±é†«ï¼Œæ”¯å‡ºé†«ç™‚åŠäº¤é€šè²»ç”¨å…±è¨ˆ255,830å…ƒã€‚

ï¼ˆäºŒï¼‰çœ‹è­·è²»ç”¨ï¼š270,000å…ƒ
åŸå‘Šå› æœ¬æ¬¡äº‹æ•…éœ€å°ˆäººç…§é¡§ï¼Œæ”¯å‡ºçœ‹è­·è²»ç”¨å…±è¨ˆ270,000å…ƒã€‚

ï¼ˆä¸‰ï¼‰å·¥ä½œæå¤±ï¼š113,625å…ƒ
åŸå‘Šå› æœ¬æ¬¡äº‹æ•…ç„¡æ³•å·¥ä½œï¼Œå—æœ‰è–ªè³‡æå¤±å…±è¨ˆ113,625å…ƒã€‚

ï¼ˆå››ï¼‰æ…°æ’«é‡‘ï¼š300,000å…ƒ
åŸå‘Šå› æœ¬æ¬¡äº‹æ•…å—æœ‰èº«å¿ƒç—›è‹¦ï¼Œè«‹æ±‚ç²¾ç¥æ…°æ’«é‡‘300,000å…ƒã€‚

ã€åš´æ ¼è¦æ±‚ã€‘
- åªè¼¸å‡ºçœŸæ­£çš„æ±‚å„Ÿé …ç›®ï¼Œæ’é™¤è¨ˆç®—åŸºæº–
- æ¯é …ç†ç”±1-2å¥è©±ï¼Œç°¡æ½”æ˜ç­
- ä½¿ç”¨åƒåˆ†ä½é€—è™Ÿæ ¼å¼
- ä¸è¦åŒ…å«ç¸½è¨ˆã€ç¶œä¸Šæ‰€è¿°ç­‰çµè«–æ€§æ–‡å­—

è«‹åˆ†æä¸¦è¼¸å‡ºï¼š"""
            
        else:
            # å…¶ä»–æ ¼å¼æˆ–æœªçŸ¥æ ¼å¼ - ä½¿ç”¨é€šç”¨ç­–ç•¥
            if plaintiff_count == 1:
                return f"""è«‹å°‡ä»¥ä¸‹æå®³å…§å®¹æ•´ç†æˆæ¨™æº–æ ¼å¼çš„æå®³é …ç›®æ¸…å–®ã€‚

ã€åŸå§‹å…§å®¹ã€‘
{preprocessed_facts}

ã€è¼¸å‡ºæ ¼å¼ã€‘
ï¼ˆä¸€ï¼‰é …ç›®åç¨±ï¼šé‡‘é¡å…ƒ
åŸå‘Šå› æœ¬æ¬¡äº‹æ•…[ç°¡æ½”ç†ç”±]ï¼Œæ”¯å‡º/å—æœ‰[é …ç›®åç¨±]é‡‘é¡å…ƒã€‚

ã€è¦æ±‚ã€‘
- æ¯é …ç†ç”±1-2å¥è©±ï¼Œç°¡æ½”æ˜ç­
- ä¿ç•™é‡è¦é†«é™¢åç¨±ã€å·¥ä½œå–®ä½ç­‰é—œéµè³‡è¨Š  
- çµ±ä¸€ä½¿ç”¨"æœ¬æ¬¡äº‹æ•…"
- å·¥ä½œæå¤±é …ç›®åç¨±ç›´æ¥ç”¨"å·¥ä½œæå¤±"
- ç²¾ç¥æ…°æ’«é‡‘é …ç›®åç¨±ç›´æ¥ç”¨"æ…°æ’«é‡‘"
- ä½¿ç”¨åƒåˆ†ä½é€—è™Ÿæ ¼å¼é¡¯ç¤ºé‡‘é¡

è«‹è¼¸å‡ºï¼š"""
            else:
                # å¤šåŸå‘Šæƒ…æ³
                return f"""ä½ æ˜¯å°ç£å¾‹å¸«ï¼Œè«‹æ ¹æ“šè»Šç¦æ¡ˆä»¶çš„æå®³è³ å„Ÿå…§å®¹ï¼Œåˆ†æä¸¦é‡æ–°æ•´ç†æˆæ¨™æº–çš„èµ·è¨´ç‹€æå®³é …ç›®æ ¼å¼ï¼š

ã€ç•¶äº‹äººè³‡è¨Šã€‘
åŸå‘Šï¼š{parties.get('åŸå‘Š', 'æœªæåŠ')}ï¼ˆå…±{plaintiff_count}åï¼‰
è¢«å‘Šï¼š{parties.get('è¢«å‘Š', 'æœªæåŠ')}ï¼ˆå…±{parties.get('è¢«å‘Šæ•¸é‡', 1)}åï¼‰

ã€åŸå§‹æå®³æè¿°ã€‘
{preprocessed_facts}

ã€æ¨™æº–è¼¸å‡ºæ ¼å¼ã€‘
ï¼ˆä¸€ï¼‰åŸå‘Š[å§“å]ä¹‹æå®³ï¼š
1. é†«ç™‚è²»ç”¨ï¼šé‡‘é¡å…ƒ
åŸå‘Š[å§“å]å› æœ¬æ¬¡è»Šç¦å—å‚·ï¼Œ[è©³ç´°å°±é†«æƒ…æ³]ï¼Œæ”¯å‡ºé†«ç™‚è²»ç”¨è¨ˆé‡‘é¡å…ƒã€‚

ã€è¦æ±‚ã€‘
- æ¯ä½åŸå‘Šå…ˆç”¨ï¼ˆä¸€ï¼‰ï¼ˆäºŒï¼‰ç­‰ç·¨è™Ÿå€åˆ†
- æ¯ä½åŸå‘Šå…§éƒ¨çš„æå®³é …ç›®ä½¿ç”¨ 1. 2. 3. ç­‰æ•¸å­—ç·¨è™Ÿ
- æ¯é …å¿…é ˆåŒ…å«ï¼šæ•¸å­—ç·¨è™Ÿ. é …ç›®åç¨±ï¼šé‡‘é¡ + è©³ç´°ç†ç”±èªªæ˜
- ç†ç”±è¦å®Œæ•´ï¼ŒåŒ…å«æå®³åŸå› ã€äº‹å¯¦ä¾æ“šã€è©³ç´°èªªæ˜
- ä¸å¯åŒ…å«ç¸½è¨ˆã€ç¶œä¸Šæ‰€è¿°ç­‰çµè«–æ€§æ–‡å­—

è«‹åˆ†æä¸¦è¼¸å‡ºï¼š"""

    def _comprehensive_number_preprocessing(self, text: str) -> str:
        """å…¨é¢é è™•ç†ä¸­æ–‡æ•¸å­—å’Œç‰¹æ®Šæ ¼å¼"""
        import re
        
        # è™•ç† Xè¬Y,YYYå…ƒ æ ¼å¼ (å¦‚ï¼š26è¬4,379å…ƒ)
        pattern1 = r'(\d+)è¬(\d+,?\d+)å…ƒ'
        def replace1(match):
            wan = int(match.group(1))
            rest = int(match.group(2).replace(',', ''))
            total = wan * 10000 + rest
            return f"{total}å…ƒ"
        text = re.sub(pattern1, replace1, text)
        
        # è™•ç†å…¶ä»–ä¸­æ–‡æ•¸å­—æ ¼å¼
        text = re.sub(r'(\d+)è¬(\d+)åƒå…ƒ', lambda m: f"{int(m.group(1))*10000 + int(m.group(2))*1000}å…ƒ", text)
        text = re.sub(r'(\d+)è¬å…ƒ', lambda m: f"{int(m.group(1))*10000}å…ƒ", text)
        text = re.sub(r'(\d+)åƒå…ƒ', lambda m: f"{int(m.group(1))*1000}å…ƒ", text)
        
        return text

    def _is_same_damage_type(self, context1: str, context2: str) -> bool:
        """åˆ¤æ–·å…©å€‹ä¸Šä¸‹æ–‡æ˜¯å¦ç‚ºç›¸åŒçš„æå®³é¡å‹"""
        damage_types = [
            ['é†«ç™‚', 'æ²»ç™‚', 'å°±è¨º'],
            ['çœ‹è­·', 'ç…§é¡§'],
            ['ç‰™é½’', 'å‡ç‰™'],
            ['æ…°æ’«', 'ç²¾ç¥', 'ç—›è‹¦'],
            ['äº¤é€š', 'è»Šè³‡'],
            ['å·¥ä½œ', 'æ”¶å…¥', 'è–ªè³‡'],
            ['ä¿®å¾©', 'ä¿®ç†', 'ç¶­ä¿®']
        ]
        
        # æ‰¾å‡ºæ¯å€‹ä¸Šä¸‹æ–‡çš„æå®³é¡å‹
        type1 = None
        type2 = None
        
        for i, keywords in enumerate(damage_types):
            if any(keyword in context1 for keyword in keywords):
                type1 = i
            if any(keyword in context2 for keyword in keywords):
                type2 = i
        
        return type1 is not None and type1 == type2

    def _extract_valid_claim_amounts(self, text: str) -> list:
        """é€šç”¨æ™ºèƒ½é‡‘é¡æå– - ä½¿ç”¨å¤šå±¤æ¬¡ç­–ç•¥é©æ‡‰å„ç¨®æ ¼å¼"""
        print(f"ğŸ” ã€é€šç”¨é‡‘é¡æå–ã€‘åŸå§‹æ–‡æœ¬: {text[:200]}...")

        # ç­–ç•¥1: ä½¿ç”¨é€šç”¨æ ¼å¼è™•ç†å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.format_handler:
            try:
                damage_items = self.format_handler.extract_damage_items(text)
                if damage_items and len(damage_items) >= 1:  # ä¿®æ­£ï¼šåªè¦æœ‰1å€‹çµæœå°±ä½¿ç”¨
                    amounts = [item.amount for item in damage_items]
                    
                    # å¾Œè™•ç†ï¼šç§»é™¤æ˜é¡¯çš„è¨ˆç®—åŸºæº–é‡‘é¡ - å¢å¼·ç‰ˆ
                    filtered_amounts = []
                    for amount in amounts:
                        is_calculation_base = False
                        
                        # æª¢æŸ¥å¸¸è¦‹çš„è¨ˆç®—åŸºæº–é‡‘é¡ - å¢å¼·ç‰ˆ
                        calculation_base_checks = [
                            # æœˆè–ªåŸºæº–æª¢æŸ¥
                            {
                                'patterns': ['æ¯å€‹æœˆæœˆè–ª', 'æœˆè–ª', 'å¹³å‡æ¯æœˆè–ªè³‡', 'æ¯æœˆå·¥è³‡.*ç‚º', 'æœˆå·¥è³‡.*ç‚º'],
                                'description': 'æœˆè–ªåŸºæº–'
                            },
                            # æ¯æ—¥è²»ç”¨åŸºæº–æª¢æŸ¥  
                            {
                                'patterns': ['æ¯æ—¥', 'æ—¥è–ª', 'å…¨æ—¥ç…§è­·è²»ç”¨æ¯æ—¥', 'ä»¥.*æ¯æ—¥.*è¨ˆç®—', 'ä»¥.*æ¯æ—¥.*ä½œç‚ºè¨ˆç®—åŸºæº–'],
                                'description': 'æ¯æ—¥è²»ç”¨åŸºæº–'
                            },
                            # æ¯æœˆè¨ˆç®—åŸºæº–æª¢æŸ¥
                            {
                                'patterns': ['æ¯æœˆ.*è¨ˆç®—', 'ä¾æ¯æœˆ.*è¨ˆç®—', 'æ¯æœˆ.*å‹å‹•èƒ½åŠ›', 'ä¾æ¯æœˆ.*æ¸›å°‘'],
                                'description': 'æ¯æœˆè¨ˆç®—åŸºæº–'
                            },
                            # ç™¾åˆ†æ¯”è¨ˆç®—åŸºæº–æª¢æŸ¥
                            {
                                'patterns': ['è¨ˆç®—å¼', 'Ã—', 'ï¼…', '76ï¼…è¨ˆç®—', '.*ï¼….*è¨ˆç®—'],
                                'description': 'è¨ˆç®—å¼åŸºæº–'
                            },
                            # ç‰¹å®šé‡‘é¡åŸºæº–æª¢æŸ¥ï¼ˆé‡å°å¯¦éš›æ¡ˆä¾‹ï¼‰
                            {
                                'patterns': ['ä½œç‚ºè¨ˆç®—åŸºæº–', 'è¨ˆç®—åŸºæº–', 'ä»¥.*è¨ˆç®—', 'æŒ‰.*è¨ˆç®—'],
                                'description': 'è¨ˆç®—åŸºæº–é‡‘é¡'
                            },
                            # å‹å‹•èƒ½åŠ›æ¸›å°‘åŸºæº–
                            {
                                'patterns': ['å‹å‹•èƒ½åŠ›.*æ¸›å°‘', 'å‹å‹•èƒ½åŠ›.*æå¤±.*è¨ˆç®—', 'å‹å‹•èƒ½åŠ›.*è©•ä¼°'],
                                'description': 'å‹å‹•èƒ½åŠ›è©•ä¼°åŸºæº–'
                            }
                        ]
                        
                        # æª¢æŸ¥é‡‘é¡æ˜¯å¦å‡ºç¾åœ¨è¨ˆç®—åŸºæº–çš„ä¸Šä¸‹æ–‡ä¸­
                        amount_str_patterns = [f'{amount:,}å…ƒ', f'{amount}å…ƒ']
                        
                        for pattern in amount_str_patterns:
                            if pattern in text:
                                pos = text.find(pattern)
                                # æ“´å¤§ä¸Šä¸‹æ–‡æª¢æŸ¥ç¯„åœåˆ°120å­—ç¬¦
                                context_start = max(0, pos - 120)
                                context_end = min(len(text), pos + 120)
                                context = text[context_start:context_end]
                                
                                # æª¢æŸ¥æ‰€æœ‰é¡å‹çš„è¨ˆç®—åŸºæº–
                                for check in calculation_base_checks:
                                    for check_pattern in check['patterns']:
                                        # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼é€²è¡Œæ›´ç²¾ç¢ºçš„åŒ¹é…
                                        import re as regex_module
                                        if regex_module.search(check_pattern, context):
                                            # é€²ä¸€æ­¥é©—è­‰ï¼šç¢ºä¿åŸºæº–è©å’Œé‡‘é¡è·é›¢ä¸è¶…é30å­—ç¬¦
                                            pattern_pos = context.find(check_pattern) if check_pattern in context else -1
                                            amount_pos_in_context = context.find(pattern)
                                            
                                            if pattern_pos != -1 and amount_pos_in_context != -1:
                                                distance = abs(pattern_pos - amount_pos_in_context)
                                                if distance < 30:  # è·é›¢å°æ–¼30å­—ç¬¦
                                                    # æª¢æŸ¥ä¸­é–“æ˜¯å¦æœ‰æ’é™¤è©
                                                    start_pos = min(pattern_pos, amount_pos_in_context)
                                                    end_pos = max(pattern_pos, amount_pos_in_context)
                                                    between_text = context[start_pos:end_pos]
                                                    
                                                    # å¦‚æœä¸­é–“åŒ…å«æ˜ç¢ºçš„æ±‚å„Ÿè©ï¼Œå‰‡ä¸è¦–ç‚ºè¨ˆç®—åŸºæº–
                                                    claim_words = ['è«‹æ±‚', 'è³ å„Ÿ', 'æå®³', 'æå¤±', 'æ”¯å‡º', 'è²»ç”¨']
                                                    has_claim_word = any(word in between_text for word in claim_words)
                                                    
                                                    if not has_claim_word:
                                                        is_calculation_base = True
                                                        print(f'ğŸ” ã€å¾Œè™•ç†éæ¿¾ã€‘æ’é™¤{check["description"]}: {amount:,}å…ƒ (è·é›¢: {distance}å­—ç¬¦)')
                                                        print(f'    ä¸Šä¸‹æ–‡: ...{context[max(0, pattern_pos-10):min(len(context), amount_pos_in_context+10)]}...')
                                                        break
                                    if is_calculation_base:
                                        break
                                if is_calculation_base:
                                    break
                        
                        if not is_calculation_base:
                            filtered_amounts.append(amount)
                    
                    print(f"ğŸ” ã€é€šç”¨æ ¼å¼è™•ç†å™¨ã€‘å¾Œè™•ç†å¾Œå‰©é¤˜ {len(filtered_amounts)} å€‹é‡‘é¡: {filtered_amounts}")
                    print(f"ğŸ” ã€é€šç”¨æ ¼å¼è™•ç†å™¨ã€‘æœ€çµ‚ç¸½è¨ˆ: {sum(filtered_amounts):,}å…ƒ")
                    return filtered_amounts
                else:
                    print("ğŸ” ã€é€šç”¨æ ¼å¼è™•ç†å™¨ã€‘æå–çµæœä¸è¶³ï¼Œä½¿ç”¨fallbackç­–ç•¥")
            except Exception as e:
                print(f"ğŸ” ã€é€šç”¨æ ¼å¼è™•ç†å™¨ã€‘è™•ç†å¤±æ•—: {e}ï¼Œä½¿ç”¨fallbackç­–ç•¥")

        # ç­–ç•¥2: Fallbackåˆ°åŸæœ‰é‚è¼¯ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
        return self._extract_amounts_legacy_method(text)

    def _extract_amounts_legacy_method(self, text: str) -> list:
        """åŸæœ‰çš„é‡‘é¡æå–é‚è¼¯ï¼ˆä½œç‚ºfallbackï¼‰"""
        import re

        print(f"ğŸ” ã€å‚³çµ±é‡‘é¡æå–ã€‘Fallbackåˆ°åŸæœ‰é‚è¼¯...")

        # 1. å…ˆé è™•ç†ä¸­æ–‡æ•¸å­—
        processed_text = self._comprehensive_number_preprocessing(text)
        clean_text = processed_text.replace(',', '')

        # 2. å®šç¾©æœ‰æ•ˆçš„æ±‚å„Ÿé—œéµè©
        valid_claim_keywords = [
            'è²»ç”¨', 'æå¤±', 'æ…°æ’«é‡‘', 'è³ å„Ÿ', 'æ”¯å‡º', 'èŠ±è²»',
            'é†«ç™‚', 'ä¿®å¾©', 'ä¿®ç†', 'äº¤é€š', 'çœ‹è­·', 'æ‰‹è¡“',
            'å‡ç‰™', 'å¾©å¥', 'æ²»ç™‚', 'å·¥ä½œæ”¶å…¥', 'é ä¼°', 'æœªä¾†', 'é è¨ˆ', 'ç”¨å“'
        ]

        # 3. å®šç¾©æ’é™¤çš„é—œéµè©ï¼ˆéæ±‚å„Ÿé …ç›®ï¼‰- æ›´ç²¾ç¢ºçš„åŒ¹é…
        exclude_keywords = [
            'æ—¥è–ª', 'å¹´åº¦æ‰€å¾—', 'æœˆæ”¶å…¥', 'æ™‚è–ª', 'å­¸æ­·', 'ç•¢æ¥­',
            'åä¸‹', 'å‹•ç”¢', 'ç¸½è¨ˆæ–°å°å¹£', 'åˆè¨ˆæ–°å°å¹£', 'å°è¨ˆæ–°å°å¹£',
            'åŒ…æ‹¬', 'å…¶ä¸­', 'åŒ…å«',  # æ·»åŠ ç´°é …åˆ†è§£é—œéµè©
            'æ¯æ—¥', 'ä¸€æ—¥', 'æ—¥è¨ˆ', 'ä»¥æ¯æ—¥',  # æ—¥è–ªç›¸é—œé—œéµè©
            'æ¯æœˆ', 'æœˆè¨ˆ', 'ä»¥æ¯æœˆ', 'åŸºæœ¬å·¥è³‡', 'æœˆè–ª', 'åº•è–ª',  # è–ªè³‡åƒè€ƒæ•¸æ“š
            'æ‰€å¾—', 'è–ªè³‡æ‰€å¾—', 'å¹´æ”¶å…¥',  # è–ªè³‡åƒè€ƒæ•¸æ“š
            'æ­¤æœ‰', 'å¯è­‰', 'ç‚ºè­‰', 'æ”¶æ“š', 'ç™¼ç¥¨', 'è­‰æ˜',  # è­‰æ“šç›¸é—œ
            'ç¶“æŸ¥', 'æŸ¥æ˜', 'ç¶“å¯©ç†',  # åˆ¤æ±ºæ›¸ç”¨èª
            'ä½œç‚ºè¨ˆç®—åŸºæº–', 'è¨ˆç®—åŸºæº–', 'åŸºæº–',  # è¨ˆç®—åŸºæº–ç›¸é—œ
            'å…±è¨ˆ', 'ç¸½è¨ˆ', 'åˆè¨ˆ', 'å°è¨ˆ', 'è¨ˆ',  # ç¸½å’Œé¡é—œéµè©
            'å…±', 'ç¸½', 'åˆ',  # ç¸½å’Œç°¡å¯«
        ]

        amounts = []
        lines = clean_text.split('\n')

        for line in lines:
            # æ‰¾å‡ºè©²è¡Œä¸­çš„æ‰€æœ‰é‡‘é¡
            line_amounts = re.findall(r'(\d+)\s*å…ƒ', line)

            for amt_str in line_amounts:
                try:
                    amount = int(amt_str)
                    # æ”¹é€²å°é¡æª¢æ¸¬é‚è¼¯ï¼šåªè·³ééå¸¸å°çš„é‡‘é¡ï¼Œä½†ä¿ç•™å¯èƒ½çš„è²»ç”¨
                    if amount < 10:  # åªè·³éæ¥µå°é¡ï¼ˆå¯èƒ½æ˜¯ç·¨è™Ÿç­‰ï¼‰
                        continue
                    
                    # å°æ–¼50-500å…ƒä¹‹é–“çš„é‡‘é¡ï¼Œéœ€è¦æ›´åš´æ ¼çš„æª¢æŸ¥ç¢ºä¿æ˜¯çœŸæ­£çš„è²»ç”¨
                    if 10 <= amount <= 500:
                        # æª¢æŸ¥æ˜¯å¦æ˜ç¢ºæåˆ°æ˜¯è²»ç”¨ã€æ”¯å‡ºç­‰
                        small_amount_context = line[max(0, line.find(amt_str + 'å…ƒ') - 30):line.find(amt_str + 'å…ƒ') + 20]
                        small_amount_keywords = ['è²»ç”¨', 'æ”¯å‡º', 'èŠ±è²»', 'æå¤±', 'è³ å„Ÿ', 'é†«ç™‚', 'äº¤é€š', 'æ›è™Ÿ']
                        if not any(keyword in small_amount_context for keyword in small_amount_keywords):
                            print(f"ğŸ” ã€å°é¡è·³éã€‘{amount}å…ƒ - ç„¡æ˜ç¢ºè²»ç”¨é—œéµè©: {small_amount_context}")
                            continue
                        else:
                            print(f"ğŸ” ã€å°é¡ä¿ç•™ã€‘{amount}å…ƒ - åŒ…å«è²»ç”¨é—œéµè©: {small_amount_context}")

                    # æª¢æŸ¥é‡‘é¡å‘¨åœçš„ä¸Šä¸‹æ–‡
                    # æ‰¾åˆ°é‡‘é¡åœ¨åŸæ–‡ä¸­çš„ä½ç½®
                    amount_pos = line.find(amt_str + 'å…ƒ')
                    if amount_pos == -1:
                        continue

                    # æå–é‡‘é¡å‰å¾Œ100å€‹å­—ç¬¦çš„ä¸Šä¸‹æ–‡ï¼ˆæ“´å¤§ç¯„åœï¼‰
                    start = max(0, amount_pos - 100)
                    end = min(len(line), amount_pos + 100)
                    context = line[start:end]
                    
                    # ä¹Ÿæª¢æŸ¥æ•´å€‹æ–‡æœ¬ä¸­è©²é‡‘é¡çš„ä¸Šä¸‹æ–‡ï¼ˆç”¨æ–¼æ›´æº–ç¢ºçš„åŸºæº–æª¢æ¸¬ï¼‰
                    full_text_pos = clean_text.find(amt_str + 'å…ƒ')
                    if full_text_pos != -1:
                        full_start = max(0, full_text_pos - 150)
                        full_end = min(len(clean_text), full_text_pos + 150)
                        full_context = clean_text[full_start:full_end]
                    else:
                        full_context = context

                    # å…ˆæª¢æŸ¥æ˜¯å¦åŒ…å«æœ‰æ•ˆæ±‚å„Ÿé—œéµè©
                    is_valid_claim = any(keyword in context for keyword in valid_claim_keywords)
                    
                    if is_valid_claim:
                        # å¦‚æœæ˜¯æœ‰æ•ˆæ±‚å„Ÿé …ç›®ï¼Œå†æª¢æŸ¥æ˜¯å¦éœ€è¦æ’é™¤
                        should_exclude = any(keyword in context for keyword in exclude_keywords)
                        
                        # æª¢æŸ¥æ˜¯å¦ç‚ºè¨ˆç®—åŸºæº–ï¼ˆä¸­é–“å€¼ï¼‰- ä½¿ç”¨å®Œæ•´ä¸Šä¸‹æ–‡
                        calculation_pattern = f'{amount}å…ƒè¨ˆç®—'
                        clean_full_context = full_context.replace(',', '')
                        is_calculation_base = (
                            'ä½œç‚ºè¨ˆç®—åŸºæº–' in full_context or
                            ('ä»¥æ¯æ—¥' in full_context and 'ä½œç‚ºè¨ˆç®—åŸºæº–' in full_context) or
                            ('åŸºæœ¬å·¥è³‡' in full_context and calculation_pattern in clean_full_context)  # å¦‚æœæ˜¯ "XXXå…ƒè¨ˆç®—" æ ¼å¼å°±æ˜¯åŸºæº–
                        )
                        
                        # æª¢æŸ¥æ˜¯å¦ç‚ºæœ€çµ‚æ±‚å„Ÿé‡‘é¡ï¼ˆé‡‘é¡ç›´æ¥è·Ÿåœ¨é—œéµè©å¾Œé¢ï¼‰
                        claim_patterns = [
                            f'æå¤±ç‚º{amount}å…ƒ',  # å—æœ‰ä¹‹è–ªè³‡æå¤±ç‚º113625å…ƒ
                            f'å…±è«‹æ±‚{amount}å…ƒ',  # å…±è«‹æ±‚270000å…ƒ
                            f'æ”¯å‡º.*{amount}å…ƒ',  # æ”¯å‡ºé†«ç™‚è²»ç”¨255830å…ƒ
                            f'è«‹æ±‚.*{amount}å…ƒ',  # è«‹æ±‚æ…°æ’«é‡‘300000å…ƒ
                            f'è³ å„Ÿ.*{amount}å…ƒ'   # è³ å„Ÿé‡‘é¡
                        ]
                        # æª¢æŸ¥æ™‚ç¢ºä¿é‡‘é¡ç·Šè·Ÿåœ¨æè¿°å¾Œé¢ï¼Œä¸æ˜¯ä½œç‚ºè¨ˆç®—åŸºæº–
                        context_clean = context.replace(',', '')
                        is_final_claim = False
                        for pattern in claim_patterns:
                            if re.search(pattern, context_clean):
                                # é€²ä¸€æ­¥æª¢æŸ¥ï¼šå¦‚æœåŒæ™‚åŒ…å«"è¨ˆç®—"é—œéµè©ï¼Œå¯èƒ½æ˜¯åŸºæº–è€Œéæœ€çµ‚é‡‘é¡
                                if 'è¨ˆç®—' not in context or f'{amount}å…ƒè¨ˆç®—' not in context_clean:
                                    is_final_claim = True
                                    break
                        
                        
                        # å¦‚æœæ˜¯è¨ˆç®—åŸºæº–ä½†ä¸æ˜¯æœ€çµ‚æ±‚å„Ÿï¼Œæ’é™¤
                        if is_calculation_base and not is_final_claim:
                            should_exclude = True
                        # å¦‚æœæ˜¯æœ€çµ‚æ±‚å„Ÿï¼Œå³ä½¿åŒ…å«å…¶ä»–æ’é™¤é—œéµè©ä¹Ÿä¸æ’é™¤    
                        elif is_final_claim:
                            should_exclude = False
                        
                        if should_exclude:
                            print(f"ğŸ” ã€æ’é™¤ã€‘{amount:,}å…ƒ - åŒ…å«æ’é™¤é—œéµè©: {context[:50]}...")
                        else:
                            print(f"ğŸ” ã€æœ‰æ•ˆã€‘{amount:,}å…ƒ - ä¸Šä¸‹æ–‡: {context[:50]}...")
                            amounts.append(amount)
                    else:
                        print(f"ğŸ” ã€è·³éã€‘{amount:,}å…ƒ - ç„¡æ˜ç¢ºæ±‚å„Ÿé—œéµè©: {context[:50]}...")

                except ValueError:
                    continue

        # 4. æ”¹é€²çš„å»é‡é‚è¼¯ï¼ˆæŒ‰é …ç›®é¡å‹åˆ†çµ„ï¼‰
        damage_items = {}  # æŒ‰é¡å‹åˆ†çµ„ï¼š{é¡å‹: [é‡‘é¡åˆ—è¡¨]}
        
        for line in clean_text.split('\n'):
            # è­˜åˆ¥æå®³é …ç›®æ¨™é¡Œè¡Œï¼ˆå¦‚ï¼šï¼ˆä¸€ï¼‰é†«ç™‚è²»ç”¨38,073å…ƒ æˆ– 1. é†«ç™‚è²»ç”¨38,073å…ƒï¼‰
            if (re.match(r'^[ï¼ˆ][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å][ï¼‰]', line.strip()) or 
                re.match(r'^[ãˆ ãˆ¡ãˆ¢ãˆ£ãˆ¤ãˆ¥ãˆ¦ãˆ§ãˆ¨ãˆ©]', line.strip()) or 
                re.match(r'^\d+\.\s*[^\d]*\d+å…ƒ', line.strip())):
                line_amounts = re.findall(r'(\d+)\s*å…ƒ', line)
                for amt_str in line_amounts:
                    try:
                        amount = int(amt_str)
                        if amount >= 100:  # æ’é™¤å°é¡
                            # åˆ¤æ–·æå®³é¡å‹
                            damage_type = "å…¶ä»–"
                            if 'é ä¼°é†«ç™‚' in line or 'æœªä¾†é†«ç™‚' in line or 'é è¨ˆé†«ç™‚' in line:
                                damage_type = "é ä¼°é†«ç™‚è²»ç”¨"
                            elif 'é†«ç™‚ç”¨å“' in line:
                                damage_type = "é†«ç™‚ç”¨å“è²»ç”¨"
                            elif 'é†«ç™‚å™¨æ' in line:
                                damage_type = "é†«ç™‚å™¨æè²»ç”¨"
                            elif 'é†«ç™‚' in line:
                                damage_type = "é†«ç™‚è²»ç”¨"
                            elif 'çœ‹è­·' in line:
                                damage_type = "çœ‹è­·è²»ç”¨"
                            elif 'ç‰™é½’' in line or 'å‡ç‰™' in line:
                                damage_type = "ç‰™é½’æå®³"
                            elif 'æ…°æ’«' in line or 'ç²¾ç¥' in line:
                                damage_type = "ç²¾ç¥æ…°æ’«é‡‘"
                            elif any(keyword in line for keyword in ['äº¤é€š', 'å¾€è¿”', 'ä¾†å›', 'è»Šè²»', 'æ²¹è²»', 'åœè»Š', 'éè·¯', 'é€šè¡Œ']):
                                damage_type = "äº¤é€šè²»ç”¨"
                            elif 'è»Šè¼›' in line or 'æ©Ÿè»Š' in line or 'ä¿®å¾©' in line or 'ä¿®ç†' in line or 'ç¶­ä¿®' in line:
                                damage_type = "è»Šè¼›ä¿®å¾©è²»ç”¨"
                            elif 'ç„¡æ³•å·¥ä½œ' in line or 'å·¥ä½œæå¤±' in line:
                                damage_type = "ç„¡æ³•å·¥ä½œæå¤±"
                            elif 'å·¥ä½œ' in line or 'æ”¶å…¥' in line or 'æå¤±' in line:
                                damage_type = "å·¥ä½œæå¤±"
                            
                            if damage_type not in damage_items:
                                damage_items[damage_type] = []
                            damage_items[damage_type].append(amount)
                            print(f"ğŸ” ã€ç¢ºèªé …ç›®ã€‘{damage_type}: {amount:,}å…ƒ")
                    except ValueError:
                        continue
        
        # æ¯ç¨®æå®³é¡å‹åªå–ä¸€å€‹é‡‘é¡ï¼ˆé€šå¸¸æ¨™é¡Œè¡Œçš„é‡‘é¡æ˜¯æ­£ç¢ºçš„ï¼‰
        final_amounts = []
        for damage_type, amounts_list in damage_items.items():
            if amounts_list:
                # å–è©²é¡å‹çš„ç¬¬ä¸€å€‹é‡‘é¡ï¼ˆæ¨™é¡Œè¡Œï¼‰
                final_amounts.append(amounts_list[0])
                print(f"âœ… ã€æ¡ç”¨ã€‘{damage_type}: {amounts_list[0]:,}å…ƒ")

        # å¦‚æœæ²’æœ‰æ‰¾åˆ°çµæ§‹åŒ–é …ç›®ï¼Œä½¿ç”¨ç¬¬ä¸€éšæ®µæå–çš„æ‰€æœ‰æœ‰æ•ˆé‡‘é¡ï¼ˆå»é‡ï¼‰
        if not final_amounts and amounts:
            print("ğŸ” ã€å‚™ç”¨æ–¹æ¡ˆã€‘æœªæ‰¾åˆ°çµæ§‹åŒ–é …ç›®ï¼Œä½¿ç”¨ç¬¬ä¸€éšæ®µçš„æœ‰æ•ˆé‡‘é¡")
            # ç°¡å–®å»é‡ï¼šä¿ç•™ä¸åŒçš„é‡‘é¡
            seen_amounts = set()
            for amount in amounts:
                if amount not in seen_amounts:
                    final_amounts.append(amount)
                    seen_amounts.add(amount)
                    print(f"âœ… ã€æ¡ç”¨ã€‘é‡‘é¡: {amount:,}å…ƒ")

        print(f"ğŸ” ã€å‚³çµ±é‡‘é¡æå–ã€‘å»é‡å¾Œæœ‰æ•ˆé‡‘é¡: {final_amounts}")
        print(f"ğŸ” ã€å‚³çµ±é‡‘é¡æå–ã€‘æœ€çµ‚ç¸½è¨ˆ: {sum(final_amounts):,}å…ƒ")

        return final_amounts

    def _extract_damage_items_from_text(self, text: str) -> Dict[str, List[Dict]]:
        """å¾æ–‡æœ¬ä¸­ç²¾ç¢ºæå–æå®³é …ç›® - æ”¹å–„ç‰ˆ"""
        # æŒ‰åŸå‘Šåˆ†çµ„
        plaintiff_damages = {}
        
        # åˆ†å¥è™•ç†
        sentences = re.split(r'[ã€‚]', text)
        
        for sentence in sentences:
            # æ”¹å–„åŸå‘Šè­˜åˆ¥ - æ”¯æ´æ›´å¤šæ ¼å¼
            plaintiff_patterns = [
                r'åŸå‘Š([^ï¼Œã€‚ï¼›ã€\så› ç”±å°±æ”¯å‡ºç”¢ç”Ÿä¹‹]{2,4})',     # æ¨™æº–æ ¼å¼ï¼Œé¿å…æŠ“å–å‹•è©
                r'([^ï¼Œã€‚ï¼›ã€\s]{2,4})ä¹‹é†«ç™‚è²»ç”¨',              # å¾æå®³é …ç›®åæ¨åŸå‘Š
                r'([^ï¼Œã€‚ï¼›ã€\s]{2,4})ä¹‹äº¤é€šè²»',                # å¾æå®³é …ç›®åæ¨åŸå‘Š  
                r'([^ï¼Œã€‚ï¼›ã€\s]{2,4})ä¹‹å·¥è³‡æå¤±',              # å¾æå®³é …ç›®åæ¨åŸå‘Š
                r'([^ï¼Œã€‚ï¼›ã€\s]{2,4})ä¹‹æ…°æ’«é‡‘',                # å¾æå®³é …ç›®åæ¨åŸå‘Š
                r'åŸå‘Š([^ï¼Œã€‚ï¼›ã€\s]{2,4})å› ',                  # åŸå‘ŠXXå› æ ¼å¼
                r'åŸå‘Š([^ï¼Œã€‚ï¼›ã€\s]{2,4})[æ”¯å—]',               # åŸå‘ŠXXæ”¯å‡º/å—æœ‰æ ¼å¼
            ]
            
            plaintiff = None
            for pattern in plaintiff_patterns:
                plaintiff_match = re.search(pattern, sentence)
                if plaintiff_match:
                    potential_plaintiff = plaintiff_match.group(1)
                    # é©—è­‰æ˜¯å¦ç‚ºæœ‰æ•ˆå§“åï¼ˆæ’é™¤ç„¡é—œè©å½™ï¼‰
                    if self._is_valid_plaintiff_name(potential_plaintiff):
                        plaintiff = potential_plaintiff
                        break
            
            # å¦‚æœæ²’æœ‰æ‰¾åˆ°å…·é«”å§“åï¼Œæª¢æŸ¥æ˜¯å¦æœ‰é€šç”¨çš„"åŸå‘Š"
            if not plaintiff:
                if 'åŸå‘Š' in sentence and ('å› ' in sentence or 'æ”¯å‡º' in sentence or 'å—æœ‰' in sentence or 'ç”¢ç”Ÿ' in sentence):
                    plaintiff = "åŸå‘Š"  # ä½¿ç”¨é€šç”¨åŸå‘Šæ¨™è­˜
                
            if not plaintiff:
                continue
                
            if plaintiff not in plaintiff_damages:
                plaintiff_damages[plaintiff] = []
            
            # ç²¾ç¢ºåŒ¹é…å„ç¨®æå®³é¡å‹ - æ”¹å–„ç‰ˆ
            # ========== ç²¾ç¥æ…°æ’«é‡‘ï¼ˆçµ•å°æœ€é«˜å„ªå…ˆç´š - ç³»çµ±é–‹ç™¼æ ¸å¿ƒç²¾ç¥ï¼‰ ==========
            if any(keyword in sentence for keyword in [
                # æ ¸å¿ƒé—œéµè©ï¼ˆæœ€é«˜æ¬Šé‡ï¼‰
                'æ…°æ’«é‡‘', 'ç²¾ç¥æ…°æ’«é‡‘', 
                # ä¸»è¦é—œéµè©
                'ç²¾ç¥è³ å„Ÿ', 'ç²¾ç¥æå®³', 'ç²¾ç¥æå®³è³ å„Ÿ', 'ç²¾ç¥ç—›è‹¦',
                # æ³•å¾‹ç”¨èª
                'éè²¡ç”¢ä¸Šä¹‹æå®³', 'éè²¡ç”¢æå®³', 'äººæ ¼æ¬Š', 'äººæ ¼æ³•ç›Š',
                # æè¿°æ€§é—œéµè©
                'èº«å¿ƒç—›è‹¦', 'ç²¾ç¥å—å‰µ', 'å¿ƒç†å‰µå‚·', 'ç²¾ç¥å‰µå‚·',
                'ç²¾ç¥ä¸Šç—›è‹¦', 'èº«å¿ƒé­å—ç—›è‹¦', 'ç²¾ç¥ä¸Šæå®³', 
                'ç²¾ç¥ä¸Šæ‰€å—ç—›è‹¦', 'èº«å¿ƒæ‰€å—ç—›è‹¦', 'å¿ƒéˆå‰µå‚·',
                # å®Œæ•´è¡¨é”æ¨¡å¼
                'å› .*èº«å¿ƒ.*ç—›è‹¦', 'å› .*ç²¾ç¥.*ç—›è‹¦', 'ç²¾ç¥.*è«‹æ±‚',
                'èº«å¿ƒ.*è³ å„Ÿ', 'ç²¾ç¥.*æ…°æ’«', 'ç—›è‹¦.*æ…°æ’«',
                # å¯¬æ³›åŒ¹é…ï¼ˆç¢ºä¿ä¸éºæ¼ï¼‰
                'ç²¾ç¥', 'æ…°æ’«', 'ç—›è‹¦'
            ]):
                # è¶…ç´šéˆæ´»çš„æ­£å‰‡è¡¨é”å¼é™£åˆ— - æ¶µè“‹æ‰€æœ‰å¯èƒ½è¡¨é”æ–¹å¼
                amount_patterns = [
                    # 1. æ¨™æº–ç›´æ¥æ ¼å¼
                    r'(?:æ…°æ’«é‡‘|ç²¾ç¥æ…°æ’«é‡‘).*?(\d+(?:,\d{3})*)\s*å…ƒ',
                    r'(?:ç²¾ç¥è³ å„Ÿ|ç²¾ç¥æå®³è³ å„Ÿ|ç²¾ç¥æå®³).*?(\d+(?:,\d{3})*)\s*å…ƒ',
                    r'(?:ç²¾ç¥ç—›è‹¦).*?(\d+(?:,\d{3})*)\s*å…ƒ',
                    
                    # 2. æ³•å¾‹ç”¨èªæ ¼å¼
                    r'(?:éè²¡ç”¢ä¸Šä¹‹æå®³|éè²¡ç”¢æå®³|äººæ ¼æ¬Š|äººæ ¼æ³•ç›Š).*?(\d+(?:,\d{3})*)\s*å…ƒ',
                    
                    # 3. æè¿°æ€§æ ¼å¼
                    r'(?:èº«å¿ƒç—›è‹¦|ç²¾ç¥å—å‰µ|å¿ƒç†å‰µå‚·|ç²¾ç¥å‰µå‚·).*?(?:è³ å„Ÿ|æ…°æ’«é‡‘).*?(\d+(?:,\d{3})*)\s*å…ƒ',
                    r'(?:ç²¾ç¥ä¸Šç—›è‹¦|èº«å¿ƒé­å—ç—›è‹¦|ç²¾ç¥ä¸Šæå®³).*?(\d+(?:,\d{3})*)\s*å…ƒ',
                    
                    # 4. è«‹æ±‚æ ¼å¼ï¼ˆå«å‹•ä½œè©ï¼‰
                    r'è«‹æ±‚.*?(?:æ…°æ’«é‡‘|ç²¾ç¥æ…°æ’«é‡‘|ç²¾ç¥è³ å„Ÿ|ç²¾ç¥æå®³).*?(\d+(?:,\d{3})*)\s*å…ƒ',
                    r'æ”¯å‡º.*?(?:æ…°æ’«é‡‘|ç²¾ç¥æ…°æ’«é‡‘|ç²¾ç¥è³ å„Ÿ).*?(\d+(?:,\d{3})*)\s*å…ƒ',
                    r'å—æœ‰.*?(?:æ…°æ’«é‡‘|ç²¾ç¥æ…°æ’«é‡‘|ç²¾ç¥è³ å„Ÿ).*?(\d+(?:,\d{3})*)\s*å…ƒ',
                    
                    # 5. ç¸½è¨ˆæ ¼å¼
                    r'(?:æ…°æ’«é‡‘|ç²¾ç¥æ…°æ’«é‡‘|ç²¾ç¥è³ å„Ÿ).*?(?:ç¸½è¨ˆ|å…±è¨ˆ|è¨ˆ|åˆè¨ˆ).*?(\d+(?:,\d{3})*)\s*å…ƒ',
                    r'(?:ç¸½è¨ˆ|å…±è¨ˆ|è¨ˆ|åˆè¨ˆ).*?(?:æ…°æ’«é‡‘|ç²¾ç¥æ…°æ’«é‡‘|ç²¾ç¥è³ å„Ÿ).*?(\d+(?:,\d{3})*)\s*å…ƒ',
                    
                    # 6. å€’è£æ ¼å¼
                    r'(\d+(?:,\d{3})*)\s*å…ƒ.*?(?:æ…°æ’«é‡‘|ç²¾ç¥æ…°æ’«é‡‘|ç²¾ç¥è³ å„Ÿ)',
                    
                    # 7. è¤‡é›œå¥å¼æ ¼å¼
                    r'å› .*?(?:äº‹æ•…|æ„å¤–).*?(?:èº«å¿ƒ|ç²¾ç¥).*?(?:ç—›è‹¦|å‰µå‚·|æå®³).*?(?:è«‹æ±‚|è³ å„Ÿ).*?(\d+(?:,\d{3})*)\s*å…ƒ',
                    r'(?:èº«å¿ƒ|ç²¾ç¥).*?(?:ç—›è‹¦|å‰µå‚·|æå®³).*?(?:è«‹æ±‚|è³ å„Ÿ).*?(\d+(?:,\d{3})*)\s*å…ƒ',
                    
                    # 8. æœ€å¯¬æ³›åŒ¹é…ï¼ˆåŒ…å«ç²¾ç¥æˆ–æ…°æ’«ä¸”æœ‰é‡‘é¡ï¼‰
                    r'ç²¾ç¥.*?(\d+(?:,\d{3})*)\s*å…ƒ',
                    r'æ…°æ’«.*?(\d+(?:,\d{3})*)\s*å…ƒ',
                    r'ç—›è‹¦.*?(\d+(?:,\d{3})*)\s*å…ƒ'
                ]
                
                amount_match = None
                matched_pattern = None
                
                # æŒ‰é †åºå˜—è©¦åŒ¹é…ï¼Œå„ªå…ˆä½¿ç”¨æ›´ç²¾ç¢ºçš„æ¨¡å¼
                for i, pattern in enumerate(amount_patterns):
                    amount_match = re.search(pattern, sentence)
                    if amount_match:
                        matched_pattern = i + 1
                        break
                
                if amount_match:
                    amount = int(amount_match.group(1).replace(',', ''))
                    print(f"ğŸ¯ æ…°æ’«é‡‘æª¢æ¸¬æˆåŠŸï¼š{amount:,}å…ƒ (æ¨¡å¼ {matched_pattern})")
                    plaintiff_damages[plaintiff].append({
                        'name': 'ç²¾ç¥æ…°æ’«é‡‘',
                        'amount': amount,
                        'description': f'{plaintiff}å› æœ¬æ¬¡äº‹æ•…æ‰€å—ç²¾ç¥ç—›è‹¦ä¹‹æ…°æ’«é‡‘'
                    })
            
            # æœªä¾†é†«ç™‚è²»ç”¨ï¼ˆå„ªå…ˆæª¢æŸ¥ï¼Œé¿å…è¢«ä¸€èˆ¬é†«ç™‚è²»ç”¨èª¤åˆ¤ï¼‰
            elif any(keyword in sentence for keyword in ['æœªä¾†é†«ç™‚', 'å°‡ä¾†é†«ç™‚', 'é ä¼°.*é†«ç™‚', '15è¬', 'åäº”è¬']):
                amount_match = re.search(r'(?:æœªä¾†é†«ç™‚|å°‡ä¾†é†«ç™‚|é ä¼°.*é†«ç™‚).*?(\d+(?:,\d{3})*)\s*å…ƒ|15è¬|åäº”è¬', sentence)
                if amount_match:
                    # è™•ç†"15è¬"çš„æƒ…æ³
                    if '15è¬' in sentence or 'åäº”è¬' in sentence:
                        amount = 150000
                    else:
                        amount = int(amount_match.group(1).replace(',', ''))
                    
                    plaintiff_damages[plaintiff].append({
                        'name': 'æœªä¾†é†«ç™‚è²»ç”¨',
                        'amount': amount,
                        'description': f'{plaintiff}å› æœ¬æ¬¡äº‹æ•…é ä¼°ä¹‹æœªä¾†é†«ç™‚è²»ç”¨'
                    })
            
            # é†«ç™‚è²»ç”¨ï¼ˆä½¿ç”¨æ›´éˆæ´»çš„æ­£å‰‡è¡¨é”å¼ï¼‰
            elif any(keyword in sentence for keyword in ['é†«ç™‚è²»ç”¨', 'é†«ç™‚è²»', 'å°±é†«è²»ç”¨', 'é†«ç™‚']):
                amount_match = re.search(r'(?:é†«ç™‚è²»ç”¨|é†«ç™‚è²»|å°±é†«è²»ç”¨|é†«ç™‚).*?(\d+(?:,\d{3})*)\s*å…ƒ', sentence)
                if amount_match:
                    plaintiff_damages[plaintiff].append({
                        'name': 'é†«ç™‚è²»ç”¨',
                        'amount': int(amount_match.group(1).replace(',', '')),
                        'description': f'{plaintiff}å› æœ¬æ¬¡äº‹æ•…å—å‚·å°±é†«ä¹‹é†«ç™‚è²»ç”¨'
                    })
            
            # è²¡ç‰©æå¤±ï¼ˆå„ªå…ˆæª¢æŸ¥ï¼Œé¿å…è¢«å…¶ä»–é …ç›®èª¤åˆ¤ï¼‰
            elif any(keyword in sentence for keyword in ['è²¡ç‰©æå¤±', 'è²¡ç‰©', 'è¡£ç‰©', 'ç‰©å“æå¤±']):
                amount_match = re.search(r'(?:è²¡ç‰©æå¤±|è²¡ç‰©|è¡£ç‰©|ç‰©å“æå¤±).*?(\d+(?:,\d{3})*)\s*å…ƒ', sentence)
                if amount_match:
                    plaintiff_damages[plaintiff].append({
                        'name': 'è²¡ç‰©æå¤±',
                        'amount': int(amount_match.group(1).replace(',', '')),
                        'description': f'{plaintiff}å› æœ¬æ¬¡äº‹æ•…æ‰€å—ä¹‹è²¡ç‰©æå¤±'
                    })
            
            # ç‰™é½’ç›¸é—œè²»ç”¨ï¼ˆæ“´å±•æª¢æ¸¬ç¯„åœï¼‰
            elif any(keyword in sentence for keyword in [
                # å‡ç‰™ç›¸é—œ
                'å‡ç‰™', 'å‡ç‰™è£ç½®', 'å‡ç‰™è„«è½', 'é‡æ–°å®‰è£å‡ç‰™',
                # ç‰™é½’æå®³ç›¸é—œ
                'ç‰™é½’æå®³', 'ç‰™é½’å—æ', 'ç‰™é½’æ²»ç™‚', 'ç‰™é½’ç ´æ',
                # ç‰™ç§‘ç›¸é—œ
                'ç‰™ç§‘', 'ç‰™ç§‘æ²»ç™‚', 'ç‰™é†«', 'å£è…”', 'å£è…”æ²»ç™‚',
                # é€šç”¨ç‰™é½’
                'ç‰™é½’', 'é½’'
            ]) and 'è²¡ç‰©' not in sentence:
                # æ›´éˆæ´»çš„æ­£å‰‡è¡¨é”å¼
                amount_patterns = [
                    # æ¨™æº–æ ¼å¼
                    r'(?:å‡ç‰™|ç‰™é½’|ç‰™ç§‘|å£è…”|é½’).*?(?:è²»ç”¨|æå®³|æ²»ç™‚|è£ç½®).*?(\d+(?:,\d{3})*)\s*å…ƒ',
                    # æå®³è²»ç”¨æ ¼å¼
                    r'(?:ç‰™é½’æå®³è²»ç”¨|ç‰™ç§‘è²»ç”¨|å‡ç‰™è²»ç”¨|å£è…”.*?è²»ç”¨).*?(\d+(?:,\d{3})*)\s*å…ƒ',
                    # æ²»ç™‚è²»ç”¨æ ¼å¼
                    r'(?:ç‰™é½’|å‡ç‰™|ç‰™ç§‘|å£è…”).*?æ²»ç™‚.*?è²»ç”¨.*?(\d+(?:,\d{3})*)\s*å…ƒ',
                    # å¯¬æ³›åŒ¹é…
                    r'(?:ç‰™é½’|å‡ç‰™|ç‰™ç§‘|å£è…”|é½’).*?(\d+(?:,\d{3})*)\s*å…ƒ'
                ]
                
                amount_match = None
                for pattern in amount_patterns:
                    amount_match = re.search(pattern, sentence)
                    if amount_match:
                        break
                
                if amount_match:
                    # æ ¹æ“šå…§å®¹åˆ¤æ–·å…·é«”é¡å‹
                    if any(word in sentence for word in ['å‡ç‰™', 'è£ç½®', 'è„«è½', 'é‡æ–°å®‰è£']):
                        damage_name = 'å‡ç‰™è²»ç”¨'
                        description = f'{plaintiff}å› æœ¬æ¬¡äº‹æ•…æ‰€éœ€ä¹‹å‡ç‰™è²»ç”¨'
                    elif any(word in sentence for word in ['æå®³', 'å—æ', 'ç ´æ']):
                        damage_name = 'ç‰™é½’æå®³è²»ç”¨'
                        description = f'{plaintiff}å› æœ¬æ¬¡äº‹æ•…ç‰™é½’å—æä¹‹æ²»ç™‚è²»ç”¨'
                    elif any(word in sentence for word in ['å£è…”']):
                        damage_name = 'å£è…”æ²»ç™‚è²»ç”¨'
                        description = f'{plaintiff}å› æœ¬æ¬¡äº‹æ•…æ‰€éœ€ä¹‹å£è…”æ²»ç™‚è²»ç”¨'
                    elif any(word in sentence for word in ['æ²»ç™‚', 'ç‰™ç§‘', 'ç‰™é†«']):
                        damage_name = 'ç‰™ç§‘æ²»ç™‚è²»ç”¨'
                        description = f'{plaintiff}å› æœ¬æ¬¡äº‹æ•…æ‰€éœ€ä¹‹ç‰™ç§‘æ²»ç™‚è²»ç”¨'
                    else:
                        damage_name = 'ç‰™é½’ç›¸é—œè²»ç”¨'
                        description = f'{plaintiff}å› æœ¬æ¬¡äº‹æ•…ä¹‹ç‰™é½’ç›¸é—œè²»ç”¨'
                    
                    print(f"ğŸ¦· ç‰™é½’ç›¸é—œè²»ç”¨æª¢æ¸¬æˆåŠŸï¼š{damage_name} {int(amount_match.group(1).replace(',', '')):,}å…ƒ")
                    plaintiff_damages[plaintiff].append({
                        'name': damage_name,
                        'amount': int(amount_match.group(1).replace(',', '')),
                        'description': description
                    })
            
            # æ‰‹è¡“è²»ç”¨
            elif any(keyword in sentence for keyword in ['æ‰‹è¡“', 'é–‹åˆ€', 'æ‰‹è¡“è²»']):
                amount_match = re.search(r'(?:æ‰‹è¡“|é–‹åˆ€|æ‰‹è¡“è²»).*?(\d+(?:,\d{3})*)\s*å…ƒ', sentence)
                if amount_match:
                    plaintiff_damages[plaintiff].append({
                        'name': 'æ‰‹è¡“è²»ç”¨',
                        'amount': int(amount_match.group(1).replace(',', '')),
                        'description': f'{plaintiff}å› æœ¬æ¬¡äº‹æ•…æ‰€éœ€ä¹‹æ‰‹è¡“è²»ç”¨'
                    })
            
            # ç‡Ÿé¤Šå“è²»ç”¨
            elif any(keyword in sentence for keyword in ['ç‡Ÿé¤Šå“', 'è† åŸè›‹ç™½', 'ç‡Ÿé¤Š', 'è£œå“']):
                amount_match = re.search(r'(?:ç‡Ÿé¤Šå“|è† åŸè›‹ç™½|ç‡Ÿé¤Š|è£œå“).*?(\d+(?:,\d{3})*)\s*å…ƒ', sentence)
                if amount_match:
                    plaintiff_damages[plaintiff].append({
                        'name': 'ç‡Ÿé¤Šå“è²»ç”¨',
                        'amount': int(amount_match.group(1).replace(',', '')),
                        'description': f'{plaintiff}å› æœ¬æ¬¡äº‹æ•…æ‰€éœ€ä¹‹ç‡Ÿé¤Šå“è²»ç”¨'
                    })
            
            # çœ‹è­·è²»ç”¨
            elif any(keyword in sentence for keyword in ['çœ‹è­·', 'ç…§è­·', 'è­·ç†', 'çœ‹è­·è²»']):
                amount_match = re.search(r'(?:çœ‹è­·|ç…§è­·|è­·ç†|çœ‹è­·è²»).*?(\d+(?:,\d{3})*)\s*å…ƒ', sentence)
                if amount_match:
                    plaintiff_damages[plaintiff].append({
                        'name': 'çœ‹è­·è²»ç”¨',
                        'amount': int(amount_match.group(1).replace(',', '')),
                        'description': f'{plaintiff}å› æœ¬æ¬¡äº‹æ•…æ‰€éœ€ä¹‹çœ‹è­·è²»ç”¨'
                    })
            
            # äº¤é€šè²»
            elif any(keyword in sentence for keyword in ['äº¤é€šè²»ç”¨', 'äº¤é€šè²»', 'å¾€è¿”è²»ç”¨', 'ä¾†å›è²»ç”¨', 'è»Šè²»', 'æ²¹è²»', 'åœè»Šè²»', 'éè·¯è²»', 'é€šè¡Œè²»']):
                # ä½¿ç”¨æ›´éˆæ´»çš„æ­£å‰‡è¡¨é”å¼ï¼Œå…è¨±é—œéµè©å’Œé‡‘é¡ä¹‹é–“æœ‰å…¶ä»–æ–‡å­—
                amount_match = re.search(r'(?:äº¤é€šè²»ç”¨|äº¤é€šè²»|å¾€è¿”è²»ç”¨|ä¾†å›è²»ç”¨|è»Šè²»|æ²¹è²»|åœè»Šè²»|éè·¯è²»|é€šè¡Œè²»).*?(\d+(?:,\d{3})*)\s*å…ƒ', sentence)
                if amount_match:
                    plaintiff_damages[plaintiff].append({
                        'name': 'äº¤é€šè²»ç”¨',
                        'amount': int(amount_match.group(1).replace(',', '')),
                        'description': f'{plaintiff}å› æœ¬æ¬¡äº‹æ•…æ‰€ç”Ÿä¹‹äº¤é€šè²»ç”¨'
                    })
            
            # å‹å‹•èƒ½åŠ›æ¸›æï¼ˆå„ªå…ˆæª¢æŸ¥ï¼Œé¿å…è¢«å·¥ä½œæå¤±èª¤åˆ¤ï¼‰
            elif any(keyword in sentence for keyword in ['å‹å‹•èƒ½åŠ›', 'å‹å‹•æå¤±', 'æ¸›å°‘å‹å‹•', 'å‹å‹•æ¸›å°‘', 'å¤±èƒ½']):
                amount_match = re.search(r'(?:å‹å‹•èƒ½åŠ›|å‹å‹•æå¤±|æ¸›å°‘å‹å‹•|å‹å‹•æ¸›å°‘|å¤±èƒ½).*?(\d+(?:,\d{3})*)\s*å…ƒ', sentence)
                if amount_match:
                    plaintiff_damages[plaintiff].append({
                        'name': 'å‹å‹•èƒ½åŠ›æ¸›æ',
                        'amount': int(amount_match.group(1).replace(',', '')),
                        'description': f'{plaintiff}å› æœ¬æ¬¡äº‹æ•…å‹å‹•èƒ½åŠ›æ¸›æä¹‹æå¤±'
                    })
            
            # å·¥ä½œæå¤±
            elif any(keyword in sentence for keyword in ['å·¥è³‡æå¤±', 'è–ªè³‡æå¤±', 'å·¥ä½œæå¤±', 'ä¸èƒ½å·¥ä½œ', 'ç„¡æ³•å·¥ä½œ', 'æ”¶å…¥æå¤±']):
                amount_match = re.search(r'(?:å·¥è³‡æå¤±|è–ªè³‡æå¤±|å·¥ä½œæå¤±|æ”¶å…¥æå¤±|æå¤±|å—æœ‰).*?(\d+(?:,\d{3})*)\s*å…ƒ', sentence)
                if amount_match:
                    plaintiff_damages[plaintiff].append({
                        'name': 'å·¥ä½œæå¤±',
                        'amount': int(amount_match.group(1).replace(',', '')),
                        'description': f'{plaintiff}å› æœ¬æ¬¡äº‹æ•…ç„¡æ³•å·¥ä½œä¹‹æ”¶å…¥æå¤±'
                    })
            
            
            # è»Šè¼›ç¶­ä¿®è²»ç”¨
            elif any(keyword in sentence for keyword in ['è»Šè¼›ç¶­ä¿®', 'ç¶­ä¿®è²»', 'ä¿®ç†è²»', 'è»Šè¼›ä¿®ç†']):
                amount_match = re.search(r'(?:è»Šè¼›ç¶­ä¿®|ç¶­ä¿®è²»|ä¿®ç†è²»|è»Šè¼›ä¿®ç†).*?(\d+(?:,\d{3})*)\s*å…ƒ', sentence)
                if amount_match:
                    plaintiff_damages[plaintiff].append({
                        'name': 'è»Šè¼›ç¶­ä¿®è²»',
                        'amount': int(amount_match.group(1).replace(',', '')),
                        'description': f'{plaintiff}å› æœ¬æ¬¡äº‹æ•…è»Šè¼›ç¶­ä¿®ä¹‹è²»ç”¨'
                    })
            
            # è»Šè¼›è²¶å€¼
            if any(keyword in sentence for keyword in ['è²¶å€¼', 'è²¶æ', 'åƒ¹å€¼æ¸›æ']):
                amount_match = re.search(r'(?:è²¶æ|æ¸›æ)\s*(\d+(?:,\d{3})*)\s*å…ƒ', sentence)
                if amount_match:
                    plaintiff_damages[plaintiff].append({
                        'name': 'è»Šè¼›è²¶å€¼æå¤±',
                        'amount': int(amount_match.group(1).replace(',', '')),
                        'description': 'ç³»çˆ­è»Šè¼›å› æœ¬æ¬¡äº‹æ•…ä¹‹åƒ¹å€¼æ¸›æ'
                    })
            
            # é‘‘å®šè²»
            if 'é‘‘å®šè²»' in sentence:
                amount_match = re.search(r'é‘‘å®šè²»\s*(\d+(?:,\d{3})*)\s*å…ƒ', sentence)
                if amount_match:
                    plaintiff_damages[plaintiff].append({
                        'name': 'é‘‘å®šè²»ç”¨',
                        'amount': int(amount_match.group(1).replace(',', '')),
                        'description': 'è»Šè¼›æå®³é‘‘å®šè²»ç”¨'
                    })
        
        # è™•ç†å…±åŒè²»ç”¨åˆ†é…
        plaintiff_damages = self._handle_shared_costs(plaintiff_damages, text)
        
        return plaintiff_damages
    
    def _handle_shared_costs(self, plaintiff_damages: Dict[str, List[Dict]], text: str) -> Dict[str, List[Dict]]:
        """è™•ç†å…±åŒè²»ç”¨çš„åˆ†é…"""
        if len(plaintiff_damages) <= 1:
            return plaintiff_damages
        
        # æª¢æ¸¬å…±åŒè²»ç”¨é …ç›®
        shared_costs = []
        plaintiffs = list(plaintiff_damages.keys())
        
        # æ‰¾å‡ºè¢«åˆ†é…çµ¦ç¬¬ä¸€å€‹åŸå‘Šä½†æ‡‰è©²æ˜¯å…±åŒè²»ç”¨çš„é …ç›®
        for plaintiff, damages in plaintiff_damages.items():
            for damage in damages[:]:  # ä½¿ç”¨åˆ‡ç‰‡é¿å…ä¿®æ”¹éç¨‹ä¸­å‡ºéŒ¯
                # æª¢æŸ¥è©²è²»ç”¨æ˜¯å¦åœ¨å…±åŒæå®³æ®µè½ä¸­æåˆ°
                if self._is_shared_cost(damage, text, plaintiffs):
                    shared_costs.append(damage)
                    damages.remove(damage)  # å¾å€‹äººè²»ç”¨ä¸­ç§»é™¤
        
        # å°‡å…±åŒè²»ç”¨å¹³åˆ†çµ¦æ‰€æœ‰åŸå‘Š
        if shared_costs:
            print(f"ğŸ” æª¢æ¸¬åˆ° {len(shared_costs)} é …å…±åŒè²»ç”¨éœ€è¦åˆ†é…")
            for shared_cost in shared_costs:
                shared_amount = shared_cost['amount']
                split_amount = shared_amount // len(plaintiffs)
                
                print(f"   {shared_cost['name']} {shared_amount:,}å…ƒ â†’ æ¯äºº {split_amount:,}å…ƒ")
                
                for plaintiff in plaintiffs:
                    if plaintiff not in plaintiff_damages:
                        plaintiff_damages[plaintiff] = []
                    
                    plaintiff_damages[plaintiff].append({
                        'name': shared_cost['name'],
                        'amount': split_amount,
                        'description': f"åŸå‘Š{plaintiff}åˆ†æ”¤ä¹‹{shared_cost['name']}"
                    })
        
        return plaintiff_damages
    
    def _is_shared_cost(self, damage: Dict, text: str, plaintiffs: List[str]) -> bool:
        """åˆ¤æ–·æ˜¯å¦ç‚ºå…±åŒè²»ç”¨"""
        # æ›´ç²¾ç¢ºåœ°æª¢æŸ¥æ˜¯å¦åœ¨å…±åŒæå®³æ®µè½ä¸­
        shared_section_pattern = r'[ï¼ˆ(][ä¸‰3][ï¼‰)].*?åŸå‘Š.*?ã€.*?å…±åŒ.*?æå®³.*?(?=[ï¼ˆ(][å››4][ï¼‰)]|å››ã€|$)'
        shared_section_match = re.search(shared_section_pattern, text, re.DOTALL)
        
        if not shared_section_match:
            return False
        
        shared_section_text = shared_section_match.group(0)
        
        # æª¢æŸ¥è©²è²»ç”¨çš„é‡‘é¡æ˜¯å¦åœ¨å…±åŒæ®µè½ä¸­å‡ºç¾
        damage_amount = damage['amount']
        amount_pattern = f"{damage_amount:,}å…ƒ"
        
        # åŒæ™‚æª¢æŸ¥é‡‘é¡å’Œè²»ç”¨é¡å‹é—œéµè©
        damage_keywords = {
            'çœ‹è­·è²»ç”¨': ['çœ‹è­·', 'ç…§è­·', 'è­·ç†'],
            'äº¤é€šè²»ç”¨': ['äº¤é€š', 'è»Šè²»', 'è¨ˆç¨‹è»Š'],
            'é†«ç™‚è²»ç”¨': ['é†«ç™‚'],
            'æ‰‹è¡“è²»ç”¨': ['æ‰‹è¡“'],
            'å‡ç‰™è²»ç”¨': ['å‡ç‰™', 'ç‰™é½’'],
            'ç²¾ç¥æ…°æ’«é‡‘': ['æ…°æ’«', 'ç²¾ç¥']
        }
        
        if damage['name'] in damage_keywords:
            keywords = damage_keywords[damage['name']]
            
            # æª¢æŸ¥é‡‘é¡æ˜¯å¦åœ¨å…±åŒæ®µè½ä¸­
            if amount_pattern in shared_section_text:
                # æª¢æŸ¥å°æ‡‰çš„é—œéµè©æ˜¯å¦ä¹Ÿåœ¨åŒä¸€æ®µè½ä¸­
                if any(keyword in shared_section_text for keyword in keywords):
                    print(f"   âœ… {damage['name']} {damage_amount:,}å…ƒ ç¢ºèªç‚ºå…±åŒè²»ç”¨")
                    return True
        
        return False
    
    def _format_damage_items(self, damage_items: Dict[str, List[Dict]]) -> str:
        """æ ¼å¼åŒ–æå®³é …ç›®"""
        if not damage_items:
            return ""
        
        result = "ä¸‰ã€æå®³é …ç›®ï¼š\n"
        for idx, (plaintiff, damages) in enumerate(damage_items.items()):
            chinese_num = self._chinese_num(idx + 1)
            result += f"\nï¼ˆ{chinese_num}ï¼‰åŸå‘Š{plaintiff}ä¹‹æå®³ï¼š\n"
            
            for i, damage in enumerate(damages, 1):
                result += f"{i}. {damage['name']}ï¼š{damage['amount']:,}å…ƒ\n"
                result += f"   èªªæ˜ï¼š{damage['description']}\n"
            
            # å°è¨ˆ
            subtotal = sum(d['amount'] for d in damages)
            result += f"\nå°è¨ˆï¼š{subtotal:,}å…ƒ\n"
        
        # ç¸½è¨ˆ
        total = sum(sum(d['amount'] for d in damages) for damages in damage_items.values())
        result += f"\næå®³ç¸½è¨ˆï¼šæ–°å°å¹£{total:,}å…ƒæ•´"
        
        return result
    
    def _is_valid_plaintiff_name(self, name: str) -> bool:
        """é©—è­‰æ˜¯å¦ç‚ºæœ‰æ•ˆçš„åŸå‘Šå§“å"""
        if not name or len(name) < 2 or len(name) > 5:
            return False
        
        # æ’é™¤ç„¡é—œè©å½™ï¼Œä½†ä¸åŒ…å«å¯èƒ½å‡ºç¾åœ¨å§“åä¸­çš„å­—
        invalid_keywords = [
            'é†«ç™‚', 'äº¤é€š', 'å·¥è³‡', 'æå¤±', 'è²»ç”¨', 'æ…°æ’«', 'ç²¾ç¥',
            'è»Šè¼›', 'è²¶å€¼', 'é‘‘å®š', 'è³ å„Ÿ', 'å…ƒ', 'å› ', 'å—', 'æ”¯å‡º',
            'å°±é†«', 'äº‹æ•…', 'æœ¬æ¬¡', 'æ‰€', 'ä¹‹', 'ç­‰', 'å…±', 'è¨ˆ',
            'åŒ…å«', 'ç¸½', 'åˆ', 'é‡‘é¡', 'é …ç›®', 'ä¸»å¼µ', 'æ ¹æ“š', 'åª³å©¦',
            'å…¨æ—¥', 'æ³•å®š', 'ä»£ç†', 'è¨ºæ–·', 'è­‰æ˜', 'ä¾ç…§', 'åƒè€ƒ'
        ]
        
        for keyword in invalid_keywords:
            if keyword in name:
                return False
        
        # æª¢æŸ¥æ˜¯å¦åŒ…å«æ•¸å­—
        if any(char.isdigit() for char in name):
            return False
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºå¸¸è¦‹çš„ä¸­æ–‡å§“åæ ¼å¼
        # ç°¡å–®çš„ä¸­æ–‡å­—ç¬¦æª¢æŸ¥
        chinese_char_count = sum(1 for char in name if '\u4e00' <= char <= '\u9fff')
        return chinese_char_count >= 2  # è‡³å°‘åŒ…å«2å€‹ä¸­æ–‡å­—ç¬¦
    
    def _standardize_names_in_facts(self, facts_text: str, parties: dict) -> str:
        """çµ±ä¸€äº‹å¯¦æ®µè½ä¸­çš„å§“åï¼Œç¢ºä¿èˆ‡ç•¶äº‹äººä¿¡æ¯ä¸€è‡´"""
        if not parties:
            return facts_text
        
        result = facts_text
        
        # è™•ç†åŸå‘Šå§“åçµ±ä¸€
        if 'åŸå‘Š' in parties and parties['åŸå‘Š'] != 'åŸå‘Š':
            correct_plaintiffs = [name.strip() for name in parties['åŸå‘Š'].split('ã€') if name.strip()]
            
            # æŸ¥æ‰¾ä¸¦æ›¿æ›å¯èƒ½çš„å§“åè®Šé«”
            for correct_name in correct_plaintiffs:
                if len(correct_name) >= 2:
                    # æŸ¥æ‰¾ç›¸ä¼¼å§“åï¼ˆå¯èƒ½æ˜¯ç­†èª¤æˆ–ä¸åŒå¯«æ³•ï¼‰
                    # ä¾‹å¦‚ï¼šç¾…è˜­å´´ -> ç¾…é–å´´
                    surname = correct_name[0]  # å–å§“æ°
                    
                    # æŸ¥æ‰¾ä»¥ç›¸åŒå§“æ°é–‹é ­ä½†åå­—ä¸åŒçš„å¯èƒ½è®Šé«”
                    import re
                    pattern = rf'åŸå‘Š{surname}[^ï¼Œã€‚ï¼›ã€\s]{{1,3}}'
                    matches = re.finditer(pattern, result)
                    
                    for match in matches:
                        found_name = match.group(0).replace('åŸå‘Š', '')
                        if found_name != correct_name and len(found_name) == len(correct_name):
                            # æ›¿æ›ç‚ºæ­£ç¢ºå§“å
                            result = result.replace(f'åŸå‘Š{found_name}', f'åŸå‘Š{correct_name}')
                            print(f"ğŸ”§ å§“åçµ±ä¸€: '{found_name}' -> '{correct_name}'")
        
        # è™•ç†è¢«å‘Šå§“åçµ±ä¸€ï¼ˆé¡ä¼¼é‚è¼¯ï¼‰
        if 'è¢«å‘Š' in parties and parties['è¢«å‘Š'] != 'è¢«å‘Š':
            correct_defendants = [name.strip() for name in parties['è¢«å‘Š'].split('ã€') if name.strip()]
            
            for correct_name in correct_defendants:
                if len(correct_name) >= 2:
                    surname = correct_name[0]
                    
                    import re
                    pattern = rf'è¢«å‘Š{surname}[^ï¼Œã€‚ï¼›ã€\s]{{1,3}}'
                    matches = re.finditer(pattern, result)
                    
                    for match in matches:
                        found_name = match.group(0).replace('è¢«å‘Š', '')
                        if found_name != correct_name and len(found_name) == len(correct_name):
                            result = result.replace(f'è¢«å‘Š{found_name}', f'è¢«å‘Š{correct_name}')
                            print(f"ğŸ”§ å§“åçµ±ä¸€: '{found_name}' -> '{correct_name}'")
        
        return result

# ===== ä¸»è¦äº’å‹•åŠŸèƒ½ =====

def interactive_generate_lawsuit():
    """äº’å‹•å¼èµ·è¨´ç‹€ç”Ÿæˆï¼ˆæ¢å¾©å¤šè¡Œè¼¸å…¥ç‰ˆæœ¬ï¼‰"""
    print("=" * 80)
    print("ğŸ›ï¸  è»Šç¦èµ·è¨´ç‹€ç”Ÿæˆå™¨ - æ··åˆç‰ˆæœ¬ï¼ˆæ•´åˆçµæ§‹åŒ–é‡‘é¡è™•ç†ï¼‰")
    print("=" * 80)
    print("ğŸ‘‹ æ­¡è¿ä½¿ç”¨ï¼æˆ‘æœƒç‚ºæ‚¨ç”Ÿæˆå°ˆæ¥­çš„èµ·è¨´ç‹€ï¼ŒåŒ…å«ï¼š")
    print("   ğŸ“„ ç›¸ä¼¼æ¡ˆä¾‹æª¢ç´¢")
    print("   âš–ï¸ é©ç”¨æ³•æ¢åˆ†æ")
    print("   ğŸ“‹ å®Œæ•´èµ·è¨´ç‹€ç”Ÿæˆ")
    print("ğŸ’¡ æ”¯æ´çµæ§‹åŒ–é‡‘é¡è™•ç†ï¼Œè‡ªå‹•ä¿®æ­£è¨ˆç®—éŒ¯èª¤")
    print()
    
    print("ğŸ“ ä½¿ç”¨æ–¹æ³•ï¼š")
    print("   1. è«‹ä¸€æ¬¡æ€§è¼¸å…¥å®Œæ•´çš„ä¸‰æ®µå…§å®¹")
    print("   2. å¯ä»¥å¤šè¡Œè¼¸å…¥ï¼Œæ›è¡Œç¹¼çºŒ")
    print("   3. è¼¸å…¥å®Œæˆå¾Œè¼¸å…¥ 'END' ç¢ºèª")
    print("   4. è¼¸å…¥ 'quit' å¯é€€å‡ºç¨‹å¼")
    print()
    
    # åˆå§‹åŒ–ç”Ÿæˆå™¨
    generator = HybridCoTGenerator()
    
    print("ğŸ“ è«‹è¼¸å…¥å®Œæ•´çš„è»Šç¦æ¡ˆä»¶è³‡æ–™ï¼š")
    print("ğŸ“‹ è«‹åŒ…å«ä»¥ä¸‹ä¸‰å€‹éƒ¨åˆ†ï¼š")
    print("   ä¸€ã€äº‹æ•…ç™¼ç”Ÿç·£ç”±ï¼š[è©³è¿°è»Šç¦ç¶“é]")
    print("   äºŒã€åŸå‘Šå—å‚·æƒ…å½¢ï¼š[æè¿°å‚·å‹¢]")
    print("   ä¸‰ã€è«‹æ±‚è³ å„Ÿçš„äº‹å¯¦æ ¹æ“šï¼š[åˆ—å‡ºæå®³é …ç›®å’Œé‡‘é¡]")
    print()
    print("ğŸ’¡ æç¤ºï¼šå¯ä»¥æ›è¡Œè¼¸å…¥ï¼Œå®Œæˆå¾Œè¼¸å…¥ 'END' ç¢ºèª")
    print("=" * 60)
    print("ğŸ¯ è«‹é–‹å§‹è¼¸å…¥ï¼ˆå®Œæˆå¾Œè¼¸å…¥ 'END' æˆ– 'end' ç¢ºèªï¼‰ï¼š")
    
    # å¤šè¡Œè¼¸å…¥æ¨¡å¼
    user_input_lines = []
    while True:
        try:
            line = input()
            if line.strip().upper() in ['END', 'QUIT', 'EXIT', 'é€€å‡º']:
                if line.strip().upper() == 'QUIT' or line.strip().upper() == 'EXIT' or line.strip() == 'é€€å‡º':
                    print("ğŸ‘‹ æ„Ÿè¬ä½¿ç”¨ï¼Œå†è¦‹ï¼")
                    return
                break
            user_input_lines.append(line)
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ç”¨æˆ¶ä¸­æ–·ï¼Œç¨‹åºé€€å‡º")
            return
        except EOFError:
            break
    
    user_query = '\n'.join(user_input_lines).strip()
    
    if not user_query:
        print("âš ï¸ è«‹è¼¸å…¥æœ‰æ•ˆçš„å…§å®¹")
        return
    
    print("ğŸ”„ æ­£åœ¨è™•ç†...")
    
    try:
        # åˆ†æ®µæå–è³‡è¨Š
        sections = extract_sections(user_query)
        
        # æå–ç•¶äº‹äºº
        parties = extract_parties(user_query)
        
        # æ¡ˆä»¶åˆ†é¡å’Œæª¢ç´¢ç›¸ä¼¼æ¡ˆä¾‹
        accident_facts = sections.get("accident_facts", user_query)
        case_type = determine_case_type(accident_facts, parties)
        
        print("âœ… LLMæœå‹™æ­£å¸¸")
        print()
        
        # ç›¸ä¼¼æ¡ˆä¾‹æª¢ç´¢ï¼ˆè©³ç´°æ¨¡å¼ï¼‰
        similar_cases = []
        if FULL_MODE:
            print("ğŸ” æª¢ç´¢ç›¸ä¼¼æ¡ˆä¾‹...")
            # ä½¿ç”¨å›ºå®šåƒæ•¸é€²è¡Œæª¢ç´¢
            k_final = 3
            initial_retrieve_count = 15
            use_multi_stage = True
            
            print(f"ğŸ”§ æª¢ç´¢ç­–ç•¥: ç›®æ¨™{k_final}å€‹æ¡ˆä¾‹ â†’ æª¢ç´¢{initial_retrieve_count}å€‹æ®µè½")
            
            try:
                # è©³ç´°åŸ·è¡Œæª¢ç´¢
                query_vector = embed(accident_facts)
                if query_vector:
                    hits = es_search(query_vector, case_type, top_k=initial_retrieve_count, label="Facts", quiet=False)
                    if hits:
                        print(f"ğŸ” ESåŸå§‹çµæœ: æ‰¾åˆ°{len(hits)}å€‹æ®µè½")
                        
                        # å¤šéšæ®µæ¨¡å¼
                        candidate_case_ids = []
                        for hit in hits:
                            case_id = hit['_source'].get('case_id')
                            if case_id and case_id not in candidate_case_ids:
                                candidate_case_ids.append(case_id)
                        
                        print(f"ğŸ”„ å»é‡å¾Œçµæœ: {len(candidate_case_ids)}å€‹å”¯ä¸€æ¡ˆä¾‹")
                        final_case_ids = candidate_case_ids[:k_final]
                        print(f"ğŸ“Œ æœ€çµ‚é¸å–: {len(final_case_ids)}å€‹æ¡ˆä¾‹ {final_case_ids}")
                        
                        if candidate_case_ids:
                            reranked_case_ids = rerank_case_ids_by_paragraphs(
                                accident_facts, 
                                candidate_case_ids[:k_final*2],
                                label="Facts",
                                quiet=False
                            )
                            final_case_ids = reranked_case_ids[:k_final]
                            print(f"ğŸ“˜ Rerankå¾Œæœ€çµ‚é †åº: {final_case_ids}")
                            
                            similar_cases = get_complete_cases_content(final_case_ids)
                            
                            # é¡¯ç¤ºè©³ç´°æ¡ˆä¾‹åˆ†æ
                            print()
                            print(f"ğŸ“‹ è©³ç´°æ¡ˆä¾‹åˆ†æ (åƒ…é¡¯ç¤ºå‰ {len(similar_cases)} å€‹æœ€ç›¸é—œæ¡ˆä¾‹):")
                            print("=" * 80)
                            print()
                            
                            for i, (case_content, case_id) in enumerate(zip(similar_cases, final_case_ids)):
                                # å¾hitsä¸­æ‰¾åˆ°å°æ‡‰çš„åˆ†æ•¸
                                score = 0.0
                                for hit in hits:
                                    if hit['_source'].get('case_id') == case_id:
                                        score = hit['_score']
                                        break
                                
                                print(f"ğŸ“„ ç›¸ä¼¼æ¡ˆä¾‹ {i+1}: Case ID {case_id}")
                                print(f"ğŸ¯ ESç›¸ä¼¼åº¦åˆ†æ•¸: {score:.4f}")
                                print("-" * 50)
                                case_preview = case_content[:500] + "..." if len(case_content) > 500 else case_content
                                print(case_preview)
                                print()
                                if i < len(similar_cases) - 1:
                                    print()
                        else:
                            # ç°¡å–®æ¨¡å¼
                            if hits:
                                first_hit = hits[0]['_source']
                                content_fields = ['original_text', 'content', 'text', 'facts_content', 'chunk_content', 'body']
                                content_field = None
                                for field in content_fields:
                                    if field in first_hit and first_hit[field]:
                                        content_field = field
                                        break
                                
                                if content_field:
                                    similar_cases = [hit['_source'].get(content_field, '') for hit in hits[:k_final] if hit['_source'].get(content_field)]
                                else:
                                    # Fallback
                                    similar_cases = []
                                    for hit in hits[:k_final]:
                                        all_text = " ".join([str(v) for k, v in hit['_source'].items() 
                                                           if isinstance(v, str) and len(v) > 50])
                                        if all_text:
                                            similar_cases.append(all_text)
            except Exception as e:
                similar_cases = []
        
        # ===== é–‹å§‹æ··åˆæ¨¡å¼ç”Ÿæˆ =====
        print(f"\nğŸ¯ é–‹å§‹æ··åˆæ¨¡å¼ç”Ÿæˆ...")
        print("ğŸ“ äº‹å¯¦ + æ³•æ¢ + æå®³ï¼šæ¨™æº–æ–¹å¼")
        print("ğŸ§  çµè«–ï¼šCoTæ–¹å¼ï¼ˆè¨ˆç®—ç¸½é‡‘é¡ï¼‰")
        print()
        
        # ç”Ÿæˆäº‹å¯¦æ®µè½
        print("ğŸ“ ç”Ÿæˆäº‹å¯¦æ®µè½...")
        facts = generator.generate_standard_facts(accident_facts, similar_cases, parties)
        print("âœ… äº‹å¯¦æ®µè½ç”Ÿæˆå®Œæˆ")
        
        # ç”Ÿæˆæ³•å¾‹ä¾æ“š
        print("âš–ï¸ ç”Ÿæˆæ³•å¾‹ä¾æ“š...")
        
        # çµ±è¨ˆç›¸ä¼¼æ¡ˆä¾‹ä½¿ç”¨çš„æ³•æ¢
        if similar_cases and 'final_case_ids' in locals():
            try:
                print("ğŸ“Š åˆ†æç›¸ä¼¼æ¡ˆä¾‹ä½¿ç”¨çš„æ³•æ¢...")
                similar_laws_stats = get_similar_cases_laws_stats(final_case_ids)
                if similar_laws_stats:
                    print("ğŸ“‹ ç›¸ä¼¼æ¡ˆä¾‹å¸¸ç”¨æ³•æ¢çµ±è¨ˆ:")
                    for law_name, count in similar_laws_stats[:5]:  # é¡¯ç¤ºå‰5å€‹æœ€å¸¸ç”¨çš„
                        print(f"   â€¢ {law_name}: {count}æ¬¡")
                    print()
            except Exception as e:
                print(f"âš ï¸ æ³•æ¢çµ±è¨ˆåˆ†æå¤±æ•—: {e}")
        
        laws = generator.generate_standard_laws(
            sections.get("accident_facts", user_query),
            sections.get("injuries", ""),
            parties,
            sections.get("compensation_facts", "")
        )
        print("âœ… æ³•å¾‹ä¾æ“šç”Ÿæˆå®Œæˆ")
        
        # ç”Ÿæˆæå®³è³ å„Ÿ
        print("ğŸ’° ç”Ÿæˆæå®³è³ å„Ÿ...")
        compensation_text = sections.get("compensation_facts", user_query)
        damages = generator.generate_smart_compensation(
            sections.get("injuries", ""),
            compensation_text, 
            parties
        )
        print("âœ… æå®³è³ å„Ÿç”Ÿæˆå®Œæˆ")
        
        # ç”ŸæˆCoTçµè«–
        print("ğŸ§  ç”ŸæˆCoTçµè«–ï¼ˆå«ç¸½é‡‘é¡è¨ˆç®—ï¼‰...")
        conclusion = generator.generate_cot_conclusion_with_structured_analysis(
            sections.get("accident_facts", user_query),
            damages,  # ä½¿ç”¨ç”Ÿæˆå¾Œçš„æå®³æ®µè½ï¼Œè€Œä¸æ˜¯åŸå§‹è¼¸å…¥
            parties
        )
        print("âœ… CoTçµè«–ç”Ÿæˆå®Œæˆ")
        print()
        print("âœ… æ‰€æœ‰ç”Ÿæˆæ­¥é©Ÿå®Œæˆï¼")
        
        # æå–é©ç”¨æ³•æ¢
        applicable_laws = determine_applicable_laws(
            sections.get("accident_facts", user_query),
            sections.get("injuries", ""),
            sections.get("compensation_facts", ""),
            parties
        )
        
        # ===== è¼¸å‡ºæ ¸å¿ƒçµæœ =====
        print("\n" + "=" * 60)
        print("âš–ï¸ é©ç”¨æ³•æ¢")
        print("=" * 60)
        
        for i, law in enumerate(applicable_laws, 1):
            print(f"{i}. {law}")
        
        print("\n" + "=" * 60)
        print("ğŸ“‹ ç”Ÿæˆçš„èµ·è¨´ç‹€")
        print("=" * 60)
        
        print(f"\n{facts}")
        print(f"\n{laws}")
        print(f"\n{damages}")
        print(f"\n{conclusion}")
        
        print("\n" + "=" * 60)
        print("âœ… èµ·è¨´ç‹€ç”Ÿæˆå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
        print("è«‹æª¢æŸ¥è¼¸å…¥æ ¼å¼æˆ–è¯ç¹«ç³»çµ±ç®¡ç†å“¡")

def main():
    """ä¸»ç¨‹åºå…¥å£"""
    try:
        # æª¢æŸ¥ä¾è³´
        print("ğŸ”§ æª¢æŸ¥ç³»çµ±ä¾è³´...")
        print(f"ğŸ“Š æª¢ç´¢æ¨¡å¼ï¼š{'å®Œæ•´æ¨¡å¼' if FULL_MODE else 'ç°¡åŒ–æ¨¡å¼'}")
        print(f"ğŸ—ï¸ çµæ§‹åŒ–è™•ç†å™¨ï¼š{'å¯ç”¨' if STRUCTURED_PROCESSOR_AVAILABLE else 'ä¸å¯ç”¨'}")
        print(f"ğŸ“ åŸºæœ¬æ¨™æº–åŒ–å™¨ï¼š{'å¯ç”¨' if BASIC_STANDARDIZER_AVAILABLE else 'ä¸å¯ç”¨'}")
        
        # å•Ÿå‹•äº’å‹•ç•Œé¢
        interactive_generate_lawsuit()
        
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç”¨æˆ¶ä¸­æ–·ï¼Œç¨‹åºé€€å‡º")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºåŸ·è¡ŒéŒ¯èª¤ï¼š{str(e)}")

if __name__ == "__main__":
    main()