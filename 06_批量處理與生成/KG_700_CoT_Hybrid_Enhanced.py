#!/usr/bin/env python3
"""
KG_700_CoT_Hybrid_Enhanced.py
åŸºæ–¼åŸhybridç‰ˆæœ¬çš„å¢å¼·ç‰ˆæœ¬ï¼šæ¸›å°‘ç¡¬ç·¨ç¢¼ï¼Œæå‡é€šç”¨æ€§
ä¿ç•™åŸæœ‰æ¶æ§‹ï¼Œåªé‡å°å•é¡Œéƒ¨åˆ†é€²è¡Œæ”¹é€²
"""

import os
import re
import json
import requests
import time
import sys
from typing import List, Dict, Any, Optional
from collections import Counter

# å°å…¥å¿…è¦æ¨¡çµ„
try:
    import torch
    from transformers import AutoTokenizer, AutoModel
    from elasticsearch import Elasticsearch
    from neo4j import GraphDatabase
    from dotenv import load_dotenv
    FULL_MODE = True
    print("âœ… å®Œæ•´æ¨¡å¼ï¼šæ‰€æœ‰æª¢ç´¢åŠŸèƒ½å¯ç”¨")
except ImportError as e:
    print(f"âš ï¸ éƒ¨åˆ†æ¨¡çµ„æœªå®‰è£ï¼š{e}")
    print("âš ï¸ ä½¿ç”¨ç°¡åŒ–æ¨¡å¼ï¼ˆåƒ…LLMç”ŸæˆåŠŸèƒ½ï¼‰")
    FULL_MODE = False

# å°å…¥èªç¾©è™•ç†å™¨ä½œç‚ºè¼”åŠ©
try:
    from KG_700_Semantic_Universal import SemanticLegalProcessor
    SEMANTIC_ASSISTANT_AVAILABLE = True
    print("âœ… èªç¾©è¼”åŠ©è™•ç†å™¨è¼‰å…¥æˆåŠŸ")
except ImportError:
    SEMANTIC_ASSISTANT_AVAILABLE = False
    print("âš ï¸ èªç¾©è¼”åŠ©è™•ç†å™¨æœªæ‰¾åˆ°")

# å°å…¥çµæ§‹åŒ–é‡‘é¡è™•ç†å™¨
try:
    from structured_legal_amount_processor import StructuredLegalAmountProcessor
    STRUCTURED_PROCESSOR_AVAILABLE = True
    print("âœ… çµæ§‹åŒ–é‡‘é¡è™•ç†å™¨è¼‰å…¥æˆåŠŸ")
except ImportError:
    STRUCTURED_PROCESSOR_AVAILABLE = False
    print("âš ï¸ çµæ§‹åŒ–é‡‘é¡è™•ç†å™¨æœªæ‰¾åˆ°")

# å°å…¥åŸºæœ¬é‡‘é¡æ¨™æº–åŒ–å™¨
try:
    from legal_amount_standardizer import LegalAmountStandardizer
    BASIC_STANDARDIZER_AVAILABLE = True
    print("âœ… åŸºæœ¬é‡‘é¡æ¨™æº–åŒ–å™¨è¼‰å…¥æˆåŠŸ")
except ImportError:
    BASIC_STANDARDIZER_AVAILABLE = False
    print("âš ï¸ åŸºæœ¬é‡‘é¡æ¨™æº–åŒ–å™¨æœªæ‰¾åˆ°")

# å°å…¥é€šç”¨æ ¼å¼è™•ç†å™¨
try:
    from universal_format_handler import UniversalFormatHandler
    UNIVERSAL_FORMAT_HANDLER_AVAILABLE = True
    print("âœ… é€šç”¨æ ¼å¼è™•ç†å™¨è¼‰å…¥æˆåŠŸ")
except ImportError:
    UNIVERSAL_FORMAT_HANDLER_AVAILABLE = False
    print("âš ï¸ é€šç”¨æ ¼å¼è™•ç†å™¨æœªæ‰¾åˆ°")

# ===== åŸºæœ¬è¨­å®š =====
LLM_URL = "http://localhost:11434/api/generate"
DEFAULT_MODEL = "gemma3:27b"

# ===== æª¢ç´¢ç³»çµ±è¨­å®š =====
if FULL_MODE:
    # è¼‰å…¥ç’°å¢ƒè®Šæ•¸
    env_path = os.path.join(os.path.dirname(__file__), '..', '01_è¨­å®šèˆ‡é…ç½®', '.env')
    load_dotenv(dotenv_path=env_path)
    
    # åµŒå…¥æ¨¡å‹è¨­å®š
    BERT_MODEL = "shibing624/text2vec-base-chinese"
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    try:
        TOKENIZER = AutoTokenizer.from_pretrained(BERT_MODEL)
        MODEL = AutoModel.from_pretrained(BERT_MODEL, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32).to(device)
        print("âœ… åµŒå…¥æ¨¡å‹è¼‰å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ åµŒå…¥æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        FULL_MODE = False
    
    # ES å’Œ Neo4j é€£æ¥
    try:
        # ä½¿ç”¨ requests ç›´æ¥èª¿ç”¨ ES API é¿å…ç‰ˆæœ¬å…¼å®¹æ€§å•é¡Œ
        ES_HOST = os.getenv("ELASTIC_HOST")
        ES_USER = os.getenv("ELASTIC_USER")
        ES_PASSWORD = os.getenv("ELASTIC_PASSWORD")
        ES_AUTH = (ES_USER, ES_PASSWORD)
        
        # æ¸¬è©¦ ES é€£æ¥
        response = requests.get(f"{ES_HOST}/_cluster/health", auth=ES_AUTH, verify=False)
        if response.status_code != 200:
            raise Exception(f"ESé€£æ¥å¤±æ•—: {response.status_code}")
        
        NEO4J_DRIVER = GraphDatabase.driver(
            os.getenv("NEO4J_URI"),
            auth=(os.getenv("NEO4J_USERNAME"), os.getenv("NEO4J_PASSWORD")),
        )
        CHUNK_INDEX = "legal_kg_chunks"
        print("âœ… è³‡æ–™åº«é€£æ¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ è³‡æ–™åº«é€£æ¥å¤±æ•—: {e}")
        FULL_MODE = False

def extract_parties_enhanced(text: str) -> dict:
    """å¢å¼·ç‰ˆç•¶äº‹äººæå–ï¼šçµåˆèªç¾©è¼”åŠ©å’ŒåŸæœ‰æ–¹æ³•"""
    print("ğŸ¤– ä½¿ç”¨å¢å¼·ç‰ˆç•¶äº‹äººæå–...")
    
    # å„ªå…ˆä½¿ç”¨èªç¾©è¼”åŠ©è™•ç†å™¨
    if SEMANTIC_ASSISTANT_AVAILABLE:
        try:
            semantic_processor = SemanticLegalProcessor()
            semantic_result = semantic_processor.extract_parties_semantically(text)
            
            # è½‰æ›æ ¼å¼ä»¥ç¬¦åˆåŸæœ‰æ¥å£
            result = {}
            
            # è™•ç†åŸå‘Š
            plaintiffs = [p.name for p in semantic_result.get("åŸå‘Š", []) if p.confidence > 0.3]
            result["åŸå‘Š"] = "ã€".join(plaintiffs) if plaintiffs else "åŸå‘Š"
            
            # è™•ç†è¢«å‘Š
            defendants = [p.name for p in semantic_result.get("è¢«å‘Š", []) if p.confidence > 0.3]
            result["è¢«å‘Š"] = "ã€".join(defendants) if defendants else "è¢«å‘Š"
            
            print(f"âœ… èªç¾©è¼”åŠ©æå–æˆåŠŸ: åŸå‘Š={result['åŸå‘Š']}, è¢«å‘Š={result['è¢«å‘Š']}")
            return result
            
        except Exception as e:
            print(f"âš ï¸ èªç¾©è¼”åŠ©å¤±æ•—ï¼Œä½¿ç”¨åŸæ–¹æ³•: {e}")
    
    # å‚™ç”¨ï¼šä½¿ç”¨åŸæœ‰çš„LLMæ–¹æ³•
    return extract_parties_with_llm(text)

def extract_parties_with_llm(text: str) -> dict:
    """ä½¿ç”¨LLMæå–ç•¶äº‹äººï¼ˆå¢å¼·ç‰ˆæç¤ºè©ï¼‰"""
    print("ğŸ¤– ä½¿ç”¨LLMæ™ºèƒ½æå–ç•¶äº‹äºº...")
    
    # å‰µå»ºæ›´ç²¾ç¢ºçš„æç¤ºæ¨¡æ¿
    prompt = f"""è«‹ä½ å¹«æˆ‘å¾ä»¥ä¸‹è»Šç¦æ¡ˆä»¶çš„æ³•å¾‹æ–‡ä»¶ä¸­æå–ä¸¦åˆ—å‡ºæ‰€æœ‰åŸå‘Šå’Œè¢«å‘Šçš„çœŸå¯¦å§“åã€‚

ä»¥ä¸‹æ˜¯æ¡ˆä»¶å…§å®¹ï¼š
{text}

æå–è¦æ±‚ï¼š
1. åƒ…æå–ã€ŒåŸå‘Šâ—‹â—‹â—‹ã€å’Œã€Œè¢«å‘Šâ—‹â—‹â—‹ã€ä¸­æ˜ç¢ºæåˆ°çš„çœŸå¯¦å§“å
2. ä¸è¦æå–ã€Œè¨´å¤–äººã€çš„å§“åï¼Œè¨´å¤–äººä¸æ˜¯ç•¶äº‹äºº
3. **é‡è¦**ï¼šå®Œæ•´ä¿ç•™å§“åï¼Œçµ•å°ä¸å¯æˆªæ–·æˆ–çœç•¥ä»»ä½•å­—ï¼ˆå¦‚ï¼šç¾…é–å´´å¿…é ˆå®Œæ•´å¯«æˆç¾…é–å´´ï¼Œä¸èƒ½å¯«æˆç¾…å´´ï¼‰
4. **é‡è¦**ï¼šå¦‚æœåŸæ–‡æ˜¯ã€ŒåŸå‘Šç¾…é–å´´ã€ï¼Œå¿…é ˆè¼¸å‡ºå®Œæ•´çš„ã€Œç¾…é–å´´ã€ä¸‰å€‹å­—
5. **é‡è¦**ï¼šå¦‚æœåŸæ–‡æ˜¯ã€ŒåŸå‘Šæ­é™½å¤©è¯ã€ï¼Œå¿…é ˆè¼¸å‡ºå®Œæ•´çš„ã€Œæ­é™½å¤©è¯ã€å››å€‹å­—
6. å¦‚æœæ–‡ä¸­æ²’æœ‰æ˜ç¢ºçš„å§“åï¼Œå°±ç›´æ¥å¯«ã€ŒåŸå‘Šã€ã€ã€Œè¢«å‘Šã€
7. å¤šå€‹å§“åç”¨é€—è™Ÿåˆ†éš”

è¼¸å‡ºæ ¼å¼ï¼ˆåªè¼¸å‡ºé€™å…©è¡Œï¼‰ï¼š
åŸå‘Š:å§“å1,å§“å2...
è¢«å‘Š:å§“å1,å§“å2...

ç¯„ä¾‹èªªæ˜ï¼š
- ã€ŒåŸå‘Šå³éº—å¨Ÿã€â†’ åŸå‘Š:å³éº—å¨Ÿ
- ã€Œè¢«å‘Šé„­å‡±ç¥¥ã€â†’ è¢«å‘Š:é„­å‡±ç¥¥
- ã€ŒåŸå‘Šç¾…é–å´´ã€â†’ åŸå‘Š:ç¾…é–å´´ï¼ˆå¿…é ˆä¿ç•™å®Œæ•´ä¸‰å€‹å­—ï¼‰
- ã€ŒåŸå‘Šé‚±å“å¦ã€â†’ åŸå‘Š:é‚±å“å¦ï¼ˆå¿…é ˆä¿ç•™å®Œæ•´ä¸‰å€‹å­—ï¼‰
- ã€ŒåŸå‘Šæ­é™½å¤©è¯ã€â†’ åŸå‘Š:æ­é™½å¤©è¯ï¼ˆè¤‡å§“å¿…é ˆå®Œæ•´ï¼‰
- ã€Œè¨´å¤–äººé™³æ²³ç”°ã€â†’ ä¸æ˜¯ç•¶äº‹äººï¼Œå¿½ç•¥
- å¦‚æœåªèªªã€ŒåŸå‘Šã€æ²’æœ‰å§“å â†’ åŸå‘Š:åŸå‘Š
- å¦‚æœåªèªªã€Œè¢«å‘Šã€æ²’æœ‰å§“å â†’ è¢«å‘Š:è¢«å‘Š

**å‹™å¿…ç¢ºä¿å§“åå®Œæ•´æ€§ï¼Œä¸å¯çœç•¥ä»»ä½•å­—ï¼**"""

    try:
        # èª¿ç”¨LLM
        response = requests.post(
            LLM_URL,
            json={
                "model": DEFAULT_MODEL,
                "prompt": prompt,
                "stream": False
            },
            timeout=60
        )
        
        if response.status_code == 200:
            llm_result = response.json()["response"].strip()
            print(f"ğŸ¤– LLMæå–çµæœ: {llm_result}")
            return parse_llm_parties_result(llm_result)
        else:
            print(f"âŒ LLMèª¿ç”¨å¤±æ•—: {response.status_code}")
            return extract_parties_fallback(text)
    except Exception as e:
        print(f"âŒ LLMèª¿ç”¨éŒ¯èª¤: {e}")
        return extract_parties_fallback(text)

def parse_llm_parties_result(llm_result: str) -> dict:
    """è§£æLLMç•¶äº‹äººæå–çµæœï¼ˆå¢å¼·ç‰ˆï¼‰"""
    parties = {"åŸå‘Š": "åŸå‘Š", "è¢«å‘Š": "è¢«å‘Š"}
    
    lines = llm_result.strip().split('\n')
    for line in lines:
        line = line.strip()
        
        # æ›´éˆæ´»çš„è§£ææ–¹å¼
        if "åŸå‘Š:" in line or "åŸå‘Šï¼š" in line:
            # æå–åŸå‘Šéƒ¨åˆ†
            plaintiff_part = re.split(r'åŸå‘Š[:ï¼š]', line, 1)
            if len(plaintiff_part) > 1:
                plaintiffs = plaintiff_part[1].strip()
                # æ¸…ç†å¯èƒ½çš„é¡å¤–æ–‡å­—
                plaintiffs = re.sub(r'[ï¼Œ,]\s*è¢«å‘Š.*', '', plaintiffs)
                if plaintiffs and plaintiffs != "åŸå‘Š":
                    parties["åŸå‘Š"] = plaintiffs
                    
        elif "è¢«å‘Š:" in line or "è¢«å‘Šï¼š" in line:
            # æå–è¢«å‘Šéƒ¨åˆ†
            defendant_part = re.split(r'è¢«å‘Š[:ï¼š]', line, 1)
            if len(defendant_part) > 1:
                defendants = defendant_part[1].strip()
                # æ¸…ç†å¯èƒ½çš„é¡å¤–æ–‡å­—
                defendants = re.sub(r'[ï¼Œ,]\s*åŸå‘Š.*', '', defendants)
                if defendants and defendants != "è¢«å‘Š":
                    parties["è¢«å‘Š"] = defendants
    
    # é©—è­‰å§“åå®Œæ•´æ€§
    for role, names in parties.items():
        if names not in [role, "åŸå‘Š", "è¢«å‘Š"]:  # å¦‚æœæœ‰å…·é«”å§“å
            # æª¢æŸ¥æ˜¯å¦æœ‰å¯èƒ½çš„æˆªæ–·
            name_list = [name.strip() for name in names.split('ã€') if name.strip()]
            validated_names = []
            
            for name in name_list:
                # æª¢æŸ¥å§“åé•·åº¦å’Œå­—ç¬¦é¡å‹
                if len(name) >= 2 and re.match(r'^[\u4e00-\u9fff]+$', name):
                    validated_names.append(name)
                elif len(name) == 1:
                    print(f"âš ï¸ å¯èƒ½çš„å§“åæˆªæ–·: {name}")
                    validated_names.append(name)  # ä»ç„¶ä¿ç•™ï¼Œä½†è¨˜éŒ„è­¦å‘Š
            
            if validated_names:
                parties[role] = "ã€".join(validated_names)
    
    print(f"âœ… è§£æçµæœ: åŸå‘Š={parties['åŸå‘Š']}, è¢«å‘Š={parties['è¢«å‘Š']}")
    return parties

def extract_parties_fallback(text: str) -> dict:
    """å‚™ç”¨ç•¶äº‹äººæå–æ–¹æ³•ï¼ˆæ”¹é€²ç‰ˆï¼‰"""
    print("ğŸ”„ ä½¿ç”¨å‚™ç”¨ç•¶äº‹äººæå–æ–¹æ³•...")
    parties = {"åŸå‘Š": "åŸå‘Š", "è¢«å‘Š": "è¢«å‘Š"}
    
    # æ”¹é€²çš„æ­£å‰‡è¡¨é”å¼ï¼Œæ›´å¥½åœ°æ•ç²å®Œæ•´å§“å
    plaintiff_patterns = [
        r'åŸå‘Š([^\sï¼Œã€ã€‚ï¼›ï¼šè¢«å‘Š]{2,4})(?=[\sï¼Œã€ã€‚ï¼›ï¼šè¢«å‘Š]|$)',  # 2-4å€‹å­—ç¬¦çš„å§“å
        r'åŸå‘Š\s*([^\sï¼Œã€ã€‚ï¼›ï¼šè¢«å‘Š]+)',  # æ›´å¯¬é¬†çš„åŒ¹é…
    ]
    
    defendant_patterns = [
        r'è¢«å‘Š([^\sï¼Œã€ã€‚ï¼›ï¼šåŸå‘Š]{2,4})(?=[\sï¼Œã€ã€‚ï¼›ï¼šåŸå‘Š]|$)',
        r'è¢«å‘Š\s*([^\sï¼Œã€ã€‚ï¼›ï¼šåŸå‘Š]+)',
    ]
    
    # æå–åŸå‘Š
    plaintiffs = []
    for pattern in plaintiff_patterns:
        matches = re.findall(pattern, text)
        for match in matches:
            clean_name = match.strip()
            if len(clean_name) >= 2 and clean_name not in plaintiffs:
                plaintiffs.append(clean_name)
    
    # æå–è¢«å‘Š
    defendants = []
    for pattern in defendant_patterns:
        matches = re.findall(pattern, text)
        for match in matches:
            clean_name = match.strip()
            if len(clean_name) >= 2 and clean_name not in defendants:
                defendants.append(clean_name)
    
    if plaintiffs:
        parties["åŸå‘Š"] = "ã€".join(plaintiffs)
    if defendants:
        parties["è¢«å‘Š"] = "ã€".join(defendants)
    
    return parties

def _extract_valid_claim_amounts_enhanced(text: str, parties: dict) -> List[Dict[str, Any]]:
    """å¢å¼·ç‰ˆæœ‰æ•ˆæ±‚å„Ÿé‡‘é¡æå–ï¼šçµåˆèªç¾©åˆ†æå’ŒåŸæ–¹æ³•"""
    print("ğŸ’° ä½¿ç”¨å¢å¼·ç‰ˆé‡‘é¡æå–...")
    
    # å„ªå…ˆä½¿ç”¨èªç¾©è¼”åŠ©è™•ç†å™¨
    if SEMANTIC_ASSISTANT_AVAILABLE:
        try:
            semantic_processor = SemanticLegalProcessor()
            
            # å‰µå»ºæ¨¡æ“¬çš„çµæ§‹å°è±¡
            from dataclasses import dataclass
            @dataclass
            class MockStructure:
                case_type: str = "multi_plaintiff" if "ã€" in parties.get("åŸå‘Š", "") else "single"
                plaintiff_count: int = len(parties.get("åŸå‘Š", "").split("ã€"))
                defendant_count: int = len(parties.get("è¢«å‘Š", "").split("ã€"))
                narrative_style: str = "structured_chinese"
                confidence: float = 0.8
            
            structure = MockStructure()
            amounts = semantic_processor.extract_amounts_semantically(text, structure)
            
            # è½‰æ›æ ¼å¼
            valid_amounts = []
            for amt in amounts:
                if amt.amount_type == "claim_amount" and amt.amount >= 100:
                    valid_amounts.append({
                        'amount': amt.amount,
                        'description': amt.description,
                        'context': amt.context,
                        'confidence': amt.confidence,
                        'source': 'semantic'
                    })
            
            if valid_amounts:
                print(f"âœ… èªç¾©é‡‘é¡æå–æˆåŠŸ: {len(valid_amounts)}é …")
                return valid_amounts
            
        except Exception as e:
            print(f"âš ï¸ èªç¾©é‡‘é¡æå–å¤±æ•—ï¼Œä½¿ç”¨åŸæ–¹æ³•: {e}")
    
    # å‚™ç”¨ï¼šä½¿ç”¨æ”¹é€²çš„åŸæ–¹æ³•
    return _extract_valid_claim_amounts_original_enhanced(text, parties)

def _extract_valid_claim_amounts_original_enhanced(text: str, parties: dict) -> List[Dict[str, Any]]:
    """åŸæ–¹æ³•çš„å¢å¼·ç‰ˆæœ¬ï¼šæ”¹é€²é‡‘é¡åˆ†é¡é‚è¼¯"""
    print("ğŸ’° ä½¿ç”¨å¢å¼·ç‰ˆåŸå§‹é‡‘é¡æå–...")
    
    valid_amounts = []
    
    # æ”¹é€²çš„é‡‘é¡åŒ¹é…æ¨¡å¼
    amount_patterns = [
        r'(\d+(?:,\d{3})*(?:\.\d{2})?)\\s*([è¬åƒ]?)å…ƒ',
        r'([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒè¬]+)å…ƒ',
    ]
    
    for pattern in amount_patterns:
        matches = re.finditer(pattern, text, re.IGNORECASE)
        
        for match in matches:
            amount_str = match.group(1)
            unit = match.group(2) if len(match.groups()) > 1 else ""
            
            # è½‰æ›é‡‘é¡
            try:
                if amount_str.isdigit() or ',' in amount_str or '.' in amount_str:
                    amount = float(amount_str.replace(',', ''))
                    if unit == 'è¬':
                        amount *= 10000
                    elif unit == 'åƒ':
                        amount *= 1000
                else:
                    # ä¸­æ–‡æ•¸å­—è½‰æ›
                    amount = convert_chinese_number_to_int(amount_str)
                    
                if amount < 100:  # æ’é™¤å°é¡
                    continue
                    
                # æ”¹é€²çš„ä¸Šä¸‹æ–‡åˆ†æ
                context_start = max(0, match.start() - 100)
                context_end = min(len(text), match.end() + 100)
                context = text[context_start:context_end]
                
                # æ›´ç²¾ç¢ºçš„åˆ†é¡é‚è¼¯
                is_claim_amount = classify_amount_enhanced(amount, context)
                
                if is_claim_amount:
                    # ç”Ÿæˆæè¿°
                    description = generate_amount_description(context)
                    
                    valid_amounts.append({
                        'amount': int(amount),
                        'description': description,
                        'context': context,
                        'confidence': 0.7,
                        'source': 'enhanced_original'
                    })
                    
            except Exception as e:
                print(f"âš ï¸ é‡‘é¡è½‰æ›å¤±æ•—: {amount_str} - {e}")
                continue
    
    # å»é‡å’Œæ’åº
    valid_amounts = remove_duplicate_amounts(valid_amounts)
    valid_amounts.sort(key=lambda x: x['amount'])
    
    print(f"âœ… å¢å¼·ç‰ˆé‡‘é¡æå–å®Œæˆ: {len(valid_amounts)}é …")
    return valid_amounts

def classify_amount_enhanced(amount: float, context: str) -> bool:
    """å¢å¼·ç‰ˆé‡‘é¡åˆ†é¡ï¼šæ›´æº–ç¢ºåœ°å€åˆ†æ±‚å„Ÿé‡‘é¡vsè¨ˆç®—åŸºæº–"""
    
    # æ˜ç¢ºçš„è¨ˆç®—åŸºæº–æŒ‡ç¤ºè©
    calculation_indicators = [
        'æ¯æœˆå·¥è³‡', 'æœˆå·¥è³‡', 'æœˆè–ª', 'æ¯æ—¥', 'æ—¥è–ª', 'æ™‚è–ª',
        'è¨ˆç®—', 'åŸºæº–', 'æ¨™æº–', 'ä¾æ“š', 'æŒ‰', 'ä»¥',
        'ç‚ºè¨ˆç®—åŸºæº–', 'ä½œç‚ºåŸºæº–', 'è¨ˆç®—åŸºç¤'
    ]
    
    # æ˜ç¢ºçš„æ±‚å„ŸæŒ‡ç¤ºè©
    claim_indicators = [
        'è«‹æ±‚', 'è³ å„Ÿ', 'æ”¯å‡º', 'æå¤±', 'è²»ç”¨', 'èŠ±è²»',
        'æ±‚å„Ÿ', 'çµ¦ä»˜', 'è£œå„Ÿ', 'æ…°æ’«é‡‘', 'é†«ç™‚è²»',
        'äº¤é€šè²»', 'çœ‹è­·è²»', 'ç‡Ÿé¤Šè²»', 'ç²¾ç¥æ…°æ’«'
    ]
    
    # æª¢æŸ¥è¨ˆç®—åŸºæº–
    for indicator in calculation_indicators:
        if indicator in context:
            return False
    
    # æª¢æŸ¥æ±‚å„ŸæŒ‡ç¤º
    for indicator in claim_indicators:
        if indicator in context:
            return True
    
    # é è¨­ç‚ºæ±‚å„Ÿé‡‘é¡
    return True

def generate_amount_description(context: str) -> str:
    """åŸºæ–¼ä¸Šä¸‹æ–‡ç”Ÿæˆé‡‘é¡æè¿°"""
    
    description_map = {
        'é†«ç™‚': 'é†«ç™‚è²»ç”¨',
        'äº¤é€š': 'äº¤é€šè²»ç”¨',
        'çœ‹è­·': 'çœ‹è­·è²»ç”¨',
        'ç‡Ÿé¤Š': 'ç‡Ÿé¤Šè²»ç”¨',
        'æ…°æ’«': 'ç²¾ç¥æ…°æ’«é‡‘',
        'ç²¾ç¥': 'ç²¾ç¥æ…°æ’«é‡‘',
        'è»Šè¼›': 'è»Šè¼›æå¤±',
        'è²¡ç”¢': 'è²¡ç”¢æå¤±',
        'é‘‘å®š': 'é‘‘å®šè²»ç”¨',
        'å·¥ä½œ': 'å·¥ä½œæå¤±',
        'è–ªè³‡': 'è–ªè³‡æå¤±',
    }
    
    for keyword, description in description_map.items():
        if keyword in context:
            return description
    
    return 'æå®³è³ å„Ÿ'

def convert_chinese_number_to_int(chinese_num: str) -> int:
    """è½‰æ›ä¸­æ–‡æ•¸å­—ç‚ºæ•´æ•¸ï¼ˆç°¡åŒ–ç‰ˆï¼‰"""
    chinese_map = {
        'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5,
        'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9, 'å': 10,
        'ç™¾': 100, 'åƒ': 1000, 'è¬': 10000
    }
    
    # ç°¡å–®çš„è½‰æ›é‚è¼¯
    result = 0
    for char in chinese_num:
        if char in chinese_map:
            result = chinese_map[char]
            break
    
    return result if result > 0 else 0

def remove_duplicate_amounts(amounts: List[Dict]) -> List[Dict]:
    """å»é™¤é‡è¤‡é‡‘é¡"""
    seen_amounts = set()
    unique_amounts = []
    
    for amt in amounts:
        amount_key = amt['amount']
        if amount_key not in seen_amounts:
            seen_amounts.add(amount_key)
            unique_amounts.append(amt)
    
    return unique_amounts

# ç¹¼çºŒä½¿ç”¨åŸæœ‰çš„å…¶ä»–å‡½æ•¸...
# é€™è£¡éœ€è¦å¾åŸæ–‡ä»¶è¤‡è£½å…¶ä»–å¿…è¦çš„å‡½æ•¸

if __name__ == "__main__":
    print("ğŸš€ å¢å¼·ç‰ˆHybridèµ·è¨´ç‹€ç”Ÿæˆå™¨")
    print("âœ… ä¿ç•™åŸæœ‰æ¶æ§‹ï¼Œé‡å°æ€§æ”¹é€²ç¡¬ç·¨ç¢¼å•é¡Œ")
    print("ğŸ§  æ•´åˆèªç¾©è¼”åŠ©ï¼Œæå‡æ³›åŒ–èƒ½åŠ›")
    print()
    print("ä¸»è¦æ”¹é€²ï¼š")
    print("- å¢å¼·ç‰ˆç•¶äº‹äººæå–ï¼šä¿è­‰å§“åå®Œæ•´æ€§")
    print("- æ”¹é€²ç‰ˆé‡‘é¡åˆ†é¡ï¼šæ›´æº–ç¢ºçš„æ±‚å„Ÿvsè¨ˆç®—åŸºæº–åˆ¤æ–·")
    print("- èªç¾©è¼”åŠ©ï¼šåœ¨å¯ç”¨æ™‚æä¾›é¡å¤–çš„æº–ç¢ºæ€§")
    print("- å‚™ç”¨æ©Ÿåˆ¶ï¼šç¢ºä¿ç©©å®šæ€§")