#!/usr/bin/env python3
"""
KG_700_CoT_Hybrid.py
æ··åˆæ¨¡å¼ï¼šäº‹å¯¦ã€æ³•æ¢å’Œæå®³ç”¨æ¨™æº–æ–¹æ³•ï¼Œçµè«–ç”¨CoT
å„ªåŒ–ç‰ˆæœ¬ï¼šä¿®æ­£æœªæˆå¹´èª¤åˆ¤ã€æ”¹é€²æå®³é …ç›®è™•ç†ã€å¢å¼·ç©©å®šæ€§
"""

import os
import re
import json
import requests
import time
import sys
from typing import List, Dict, Any, Optional
from collections import Counter

# å°å…¥å¿…è¦æ¨¡çµ„
try:
    import torch
    from transformers import AutoTokenizer, AutoModel
    from elasticsearch import Elasticsearch
    from neo4j import GraphDatabase
    from dotenv import load_dotenv
    FULL_MODE = True
    print("âœ… å®Œæ•´æ¨¡å¼ï¼šæ‰€æœ‰æª¢ç´¢åŠŸèƒ½å¯ç”¨")
except ImportError as e:
    print(f"âš ï¸ éƒ¨åˆ†æ¨¡çµ„æœªå®‰è£ï¼š{e}")
    print("âš ï¸ ä½¿ç”¨ç°¡åŒ–æ¨¡å¼ï¼ˆåƒ…LLMç”ŸæˆåŠŸèƒ½ï¼‰")
    FULL_MODE = False

# ===== åŸºæœ¬è¨­å®š =====
LLM_URL = "http://localhost:11434/api/generate"
DEFAULT_MODEL = "gemma3:27b"

# ===== æª¢ç´¢ç³»çµ±è¨­å®š =====
if FULL_MODE:
    # è¼‰å…¥ç’°å¢ƒè®Šæ•¸
    env_path = os.path.join(os.path.dirname(__file__), '..', '01_è¨­å®šèˆ‡é…ç½®', '.env')
    load_dotenv(dotenv_path=env_path)
    
    # åµŒå…¥æ¨¡å‹è¨­å®š
    BERT_MODEL = "shibing624/text2vec-base-chinese"
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    try:
        TOKENIZER = AutoTokenizer.from_pretrained(BERT_MODEL)
        MODEL = AutoModel.from_pretrained(BERT_MODEL, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32).to(device)
        print("âœ… åµŒå…¥æ¨¡å‹è¼‰å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ åµŒå…¥æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        FULL_MODE = False
    
    # ES å’Œ Neo4j é€£æ¥
    try:
        # ä½¿ç”¨ requests ç›´æ¥èª¿ç”¨ ES API é¿å…ç‰ˆæœ¬å…¼å®¹æ€§å•é¡Œ
        ES_HOST = os.getenv("ELASTIC_HOST")
        ES_USER = os.getenv("ELASTIC_USER")
        ES_PASSWORD = os.getenv("ELASTIC_PASSWORD")
        ES_AUTH = (ES_USER, ES_PASSWORD)
        
        # æ¸¬è©¦ ES é€£æ¥
        response = requests.get(f"{ES_HOST}/_cluster/health", auth=ES_AUTH, verify=False)
        if response.status_code != 200:
            raise Exception(f"ESé€£æ¥å¤±æ•—: {response.status_code}")
        
        NEO4J_DRIVER = GraphDatabase.driver(
            os.getenv("NEO4J_URI"),
            auth=(os.getenv("NEO4J_USERNAME"), os.getenv("NEO4J_PASSWORD")),
        )
        CHUNK_INDEX = "legal_kg_chunks"
        print("âœ… è³‡æ–™åº«é€£æ¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ è³‡æ–™åº«é€£æ¥å¤±æ•—: {e}")
        FULL_MODE = False
else:
    ES_HOST = None
    ES_AUTH = None
    NEO4J_DRIVER = None
    CHUNK_INDEX = None

# æ¡ˆä»¶é¡å‹å°ç…§è¡¨
CASE_TYPE_MAP = {
    "æ•¸ååŸå‘Š": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    "æ•¸åè¢«å‘Š": "å–®ç´”åŸè¢«å‘Šå„ä¸€", 
    "åŸè¢«å‘Šçš†æ•¸å": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    "åŸè¢«å‘Šçš†æ•¸å+Â§188åƒ±ç”¨äººæ¡ˆå‹": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    "æ•¸ååŸå‘Š+Â§188åƒ±ç”¨äººæ¡ˆå‹": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    "æ•¸åè¢«å‘Š+Â§188åƒ±ç”¨äººæ¡ˆå‹": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    "æ•¸åè¢«å‘Š+Â§187æœªæˆå¹´æ¡ˆå‹": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    "åŸè¢«å‘Šçš†æ•¸å+Â§187æœªæˆå¹´æ¡ˆå‹": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
}

# ===== è¼”åŠ©å‡½æ•¸ =====
def extract_sections(text: str) -> dict:
    """æå–æ–‡æœ¬æ®µè½"""
    result = {
        "accident_facts": "",
        "injuries": "",
        "compensation_facts": ""
    }
    
    # äº‹æ•…ç™¼ç”Ÿç·£ç”±
    fact_match = re.search(r"ä¸€[ã€ï¼.\s]*äº‹æ•…ç™¼ç”Ÿç·£ç”±[:ï¼š]?\s*(.*?)(?=äºŒ[ã€ï¼.]|$)", text, re.S)
    if fact_match:
        result["accident_facts"] = fact_match.group(1).strip()
    
    # å—å‚·æƒ…å½¢
    injury_match = re.search(r"äºŒ[ã€ï¼.\s]*(?:åŸå‘Š)?å—å‚·æƒ…å½¢[:ï¼š]?\s*(.*?)(?=ä¸‰[ã€ï¼.]|$)", text, re.S)
    if injury_match:
        result["injuries"] = injury_match.group(1).strip()
    
    # è³ å„Ÿäº‹å¯¦æ ¹æ“š
    comp_match = re.search(r"ä¸‰[ã€ï¼.\s]*è«‹æ±‚è³ å„Ÿçš„äº‹å¯¦æ ¹æ“š[:ï¼š]?\s*(.*?)$", text, re.S)
    if comp_match:
        result["compensation_facts"] = comp_match.group(1).strip()
    
    return result

def extract_parties_with_llm(text: str) -> dict:
    """ä½¿ç”¨LLMæå–ç•¶äº‹äººï¼ˆæ›´æº–ç¢ºçš„æ–¹æ³•ï¼‰"""
    print("ğŸ¤– ä½¿ç”¨LLMæ™ºèƒ½æå–ç•¶äº‹äºº...")
    
    # å‰µå»ºæç¤ºæ¨¡æ¿
    prompt = f"""è«‹ä½ å¹«æˆ‘å¾ä»¥ä¸‹è»Šç¦æ¡ˆä»¶çš„äº‹æ•…è©³æƒ…ä¸­æå–ä¸¦åˆ—å‡ºæ‰€æœ‰åŸå‘Šå’Œè¢«å‘Šçš„å§“åï¼Œä¸¦åªèƒ½ç”¨ä»¥ä¸‹æ ¼å¼è¼¸å‡º:
åŸå‘Š:åŸå‘Š1,åŸå‘Š2...
è¢«å‘Š:è¢«å‘Š1,è¢«å‘Š2...

ä»¥ä¸‹æ˜¯æœ¬èµ·è»Šç¦çš„äº‹æ•…è©³æƒ…ï¼š
{text}

é‡è¦è¦æ±‚:
1. è«‹å®Œæ•´æå–å§“åï¼Œä¸è¦æˆªæ–·æˆ–çœç•¥ä»»ä½•å­—å…ƒï¼ˆä¾‹å¦‚ï¼šç¾…é–å´´ä¸èƒ½å¯«æˆç¾…å´´ï¼‰
2. å¦‚æœæœªæåŠåŸå‘Šæˆ–è¢«å‘Šçš„å§“åæˆ–ä»£ç¨±éœ€å¯«ç‚º"æœªæåŠ"
3. ä½ åªéœ€è¦åˆ—å‡ºåŸå‘Šå’Œè¢«å‘Šçš„å§“åï¼Œè«‹ä¸è¦è¼¸å‡ºå…¶ä»–å¤šé¤˜çš„å…§å®¹
4. å§“åä¹‹é–“ç”¨é€—è™Ÿåˆ†éš”
5. å¦‚æœæœ‰ç”²ã€ä¹™ã€ä¸™ç­‰ä»£ç¨±ä¹Ÿç®—ä½œå§“å
6. è«‹ä»”ç´°æª¢æŸ¥æ¯å€‹å§“åæ˜¯å¦å®Œæ•´ï¼Œç‰¹åˆ¥æ³¨æ„ä¸‰å€‹å­—çš„å§“å

ç¯„ä¾‹ï¼š
- æ­£ç¢ºï¼šç¾…é–å´´,é‚±å“å¦
- éŒ¯èª¤ï¼šç¾…å´´,é‚±å“å¦"""

    try:
        # èª¿ç”¨LLM
        response = requests.post(
            LLM_URL,
            json={
                "model": DEFAULT_MODEL,
                "prompt": prompt,
                "stream": False
            },
            timeout=60
        )
        
        if response.status_code == 200:
            llm_result = response.json()["response"].strip()
            print(f"ğŸ¤– LLMæå–çµæœ: {llm_result}")
            return parse_llm_parties_result(llm_result)
        else:
            print(f"âŒ LLMèª¿ç”¨å¤±æ•—: {response.status_code}")
            return extract_parties_fallback(text)
            
    except Exception as e:
        print(f"âŒ LLMæå–ç•°å¸¸: {e}")
        return extract_parties_fallback(text)

def parse_llm_parties_result(llm_result: str) -> dict:
    """è§£æLLMçš„ç•¶äº‹äººæå–çµæœ"""
    result = {"åŸå‘Š": "æœªæåŠ", "è¢«å‘Š": "æœªæåŠ", "è¢«å‘Šæ•¸é‡": 1, "åŸå‘Šæ•¸é‡": 1}
    
    lines = llm_result.split('\n')
    
    for line in lines:
        line = line.strip()
        if line.startswith('åŸå‘Š:') or line.startswith('åŸå‘Šï¼š'):
            plaintiff_text = line.split(':', 1)[1].strip() if ':' in line else line.split('ï¼š', 1)[1].strip()
            if plaintiff_text and plaintiff_text != "æœªæåŠ":
                # åˆ†å‰²å¤šå€‹åŸå‘Š
                plaintiffs = [p.strip() for p in plaintiff_text.split(',') if p.strip()]
                result["åŸå‘Š"] = "ã€".join(plaintiffs)
                result["åŸå‘Šæ•¸é‡"] = len(plaintiffs)
        
        elif line.startswith('è¢«å‘Š:') or line.startswith('è¢«å‘Šï¼š'):
            defendant_text = line.split(':', 1)[1].strip() if ':' in line else line.split('ï¼š', 1)[1].strip()
            if defendant_text and defendant_text != "æœªæåŠ":
                # åˆ†å‰²å¤šå€‹è¢«å‘Š
                defendants = [d.strip() for d in defendant_text.split(',') if d.strip()]
                result["è¢«å‘Š"] = "ã€".join(defendants)
                result["è¢«å‘Šæ•¸é‡"] = len(defendants)
    
    return result

def extract_parties_fallback(text: str) -> dict:
    """ç•¶LLMæå–å¤±æ•—æ™‚çš„fallbackæ–¹æ³•ï¼ˆç°¡åŒ–ç‰ˆæ­£å‰‡ï¼‰"""
    print("âš ï¸ ä½¿ç”¨fallbackæ–¹æ³•æå–ç•¶äº‹äºº...")
    result = {"åŸå‘Š": "æœªæåŠ", "è¢«å‘Š": "æœªæåŠ", "è¢«å‘Šæ•¸é‡": 1, "åŸå‘Šæ•¸é‡": 1}
    
    # ç°¡åŒ–çš„æ­£å‰‡è¡¨é”å¼æå–
    plaintiffs = set()
    defendants = set()
    
    # åŸºæœ¬æ¨¡å¼
    plaintiff_patterns = [
        r'åŸå‘Š([\u4e00-\u9fff]{2,4})',
        r'åŸå‘Š([ç”²ä¹™ä¸™ä¸æˆŠ])'
    ]
    
    defendant_patterns = [
        r'è¢«å‘Š([\u4e00-\u9fff]{2,4})',
        r'è¢«å‘Š([ç”²ä¹™ä¸™ä¸æˆŠ])'
    ]
    
    for pattern in plaintiff_patterns:
        matches = re.findall(pattern, text)
        plaintiffs.update(matches)
    
    for pattern in defendant_patterns:
        matches = re.findall(pattern, text)
        defendants.update(matches)
    
    # æ¸…ç†å’Œçµ„åˆçµæœ
    if plaintiffs:
        result["åŸå‘Š"] = "ã€".join(sorted(plaintiffs))
        result["åŸå‘Šæ•¸é‡"] = len(plaintiffs)
    elif "åŸå‘Š" in text:
        result["åŸå‘Š"] = "åŸå‘Š"
    
    if defendants:
        result["è¢«å‘Š"] = "ã€".join(sorted(defendants))
        result["è¢«å‘Šæ•¸é‡"] = len(defendants)
    elif "è¢«å‘Š" in text:
        result["è¢«å‘Š"] = "è¢«å‘Š"
    
    return result

def extract_parties(text: str) -> dict:
    """ä¸»è¦çš„ç•¶äº‹äººæå–å‡½æ•¸ï¼ˆå„ªå…ˆä½¿ç”¨LLMï¼‰"""
    return extract_parties_with_llm(text)

# ===== æª¢ç´¢ç›¸é—œå‡½æ•¸ =====
def embed(text: str):
    """æ–‡å­—å‘é‡åŒ–"""
    if not FULL_MODE:
        return []
    
    t = TOKENIZER(text, truncation=True, padding="max_length", max_length=512, return_tensors="pt")
    t = {k: v.to(device) for k, v in t.items()}
    with torch.no_grad():
        vec = MODEL(**t).last_hidden_state.mean(dim=1).squeeze()
    return vec.cpu().numpy().tolist()

def es_search(query_vector, case_type: str, top_k: int = 3, label: str = "Facts"):
    """ES æœå°‹ï¼ˆå«fallbackæ©Ÿåˆ¶ï¼‰"""
    if not FULL_MODE or not ES_HOST:
        return []
    
    def _search(label_filter, case_type_filter):
        must_clause = [{"match": {"label": label_filter}}]
        
        if case_type_filter:
            # å˜—è©¦å¤šç¨®å¯èƒ½çš„ case_type æ¬„ä½æ ¼å¼
            case_type_options = [
                {"term": {"case_type.keyword": case_type_filter}},
                {"term": {"case_type": case_type_filter}},
                {"match": {"case_type": case_type_filter}}
            ]
            
            # ä½¿ç”¨ should æŸ¥è©¢ï¼Œä»»ä¸€ç¬¦åˆå³å¯
            must_clause.append({
                "bool": {
                    "should": case_type_options,
                    "minimum_should_match": 1
                }
            })
            
        body = {
            "size": top_k,
            "query": {
                "script_score": {
                    "query": {"bool": {"must": must_clause}},
                    "script": {
                        "source": "cosineSimilarity(params.qv,'embedding')+1.0",
                        "params": {"qv": query_vector},
                    },
                }
            },
        }
        
        # èª¿è©¦ä¿¡æ¯ï¼šè¼¸å‡ºæŸ¥è©¢æ¢ä»¶
        print(f"ğŸ” ESæŸ¥è©¢æ¢ä»¶: index={CHUNK_INDEX}, label={label_filter}, case_type={case_type_filter}")
        
        try:
            url = f"{ES_HOST}/{CHUNK_INDEX}/_search"
            response = requests.post(url, auth=ES_AUTH, json=body, verify=False)
            if response.status_code == 200:
                result = response.json()
                hits = result["hits"]["hits"]
                total_docs = result["hits"]["total"]["value"] if isinstance(result["hits"]["total"], dict) else result["hits"]["total"]
                print(f"ğŸ“Š ESæŸ¥è©¢çµæœ: æ‰¾åˆ° {len(hits)} å€‹åŒ¹é…çµæœï¼Œç¸½æ–‡æª”æ•¸: {total_docs}")
                return hits
            else:
                print(f"âŒ ESæŸ¥è©¢å¤±æ•—: {response.status_code} - {response.text}")
                return []
        except Exception as e:
            print(f"âŒ ESæŸ¥è©¢å¤±æ•—: {e}")
            return []

    print(f"ğŸ” ä½¿ç”¨ case_type='{case_type}' æœç´¢ç›¸ä¼¼æ¡ˆä¾‹...")
    hits = _search(label, case_type)
    
    if not hits:
        # å…ˆæª¢æŸ¥ç´¢å¼•æ˜ å°„å’Œå¯ç”¨çš„æ¡ˆä»¶é¡å‹
        try:
            # æª¢æŸ¥ mapping
            mapping_url = f"{ES_HOST}/{CHUNK_INDEX}/_mapping"
            mapping_response = requests.get(mapping_url, auth=ES_AUTH, verify=False)
            if mapping_response.status_code == 200:
                mapping = mapping_response.json()
                properties = mapping[CHUNK_INDEX]["mappings"]["properties"]
                has_case_type = "case_type" in properties
                print(f"ğŸ—ºï¸ case_typeæ¬„ä½å­˜åœ¨: {has_case_type}")
                
                if has_case_type:
                    # å˜—è©¦ä¸åŒçš„æ¬„ä½åç¨±
                    for field_name in ["case_type", "case_type.keyword"]:
                        try:
                            check_body = {
                                "size": 0,
                                "aggs": {
                                    "case_type_count": {"terms": {"field": field_name, "size": 20}}
                                }
                            }
                            check_url = f"{ES_HOST}/{CHUNK_INDEX}/_search"
                            check_response = requests.post(check_url, auth=ES_AUTH, json=check_body, verify=False)
                            if check_response.status_code == 200:
                                check_result = check_response.json()
                                available_types = [bucket["key"] for bucket in check_result["aggregations"]["case_type_count"]["buckets"]]
                                print(f"ğŸ“‹ ä½¿ç”¨æ¬„ä½ {field_name} æ‰¾åˆ°çš„æ¡ˆä»¶é¡å‹: {available_types}")
                                break
                        except Exception as field_e:
                            print(f"âš ï¸ æ¬„ä½ {field_name} æŸ¥è©¢å¤±æ•—: {field_e}")
                else:
                    print("âŒ case_typeæ¬„ä½ä¸å­˜åœ¨æ–¼ç´¢å¼•æ˜ å°„ä¸­")
            else:
                print(f"âŒ ç²å–æ˜ å°„å¤±æ•—: {mapping_response.status_code}")
                
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•æª¢æŸ¥ç´¢å¼•æ˜ å°„æˆ–æ¡ˆä»¶é¡å‹: {e}")
        
        fallback = CASE_TYPE_MAP.get(case_type, "å–®ç´”åŸè¢«å‘Šå„ä¸€")
        if fallback != case_type:
            print(f"âš ï¸ ä½¿ç”¨ fallback='{fallback}' é‡æ–°æœå°‹...")
            hits = _search(label, fallback)
    
    if not hits:
        print("âš ï¸ ä¸é™æ¡ˆä»¶é¡å‹é€²è¡Œæœå°‹...")
        hits = _search(label, None)
    
    return hits

def rerank_case_ids_by_paragraphs(query_text: str, case_ids: List[str], label: str = "Facts") -> List[str]:
    """æ ¹æ“šæ®µè½ç´šè³‡æ–™é‡æ–°æ’åºæ¡ˆä¾‹"""
    if not FULL_MODE or not ES_HOST:
        return case_ids
    
    print("ğŸ“˜ å•Ÿå‹•æ®µè½ç´š rerank...")
    try:
        from sklearn.metrics.pairwise import cosine_similarity
        import numpy as np
    except ImportError:
        print("âš ï¸ sklearnæœªå®‰è£ï¼Œè·³érerank")
        return case_ids

    query_vec = embed(query_text)
    if not query_vec:
        return case_ids
    
    query_vec_np = np.array(query_vec).reshape(1, -1)
    scored_cases = []
    
    for cid in case_ids:
        try:
            # ä½¿ç”¨ requests ç›´æ¥èª¿ç”¨ ES API
            search_body = {
                "query": {
                    "bool": {
                        "must": [
                            {"term": {"case_id": cid}},
                            {"term": {"label": label}}
                        ]
                    }
                },
                "size": 1
            }
            url = f"{ES_HOST}/legal_kg_paragraphs/_search"
            response = requests.post(url, auth=ES_AUTH, json=search_body, verify=False)
            if response.status_code != 200:
                continue
            res = response.json()
            hits = res.get("hits", {}).get("hits", [])
            if hits:
                vec = hits[0]['_source']['embedding']
                para_vec_np = np.array(vec).reshape(1, -1)
                score = cosine_similarity(query_vec_np, para_vec_np)[0][0]
                scored_cases.append((cid, score))
        except Exception as e:
            print(f"âš ï¸ Case {cid} rerankå¤±æ•—: {e}")
            scored_cases.append((cid, 0.0))

    scored_cases.sort(key=lambda x: x[1], reverse=True)
    return [cid for cid, _ in scored_cases]

def get_complete_cases_content(case_ids: List[str]) -> List[str]:
    """ç²å–å®Œæ•´æ¡ˆä¾‹å…§å®¹"""
    if not FULL_MODE or not NEO4J_DRIVER:
        return []
    
    complete_cases = []
    
    try:
        with NEO4J_DRIVER.session() as session:
            for case_id in case_ids:
                # æŸ¥è©¢å®Œæ•´æ¡ˆä¾‹çš„äº‹å¯¦æ®µè½
                result = session.run("""
                    MATCH (c:Case {case_id: $case_id})-[:åŒ…å«]->(f:Facts)
                    RETURN f.description AS facts_content
                """, case_id=case_id).data()
                
                # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå˜—è©¦ä½¿ç”¨idå±¬æ€§
                if not result:
                    result = session.run("""
                        MATCH (c:Case {id: $case_id})-[:åŒ…å«]->(f:Facts)
                        RETURN f.description AS facts_content
                    """, case_id=case_id).data()
                
                if result:
                    # çµ„åˆæ¡ˆä¾‹çš„æ‰€æœ‰äº‹å¯¦æ®µè½
                    case_content = "\n".join([record["facts_content"] for record in result if record["facts_content"]])
                    if case_content:
                        complete_cases.append(case_content)
                else:
                    print(f"âš ï¸ æ¡ˆä¾‹ {case_id} ç„¡æ³•ç²å–å®Œæ•´å…§å®¹")
                    
    except Exception as e:
        print(f"âŒ Neo4jæŸ¥è©¢å¤±æ•—: {e}")
        
    return complete_cases

def query_laws(case_ids):
    """æŸ¥è©¢ç›¸é—œæ³•æ¢"""
    if not FULL_MODE or not NEO4J_DRIVER:
        return []
    
    try:
        with NEO4J_DRIVER.session() as session:
            # æŸ¥è©¢èˆ‡æ¡ˆä¾‹ç›¸é—œçš„æ³•æ¢
            result = session.run("""
                MATCH (c:Case)-[:é©ç”¨]->(l:Laws)
                WHERE c.case_id IN $case_ids OR c.id IN $case_ids
                RETURN DISTINCT l.article AS article, l.description AS description
                ORDER BY l.article
            """, case_ids=case_ids).data()
            
            return result
    except Exception as e:
        print(f"âŒ æ³•æ¢æŸ¥è©¢å¤±æ•—: {e}")
        return []

def normalize_article_number(article: str) -> str:
    """æ¨™æº–åŒ–æ³•æ¢ç·¨è™Ÿ"""
    return re.sub(r'[^0-9]', '', str(article))

def detect_special_relationships(text: str, parties: dict) -> dict:
    """æª¢æ¸¬ç‰¹æ®Šé—œä¿‚ï¼ˆåƒ±å‚­ã€ç›£è­·ç­‰ï¼‰"""
    relationships = {}
    
    # åƒ±å‚­é—œä¿‚æª¢æ¸¬
    employment_patterns = [
        r"åƒ±ç”¨äºº.*è²¬ä»»", r"å—åƒ±äºº", r"åƒ±ä¸»", r"å“¡å·¥", r"åŸ·è¡Œè·å‹™",
        r"æ°‘æ³•.*ç¬¬.*ä¸€ç™¾å…«åå…«.*æ¢", r"Â§\s*188", r"ç¬¬188æ¢"
    ]
    
    for pattern in employment_patterns:
        if re.search(pattern, text, re.IGNORECASE):
            relationships["åƒ±å‚­é—œä¿‚"] = True
            break
    else:
        relationships["åƒ±å‚­é—œä¿‚"] = False
    
    # ç›£è­·é—œä¿‚æª¢æ¸¬ï¼ˆæœªæˆå¹´ï¼‰
    guardianship_patterns = [
        r"æœªæˆå¹´", r"ç›£è­·äºº", r"æ³•å®šä»£ç†äºº", r"æœªæ»¿.*æ­²",
        r"æ°‘æ³•.*ç¬¬.*ä¸€ç™¾å…«åä¸ƒ.*æ¢", r"Â§\s*187", r"ç¬¬187æ¢"
    ]
    
    for pattern in guardianship_patterns:
        if re.search(pattern, text, re.IGNORECASE):
            relationships["ç›£è­·é—œä¿‚"] = True
            break
    else:
        relationships["ç›£è­·é—œä¿‚"] = False
    
    # å‹•ç‰©ç®¡ç†äººè²¬ä»»æª¢æ¸¬
    animal_patterns = [
        r"å‹•ç‰©.*è²¬ä»»", r"é£¼ä¸»", r"ç®¡ç†äºº.*å‹•ç‰©", r"å‹•ç‰©.*ç®¡ç†",
        r"æ°‘æ³•.*ç¬¬.*ä¸€ç™¾ä¹å.*æ¢", r"Â§\s*190", r"ç¬¬190æ¢"
    ]
    
    for pattern in animal_patterns:
        if re.search(pattern, text, re.IGNORECASE):
            relationships["å‹•ç‰©ç®¡ç†"] = True
            break
    else:
        relationships["å‹•ç‰©ç®¡ç†"] = False
    
    return relationships

def determine_case_type(accident_facts: str, parties: dict) -> str:
    """æ ¹æ“šäº‹å¯¦å’Œç•¶äº‹äººåˆ¤æ–·æ¡ˆä»¶é¡å‹"""
    # ç²å–ç•¶äº‹äººæ•¸é‡
    plaintiff_count = parties.get("åŸå‘Šæ•¸é‡", 1)
    defendant_count = parties.get("è¢«å‘Šæ•¸é‡", 1)
    
    # æª¢æ¸¬ç‰¹æ®Šé—œä¿‚
    relationships = detect_special_relationships(accident_facts, parties)
    
    # åˆ¤æ–·åŸºæœ¬æ¡ˆå‹
    base_type = ""
    if plaintiff_count == 1 and defendant_count == 1:
        base_type = "å–®ç´”åŸè¢«å‘Šå„ä¸€"
    elif plaintiff_count > 1 and defendant_count == 1:
        base_type = "æ•¸ååŸå‘Š"
    elif plaintiff_count == 1 and defendant_count > 1:
        base_type = "æ•¸åè¢«å‘Š"
    elif plaintiff_count > 1 and defendant_count > 1:
        base_type = "åŸè¢«å‘Šçš†æ•¸å"
    else:
        base_type = "å–®ç´”åŸè¢«å‘Šå„ä¸€"  # é è¨­å€¼
    
    # æª¢æŸ¥ç‰¹æ®Šæ³•æ¢é©ç”¨
    if relationships.get("ç›£è­·é—œä¿‚", False):
        return "Â§187æœªæˆå¹´æ¡ˆå‹"
    elif relationships.get("åƒ±å‚­é—œä¿‚", False):
        return "Â§188åƒ±ç”¨äººæ¡ˆå‹"
    elif relationships.get("å‹•ç‰©ç®¡ç†", False):
        return "Â§190å‹•ç‰©æ¡ˆå‹"
    else:
        return base_type

def determine_applicable_laws(accident_facts: str, injuries: str, comp_facts: str, parties: dict) -> List[str]:
    """æ ¹æ“šæ¡ˆä»¶äº‹å¯¦åˆ¤æ–·é©ç”¨æ³•æ¢"""
    laws = []
    combined_text = f"{accident_facts} {injuries} {comp_facts}"
    
    # æª¢æ¸¬ç‰¹æ®Šé—œä¿‚
    relationships = detect_special_relationships(combined_text, parties)
    
    # åŸºæœ¬ä¾µæ¬Šè¡Œç‚º - ç¬¬184æ¢
    laws.append("184")
    
    # ç‰¹æ®Šè²¬ä»»æ¢æ¬¾
    if relationships.get("ç›£è­·é—œä¿‚", False):
        laws.append("187")  # æœªæˆå¹´äººè²¬ä»»
    
    if relationships.get("åƒ±å‚­é—œä¿‚", False):
        laws.append("188")  # åƒ±ç”¨äººè²¬ä»»
    
    if relationships.get("å‹•ç‰©ç®¡ç†", False):
        laws.append("190")  # å‹•ç‰©ç®¡ç†äººè²¬ä»»
    
    # æ±½è»Šäº¤é€šç›¸é—œ
    vehicle_patterns = [
        r"æ±½è»Š", r"æ©Ÿè»Š", r"æ‘©æ‰˜è»Š", r"è»Šè¼›", r"é§•é§›", r"äº¤é€šäº‹æ•…", r"è»Šç¦"
    ]
    if any(re.search(pattern, combined_text, re.IGNORECASE) for pattern in vehicle_patterns):
        laws.append("191-2")  # æ±½è»Šäº¤é€šäº‹æ•…è²¬ä»»
    
    # å…±åŒä¾µæ¬Š
    if parties.get("è¢«å‘Šæ•¸é‡", 1) > 1:
        laws.append("185")  # å…±åŒä¾µæ¬Šè¡Œç‚º
    
    # æå®³è³ å„Ÿç¯„åœç›¸é—œæ¢æ–‡
    damage_patterns = [
        r"é†«ç™‚è²»", r"çœ‹è­·è²»", r"ç²¾ç¥æ…°æ’«é‡‘", r"å·¥ä½œèƒ½åŠ›", r"æ”¶å…¥", r"ç‡Ÿæ¥­æå¤±"
    ]
    if any(re.search(pattern, combined_text, re.IGNORECASE) for pattern in damage_patterns):
        laws.extend(["193", "194", "195", "196"])  # æå®³è³ å„Ÿç›¸é—œæ¢æ–‡
    
    return list(set(laws))  # å»é‡

def call_llm(prompt: str, model: str = DEFAULT_MODEL) -> str:
    """èª¿ç”¨LLMç”Ÿæˆå›æ‡‰"""
    try:
        response = requests.post(
            LLM_URL,
            json={
                "model": model,
                "prompt": prompt,
                "stream": False
            },
            timeout=120
        )
        
        if response.status_code == 200:
            return response.json()["response"].strip()
        else:
            return f"âŒ LLMèª¿ç”¨å¤±æ•—: {response.status_code}"
            
    except Exception as e:
        return f"âŒ LLMèª¿ç”¨ç•°å¸¸: {e}"

def generate_lawsuit_content(facts: str, laws: str, compensation: str) -> str:
    """ç”Ÿæˆå®Œæ•´çš„èµ·è¨´ç‹€å…§å®¹"""
    prompt = f"""ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ³•å¾‹æ–‡æ›¸æ’°å¯«åŠ©ç†ã€‚è«‹ä½ æ ¹æ“šä»¥ä¸‹æä¾›çš„è³‡æ–™ï¼Œæ’°å¯«ä¸€ä»½å®Œæ•´çš„æ°‘äº‹èµ·è¨´ç‹€ã€‚

## æä¾›è³‡æ–™ï¼š

### äº‹å¯¦éƒ¨åˆ†ï¼š
{facts}

### é©ç”¨æ³•æ¢ï¼š
{laws}

### æå®³è³ å„Ÿï¼š
{compensation}

## æ’°å¯«è¦æ±‚ï¼š
1. æ ¼å¼è¦å®Œæ•´æ­£å¼ï¼ŒåŒ…å«æ¨™é¡Œã€ç•¶äº‹äººã€è«‹æ±‚äº‹é …ã€äº‹å¯¦åŠç†ç”±ã€è­‰æ“šè³‡æ–™ç­‰
2. äº‹å¯¦é™³è¿°è¦æ¸…æ¥šæœ‰æ¢ç†ï¼ŒæŒ‰æ™‚é–“é †åºçµ„ç¹”
3. æ³•å¾‹é©ç”¨è¦æº–ç¢ºï¼Œå¼•ç”¨æ¢æ–‡è¦å®Œæ•´
4. æå®³è³ å„Ÿè¨ˆç®—è¦è©³ç´°åˆ—å‡ºå„é …ç›®åŠé‡‘é¡
5. ç”¨å­—è¦æ­£å¼ã€å°ˆæ¥­ã€ç¬¦åˆæ³•å¾‹æ–‡æ›¸è¦ç¯„
6. è«‹æ±‚äº‹é …è¦æ˜ç¢ºå…·é«”

è«‹é–‹å§‹æ’°å¯«ï¼š"""
    
    return call_llm(prompt)

def interactive_generate_lawsuit():
    """äº’å‹•å¼ç”Ÿæˆèµ·è¨´ç‹€"""
    print("\n" + "="*60)
    print("ğŸ›ï¸  æ³•å¾‹æ–‡æ›¸ç”Ÿæˆç³»çµ± - æ··åˆæ¨¡å¼")
    print("="*60)
    
    # è¼¸å…¥äº‹æ•…æè¿°
    print("\nğŸ“ è«‹è¼¸å…¥è»Šç¦äº‹æ•…çš„è©³ç´°æè¿°ï¼š")
    print("ï¼ˆåŒ…å«äº‹æ•…ç™¼ç”Ÿç·£ç”±ã€å—å‚·æƒ…å½¢ã€è³ å„Ÿäº‹å¯¦æ ¹æ“šï¼‰")
    user_input = input("> ")
    
    if not user_input.strip():
        print("âŒ è«‹è¼¸å…¥æœ‰æ•ˆçš„äº‹æ•…æè¿°")
        return
    
    print("\nğŸ” é–‹å§‹åˆ†ææ¡ˆä»¶...")
    
    # 1. æå–æ®µè½
    sections = extract_sections(user_input)
    print(f"âœ… æ®µè½æå–å®Œæˆ")
    
    # 2. æå–ç•¶äº‹äºº
    parties = extract_parties(user_input)
    print(f"âœ… ç•¶äº‹äººæå–å®Œæˆ: åŸå‘Š{parties['åŸå‘Šæ•¸é‡']}å, è¢«å‘Š{parties['è¢«å‘Šæ•¸é‡']}å")
    
    # 3. åˆ¤æ–·æ¡ˆä»¶é¡å‹
    case_type = determine_case_type(sections["accident_facts"], parties)
    print(f"âœ… æ¡ˆä»¶åˆ†é¡: {case_type}")
    
    # 4. ç”Ÿæˆå„éƒ¨åˆ†å…§å®¹
    if FULL_MODE:
        print("\nğŸ” æœå°‹ç›¸ä¼¼æ¡ˆä¾‹...")
        
        # äº‹å¯¦éƒ¨åˆ†
        print("ğŸ“‹ ç”Ÿæˆäº‹å¯¦é™³è¿°...")
        facts_query = sections["accident_facts"][:200]
        facts_vector = embed(facts_query)
        facts_hits = es_search(facts_vector, case_type, top_k=3, label="Facts")
        
        if facts_hits:
            case_ids = [hit["_source"]["case_id"] for hit in facts_hits]
            reranked_case_ids = rerank_case_ids_by_paragraphs(facts_query, case_ids, "Facts")
            similar_cases = get_complete_cases_content(reranked_case_ids[:2])
            
            facts_context = "\n\n".join(similar_cases[:2]) if similar_cases else ""
            facts_prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹äº‹æ•…æè¿°ç”Ÿæˆæ­£å¼çš„èµ·è¨´ç‹€äº‹å¯¦é™³è¿°éƒ¨åˆ†ï¼š

## äº‹æ•…æè¿°ï¼š
{sections["accident_facts"]}

## åƒè€ƒæ¡ˆä¾‹ï¼š
{facts_context}

## è¦æ±‚ï¼š
1. ä½¿ç”¨æ­£å¼çš„æ³•å¾‹æ–‡æ›¸ç”¨èª
2. æŒ‰æ™‚é–“é †åºçµ„ç¹”äº‹å¯¦
3. é‡é»çªå‡ºéå¤±è¡Œç‚ºå’Œå› æœé—œä¿‚
4. æ ¼å¼è¦æ•´é½Šæœ‰æ¢ç†"""
            facts_result = call_llm(facts_prompt)
        else:
            facts_result = f"äº‹æ•…ç™¼ç”Ÿç·£ç”±ï¼š{sections['accident_facts']}"
        
        # æ³•æ¢éƒ¨åˆ†
        print("âš–ï¸ ç”Ÿæˆæ³•æ¢é©ç”¨...")
        applicable_laws = determine_applicable_laws(
            sections["accident_facts"], 
            sections["injuries"], 
            sections["compensation_facts"], 
            parties
        )
        
        if applicable_laws:
            laws_vector = embed(f"æ°‘æ³•ç¬¬{applicable_laws[0]}æ¢")
            laws_hits = es_search(laws_vector, case_type, top_k=2, label="Laws")
            
            if laws_hits:
                laws_content = "\n".join([hit["_source"]["content"] for hit in laws_hits])
                laws_prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹æ³•æ¢å…§å®¹ç”Ÿæˆèµ·è¨´ç‹€çš„æ³•æ¢é©ç”¨éƒ¨åˆ†ï¼š

## é©ç”¨æ³•æ¢ï¼š
{laws_content}

## æ¡ˆä»¶äº‹å¯¦ï¼š
{sections["accident_facts"]}

## è¦æ±‚ï¼š
1. æ˜ç¢ºå¼•ç”¨æ³•æ¢æ¢æ–‡
2. èªªæ˜æ³•æ¢èˆ‡æœ¬æ¡ˆäº‹å¯¦çš„å°æ‡‰é—œä¿‚
3. è«–è­‰è¢«å‘Šè²¬ä»»æˆç«‹çš„æ³•å¾‹ä¾æ“š"""
                laws_result = call_llm(laws_prompt)
            else:
                laws_result = f"é©ç”¨æ³•æ¢ï¼šæ°‘æ³•ç¬¬{"ã€ç¬¬".join(applicable_laws)}æ¢"
        else:
            laws_result = "é©ç”¨æ³•æ¢ï¼šæ°‘æ³•ç¬¬184æ¢ç¬¬1é …å‰æ®µ"
        
        # æå®³è³ å„Ÿéƒ¨åˆ†
        print("ğŸ’° ç”Ÿæˆæå®³è³ å„Ÿ...")
        comp_query = sections["compensation_facts"][:200]
        comp_vector = embed(comp_query)
        comp_hits = es_search(comp_vector, case_type, top_k=3, label="Compensation")
        
        if comp_hits:
            comp_case_ids = [hit["_source"]["case_id"] for hit in comp_hits]
            comp_reranked = rerank_case_ids_by_paragraphs(comp_query, comp_case_ids, "Compensation")
            comp_cases = get_complete_cases_content(comp_reranked[:2])
            
            comp_context = "\n\n".join(comp_cases[:2]) if comp_cases else ""
            comp_prompt = f"""è«‹æ ¹æ“šä»¥ä¸‹æå®³äº‹å¯¦ç”Ÿæˆè©³ç´°çš„æå®³è³ å„Ÿè¨ˆç®—ï¼š

## æå®³äº‹å¯¦ï¼š
{sections["compensation_facts"]}

## åƒè€ƒæ¡ˆä¾‹ï¼š
{comp_context}

## è¦æ±‚ï¼š
1. è©³ç´°åˆ—å‡ºå„é …æå®³é …ç›®
2. è¨ˆç®—æ–¹å¼è¦æ¸…æ¥šå…·é«”
3. åŒ…å«é†«ç™‚è²»ã€å·¥ä½œæå¤±ã€ç²¾ç¥æ…°æ’«é‡‘ç­‰
4. æä¾›ç¸½è¨ˆé‡‘é¡"""
            comp_result = call_llm(comp_prompt)
        else:
            comp_result = f"æå®³è³ å„Ÿï¼š{sections['compensation_facts']}"
    
    else:
        # ç°¡åŒ–æ¨¡å¼ï¼šç›´æ¥ä½¿ç”¨è¼¸å…¥å…§å®¹
        facts_result = sections["accident_facts"]
        laws_result = "é©ç”¨æ³•æ¢ï¼šæ°‘æ³•ç¬¬184æ¢ç¬¬1é …å‰æ®µ"
        comp_result = sections["compensation_facts"]
    
    # ç”Ÿæˆæœ€çµ‚èµ·è¨´ç‹€
    print("\nğŸ“‹ ç”Ÿæˆå®Œæ•´èµ·è¨´ç‹€...")
    final_lawsuit = generate_lawsuit_content(facts_result, laws_result, comp_result)
    
    # è¼¸å‡ºçµæœ
    print("\n" + "="*60)
    print("ğŸ›ï¸ ç”Ÿæˆçš„èµ·è¨´ç‹€å…§å®¹")
    print("="*60)
    print(final_lawsuit)
    
    # å„²å­˜çµæœ
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    filename = f"lawsuit_{timestamp}.txt"
    try:
        with open(filename, 'w', encoding='utf-8') as f:
            f.write(final_lawsuit)
        print(f"\nâœ… èµ·è¨´ç‹€å·²å„²å­˜ç‚ºï¼š{filename}")
    except Exception as e:
        print(f"\nâš ï¸ æª”æ¡ˆå„²å­˜å¤±æ•—ï¼š{e}")

def main():
    """ä¸»ç¨‹å¼"""
    print("\nğŸ›ï¸ æ³•å¾‹æ–‡æ›¸ç”Ÿæˆç³»çµ± (èˆŠæ··åˆç‰ˆæœ¬)")
    print(f"ğŸ“Š æ¨¡å¼: {'å®Œæ•´æ¨¡å¼' if FULL_MODE else 'ç°¡åŒ–æ¨¡å¼'}")
    
    while True:
        print("\n" + "="*40)
        print("è«‹é¸æ“‡åŠŸèƒ½ï¼š")
        print("1. ç”Ÿæˆèµ·è¨´ç‹€")
        print("2. æª¢æŸ¥ç³»çµ±ç‹€æ…‹")
        print("3. é€€å‡º")
        
        choice = input("\nè«‹è¼¸å…¥é¸é … (1-3): ").strip()
        
        if choice == "1":
            interactive_generate_lawsuit()
        elif choice == "2":
            print(f"\nğŸ“Š ç³»çµ±ç‹€æ…‹ï¼š")
            print(f"æ¨¡å¼: {'å®Œæ•´æ¨¡å¼' if FULL_MODE else 'ç°¡åŒ–æ¨¡å¼'}")
            print(f"ESé€£æ¥: {'âœ…' if FULL_MODE and ES_HOST else 'âŒ'}")
            print(f"Neo4jé€£æ¥: {'âœ…' if FULL_MODE and NEO4J_DRIVER else 'âŒ'}")
        elif choice == "3":
            print("\nğŸ‘‹ å†è¦‹ï¼")
            break
        else:
            print("\nâŒ ç„¡æ•ˆé¸é …ï¼Œè«‹é‡æ–°é¸æ“‡")

if __name__ == "__main__":
    main()