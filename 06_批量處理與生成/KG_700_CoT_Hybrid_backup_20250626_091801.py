#!/usr/bin/env python3
"""
KG_700_CoT_Hybrid.py
æ··åˆæ¨¡å¼ï¼šäº‹å¯¦ã€æ³•æ¢å’Œæå®³ç”¨æ¨™æº–æ–¹æ³•ï¼Œçµè«–ç”¨CoT
å„ªåŒ–ç‰ˆæœ¬ï¼šä¿®æ­£æœªæˆå¹´èª¤åˆ¤ã€æ”¹é€²æå®³é …ç›®è™•ç†ã€å¢å¼·ç©©å®šæ€§
"""

import os
import re
import json
import requests
import time
import sys
from typing import List, Dict, Any, Optional
from collections import Counter

# å°å…¥å¿…è¦æ¨¡çµ„
try:
    import torch
    from transformers import AutoTokenizer, AutoModel
    from elasticsearch import Elasticsearch
    from neo4j import GraphDatabase
    from dotenv import load_dotenv
    FULL_MODE = True
    print("âœ… å®Œæ•´æ¨¡å¼ï¼šæ‰€æœ‰æª¢ç´¢åŠŸèƒ½å¯ç”¨")
except ImportError as e:
    print(f"âš ï¸ éƒ¨åˆ†æ¨¡çµ„æœªå®‰è£ï¼š{e}")
    print("âš ï¸ ä½¿ç”¨ç°¡åŒ–æ¨¡å¼ï¼ˆåƒ…LLMç”ŸæˆåŠŸèƒ½ï¼‰")
    FULL_MODE = False

# å°å…¥çµæ§‹åŒ–é‡‘é¡è™•ç†å™¨
try:
    from structured_legal_amount_processor import StructuredLegalAmountProcessor
    STRUCTURED_PROCESSOR_AVAILABLE = True
    print("âœ… çµæ§‹åŒ–é‡‘é¡è™•ç†å™¨è¼‰å…¥æˆåŠŸ")
except ImportError:
    STRUCTURED_PROCESSOR_AVAILABLE = False
    print("âš ï¸ çµæ§‹åŒ–é‡‘é¡è™•ç†å™¨æœªæ‰¾åˆ°")

# å°å…¥åŸºæœ¬é‡‘é¡æ¨™æº–åŒ–å™¨
try:
    from legal_amount_standardizer import LegalAmountStandardizer
    BASIC_STANDARDIZER_AVAILABLE = True
    print("âœ… åŸºæœ¬é‡‘é¡æ¨™æº–åŒ–å™¨è¼‰å…¥æˆåŠŸ")
except ImportError:
    BASIC_STANDARDIZER_AVAILABLE = False
    print("âš ï¸ åŸºæœ¬é‡‘é¡æ¨™æº–åŒ–å™¨æœªæ‰¾åˆ°")

# å°å…¥é€šç”¨æ ¼å¼è™•ç†å™¨
try:
    from universal_format_handler import UniversalFormatHandler
    UNIVERSAL_FORMAT_HANDLER_AVAILABLE = True
    print("âœ… é€šç”¨æ ¼å¼è™•ç†å™¨è¼‰å…¥æˆåŠŸ")
except ImportError:
    UNIVERSAL_FORMAT_HANDLER_AVAILABLE = False
    print("âš ï¸ é€šç”¨æ ¼å¼è™•ç†å™¨æœªæ‰¾åˆ°")

# ===== åŸºæœ¬è¨­å®š =====
LLM_URL = "http://localhost:11434/api/generate"
DEFAULT_MODEL = "gemma3:27b"

# ===== æª¢ç´¢ç³»çµ±è¨­å®š =====
if FULL_MODE:
    # è¼‰å…¥ç’°å¢ƒè®Šæ•¸
    env_path = os.path.join(os.path.dirname(__file__), '..', '01_è¨­å®šèˆ‡é…ç½®', '.env')
    load_dotenv(dotenv_path=env_path)
    
    # åµŒå…¥æ¨¡å‹è¨­å®š
    BERT_MODEL = "shibing624/text2vec-base-chinese"
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    try:
        TOKENIZER = AutoTokenizer.from_pretrained(BERT_MODEL)
        MODEL = AutoModel.from_pretrained(BERT_MODEL, torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32).to(device)
        print("âœ… åµŒå…¥æ¨¡å‹è¼‰å…¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ åµŒå…¥æ¨¡å‹è¼‰å…¥å¤±æ•—: {e}")
        FULL_MODE = False
    
    # ES å’Œ Neo4j é€£æ¥
    try:
        # ä½¿ç”¨ requests ç›´æ¥èª¿ç”¨ ES API é¿å…ç‰ˆæœ¬å…¼å®¹æ€§å•é¡Œ
        ES_HOST = os.getenv("ELASTIC_HOST")
        ES_USER = os.getenv("ELASTIC_USER")
        ES_PASSWORD = os.getenv("ELASTIC_PASSWORD")
        ES_AUTH = (ES_USER, ES_PASSWORD)
        
        # æ¸¬è©¦ ES é€£æ¥
        response = requests.get(f"{ES_HOST}/_cluster/health", auth=ES_AUTH, verify=False)
        if response.status_code != 200:
            raise Exception(f"ESé€£æ¥å¤±æ•—: {response.status_code}")
        
        NEO4J_DRIVER = GraphDatabase.driver(
            os.getenv("NEO4J_URI"),
            auth=(os.getenv("NEO4J_USERNAME"), os.getenv("NEO4J_PASSWORD")),
        )
        CHUNK_INDEX = "legal_kg_chunks"
        print("âœ… è³‡æ–™åº«é€£æ¥æˆåŠŸ")
    except Exception as e:
        print(f"âŒ è³‡æ–™åº«é€£æ¥å¤±æ•—: {e}")
        FULL_MODE = False
else:
    ES_HOST = None
    ES_AUTH = None
    NEO4J_DRIVER = None
    CHUNK_INDEX = None

# æ¡ˆä»¶é¡å‹å°ç…§è¡¨ï¼ˆESæª¢ç´¢fallbackç”¨ï¼‰
CASE_TYPE_MAP = {
    # ç‰¹æ®Šæ¡ˆå‹å¦‚æœæ‰¾ä¸åˆ°ï¼Œfallbackåˆ°ç›¸é—œåŸºç¤é¡å‹
    "Â§190å‹•ç‰©æ¡ˆå‹": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    "Â§188åƒ±ç”¨äººæ¡ˆå‹": "å–®ç´”åŸè¢«å‘Šå„ä¸€", 
    "Â§187æœªæˆå¹´æ¡ˆå‹": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    
    # è¤‡åˆæ¡ˆå‹çš„fallback
    "åŸè¢«å‘Šçš†æ•¸å+Â§188åƒ±ç”¨äººæ¡ˆå‹": "Â§188åƒ±ç”¨äººæ¡ˆå‹",
    "æ•¸ååŸå‘Š+Â§188åƒ±ç”¨äººæ¡ˆå‹": "Â§188åƒ±ç”¨äººæ¡ˆå‹",
    "æ•¸åè¢«å‘Š+Â§188åƒ±ç”¨äººæ¡ˆå‹": "Â§188åƒ±ç”¨äººæ¡ˆå‹",
    "æ•¸åè¢«å‘Š+Â§187æœªæˆå¹´æ¡ˆå‹": "Â§187æœªæˆå¹´æ¡ˆå‹", 
    "åŸè¢«å‘Šçš†æ•¸å+Â§187æœªæˆå¹´æ¡ˆå‹": "Â§187æœªæˆå¹´æ¡ˆå‹",
    "åŸè¢«å‘Šçš†æ•¸å+Â§190å‹•ç‰©æ¡ˆå‹": "Â§190å‹•ç‰©æ¡ˆå‹",
    
    # åŸºç¤ç•¶äº‹äººæ•¸é‡é¡å‹ï¼ˆé€šå¸¸ä¸éœ€è¦fallbackï¼‰
    "æ•¸ååŸå‘Š": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    "æ•¸åè¢«å‘Š": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
    "åŸè¢«å‘Šçš†æ•¸å": "å–®ç´”åŸè¢«å‘Šå„ä¸€",
}

# ===== è¼”åŠ©å‡½æ•¸ =====
def extract_sections(text: str) -> dict:
    """æå–æ–‡æœ¬æ®µè½"""
    result = {
        "accident_facts": "",
        "injuries": "",
        "compensation_facts": ""
    }
    
    # äº‹æ•…ç™¼ç”Ÿç·£ç”±
    fact_match = re.search(r"ä¸€[ã€ï¼.\s]*äº‹æ•…ç™¼ç”Ÿç·£ç”±[:ï¼š]?\s*(.*?)(?=äºŒ[ã€ï¼.]|$)", text, re.S)
    if fact_match:
        result["accident_facts"] = fact_match.group(1).strip()
    
    # å—å‚·æƒ…å½¢
    injury_match = re.search(r"äºŒ[ã€ï¼.\s]*(?:åŸå‘Š)?å—å‚·æƒ…å½¢[:ï¼š]?\s*(.*?)(?=ä¸‰[ã€ï¼.]|$)", text, re.S)
    if injury_match:
        result["injuries"] = injury_match.group(1).strip()
    
    # è³ å„Ÿäº‹å¯¦æ ¹æ“š
    comp_match = re.search(r"ä¸‰[ã€ï¼.\s]*è«‹æ±‚è³ å„Ÿçš„äº‹å¯¦æ ¹æ“š[:ï¼š]?\s*(.*?)$", text, re.S)
    if comp_match:
        result["compensation_facts"] = comp_match.group(1).strip()
    
    return result

def extract_parties_with_llm(text: str) -> dict:
    """ä½¿ç”¨LLMæå–ç•¶äº‹äººï¼ˆæ›´æº–ç¢ºçš„æ–¹æ³•ï¼‰"""
    print("ğŸ¤– ä½¿ç”¨LLMæ™ºèƒ½æå–ç•¶äº‹äºº...")
    
    # å‰µå»ºæ›´ç²¾ç¢ºçš„æç¤ºæ¨¡æ¿
    prompt = f"""è«‹ä½ å¹«æˆ‘å¾ä»¥ä¸‹è»Šç¦æ¡ˆä»¶çš„æ³•å¾‹æ–‡ä»¶ä¸­æå–ä¸¦åˆ—å‡ºæ‰€æœ‰åŸå‘Šå’Œè¢«å‘Šçš„çœŸå¯¦å§“åã€‚

ä»¥ä¸‹æ˜¯æ¡ˆä»¶å…§å®¹ï¼š
{text}

æå–è¦æ±‚ï¼š
1. åƒ…æå–ã€ŒåŸå‘Šâ—‹â—‹â—‹ã€å’Œã€Œè¢«å‘Šâ—‹â—‹â—‹ã€ä¸­æ˜ç¢ºæåˆ°çš„çœŸå¯¦å§“å
2. ä¸è¦æå–ã€Œè¨´å¤–äººã€çš„å§“åï¼Œè¨´å¤–äººä¸æ˜¯ç•¶äº‹äºº
3. **é‡è¦**ï¼šå®Œæ•´ä¿ç•™å§“åï¼Œçµ•å°ä¸å¯æˆªæ–·æˆ–çœç•¥ä»»ä½•å­—ï¼ˆå¦‚ï¼šç¾…é–å´´å¿…é ˆå®Œæ•´å¯«æˆç¾…é–å´´ï¼Œä¸èƒ½å¯«æˆç¾…å´´ï¼‰
4. **é‡è¦**ï¼šå¦‚æœåŸæ–‡æ˜¯ã€ŒåŸå‘Šç¾…é–å´´ã€ï¼Œå¿…é ˆè¼¸å‡ºå®Œæ•´çš„ã€Œç¾…é–å´´ã€ä¸‰å€‹å­—
5. å¦‚æœæ–‡ä¸­æ²’æœ‰æ˜ç¢ºçš„å§“åï¼Œå°±ç›´æ¥å¯«ã€ŒåŸå‘Šã€ã€ã€Œè¢«å‘Šã€
6. å¤šå€‹å§“åç”¨é€—è™Ÿåˆ†éš”

è¼¸å‡ºæ ¼å¼ï¼ˆåªè¼¸å‡ºé€™å…©è¡Œï¼‰ï¼š
åŸå‘Š:å§“å1,å§“å2...
è¢«å‘Š:å§“å1,å§“å2...

ç¯„ä¾‹èªªæ˜ï¼š
- ã€ŒåŸå‘Šå³éº—å¨Ÿã€â†’ åŸå‘Š:å³éº—å¨Ÿ
- ã€Œè¢«å‘Šé„­å‡±ç¥¥ã€â†’ è¢«å‘Š:é„­å‡±ç¥¥
- ã€ŒåŸå‘Šç¾…é–å´´ã€â†’ åŸå‘Š:ç¾…é–å´´ï¼ˆå¿…é ˆä¿ç•™å®Œæ•´ä¸‰å€‹å­—ï¼‰
- ã€ŒåŸå‘Šé‚±å“å¦ã€â†’ åŸå‘Š:é‚±å“å¦ï¼ˆå¿…é ˆä¿ç•™å®Œæ•´ä¸‰å€‹å­—ï¼‰
- ã€Œè¨´å¤–äººé™³æ²³ç”°ã€â†’ ä¸æ˜¯ç•¶äº‹äººï¼Œå¿½ç•¥
- å¦‚æœåªèªªã€ŒåŸå‘Šã€æ²’æœ‰å§“å â†’ åŸå‘Š:åŸå‘Š
- å¦‚æœåªèªªã€Œè¢«å‘Šã€æ²’æœ‰å§“å â†’ è¢«å‘Š:è¢«å‘Š

**å‹™å¿…ç¢ºä¿å§“åå®Œæ•´æ€§ï¼Œä¸å¯çœç•¥ä»»ä½•å­—ï¼**"""

    try:
        # èª¿ç”¨LLM
        response = requests.post(
            LLM_URL,
            json={
                "model": DEFAULT_MODEL,
                "prompt": prompt,
                "stream": False
            },
            timeout=60
        )
        
        if response.status_code == 200:
            llm_result = response.json()["response"].strip()
            print(f"ğŸ¤– LLMæå–çµæœ: {llm_result}")
            return parse_llm_parties_result(llm_result)
        else:
            print(f"âŒ LLMèª¿ç”¨å¤±æ•—: {response.status_code}")
            return extract_parties_fallback(text)
            
    except Exception as e:
        print(f"âŒ LLMæå–ç•°å¸¸: {e}")
        return extract_parties_fallback(text)

def _is_valid_name(name: str) -> bool:
    """æª¢æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„å§“å"""
    import re
    
    # æ’é™¤åŒ…å«æ•¸å­—ã€è·æ¥­æè¿°ã€å¹´é½¡ç­‰çš„æ–‡å­—
    invalid_patterns = [
        r'\d+',  # åŒ…å«æ•¸å­—
        r'æ­²',   # å¹´é½¡
        r'å…ˆç”Ÿ|å¥³å£«|å°å§',  # ç¨±è¬‚
        r'ç¶“ç†|ä¸»ä»»|å¸æ©Ÿ|é ˜ç­|å“¡å·¥',  # è·æ¥­
        r'ä¼æ¥­|å…¬å¸|è¡Œè™Ÿ|åº—',  # å…¬å¸åç¨±
        r'ä¿‚|å³|ä¹‹|ç­‰|åŠ|æˆ–',  # é€£æ¥è©
        r'å—åƒ±äºº|åƒ±ç”¨äºº|æ³•å®šä»£ç†äºº',  # æ³•å¾‹ç”¨èª
    ]
    
    for pattern in invalid_patterns:
        if re.search(pattern, name):
            return False
    
    # æª¢æŸ¥æ˜¯å¦æ˜¯åˆç†çš„ä¸­æ–‡å§“åé•·åº¦ï¼ˆ2-4å€‹å­—ï¼‰
    if len(name) < 2 or len(name) > 6:
        return False
    
    # æª¢æŸ¥æ˜¯å¦ä¸»è¦ç”±ä¸­æ–‡å­—çµ„æˆ
    chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', name))
    if chinese_chars < len(name) * 0.7:  # è‡³å°‘70%æ˜¯ä¸­æ–‡å­—
        return False
    
    return True

def _clean_name_text(text: str) -> str:
    """æ¸…ç†å§“åæ–‡å­—ï¼Œç§»é™¤éå§“åå…§å®¹"""
    import re
    
    # ç§»é™¤å¹´é½¡ã€è·æ¥­ç­‰æè¿°
    cleaned = re.sub(r'\d+æ­²', '', text)
    cleaned = re.sub(r'å…ˆç”Ÿ|å¥³å£«|å°å§', '', cleaned)
    cleaned = re.sub(r'ï¼ˆ.*?ï¼‰|\(.*?\)', '', cleaned)  # ç§»é™¤æ‹¬è™Ÿå…§å®¹
    cleaned = re.sub(r'[ï¼Œ,]\s*\d+.*', '', cleaned)  # ç§»é™¤é€—è™Ÿå¾Œçš„æ•¸å­—å’Œæè¿°
    
    # æå–å¯èƒ½çš„å§“åï¼ˆ2-4å€‹ä¸­æ–‡å­—çš„çµ„åˆï¼‰
    name_matches = re.findall(r'[\u4e00-\u9fff]{2,4}', cleaned)
    if name_matches:
        return name_matches[0]  # è¿”å›ç¬¬ä¸€å€‹åŒ¹é…çš„å§“å
    
    return text.strip()

def parse_llm_parties_result(llm_result: str) -> dict:
    """è§£æLLMçš„ç•¶äº‹äººæå–çµæœ"""
    result = {"åŸå‘Š": "åŸå‘Š", "è¢«å‘Š": "è¢«å‘Š", "è¢«å‘Šæ•¸é‡": 1, "åŸå‘Šæ•¸é‡": 1}
    
    # æª¢æŸ¥LLMæ˜¯å¦è¿”å›äº†ç„¡æ•ˆçš„å›æ‡‰
    invalid_responses = ["è«‹æä¾›", "ç„¡æ³•æå–", "æ²’æœ‰æä¾›", "ç”±æ–¼æ‚¨æ²’æœ‰"]
    if any(invalid in llm_result for invalid in invalid_responses):
        print("âš ï¸ LLMè¿”å›ç„¡æ•ˆå›æ‡‰ï¼Œä½¿ç”¨fallback")
        return result
    
    lines = llm_result.split('\n')
    
    for line in lines:
        line = line.strip()
        if line.startswith('åŸå‘Š:') or line.startswith('åŸå‘Šï¼š'):
            plaintiff_text = line.split(':', 1)[1].strip() if ':' in line else line.split('ï¼š', 1)[1].strip()
            if plaintiff_text:
                # æ”¹é€²çš„åˆ†å‰²é‚è¼¯ï¼šåªæœ‰ç•¶ç¢ºå¯¦æœ‰å¤šå€‹æœ‰æ•ˆå§“åæ™‚æ‰åˆ†å‰²
                potential_plaintiffs = [p.strip() for p in plaintiff_text.split(',') if p.strip()]
                # é©—è­‰æ˜¯å¦çœŸçš„æ˜¯å¤šå€‹å§“å
                valid_plaintiffs = []
                for p in potential_plaintiffs:
                    # æª¢æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„å§“åï¼ˆä¸åŒ…å«æ•¸å­—ã€ä¸æ˜¯æè¿°æ€§æ–‡å­—ï¼‰
                    if _is_valid_name(p):
                        valid_plaintiffs.append(p)
                
                if len(valid_plaintiffs) > 1:
                    result["åŸå‘Š"] = "ã€".join(valid_plaintiffs)
                    result["åŸå‘Šæ•¸é‡"] = len(valid_plaintiffs)
                else:
                    # å–®ä¸€åŸå‘Šæˆ–ç„¡æ³•ç¢ºå®šå¤šå€‹ï¼Œä½¿ç”¨åŸå§‹æ–‡å­—ä½†æ¸…ç†éå§“åå…§å®¹
                    cleaned_name = _clean_name_text(plaintiff_text)
                    result["åŸå‘Š"] = cleaned_name if cleaned_name else "åŸå‘Š"
                    result["åŸå‘Šæ•¸é‡"] = 1
        
        elif line.startswith('è¢«å‘Š:') or line.startswith('è¢«å‘Šï¼š'):
            defendant_text = line.split(':', 1)[1].strip() if ':' in line else line.split('ï¼š', 1)[1].strip()
            if defendant_text:
                # æ”¹é€²çš„åˆ†å‰²é‚è¼¯ï¼šåªæœ‰ç•¶ç¢ºå¯¦æœ‰å¤šå€‹æœ‰æ•ˆå§“åæ™‚æ‰åˆ†å‰²
                potential_defendants = [d.strip() for d in defendant_text.split(',') if d.strip()]
                # é©—è­‰æ˜¯å¦çœŸçš„æ˜¯å¤šå€‹å§“å
                valid_defendants = []
                for d in potential_defendants:
                    # æª¢æŸ¥æ˜¯å¦æ˜¯æœ‰æ•ˆçš„å§“å
                    if _is_valid_name(d):
                        valid_defendants.append(d)
                
                if len(valid_defendants) > 1:
                    result["è¢«å‘Š"] = "ã€".join(valid_defendants)
                    result["è¢«å‘Šæ•¸é‡"] = len(valid_defendants)
                else:
                    # å–®ä¸€è¢«å‘Šæˆ–ç„¡æ³•ç¢ºå®šå¤šå€‹ï¼Œä½¿ç”¨åŸå§‹æ–‡å­—ä½†æ¸…ç†éå§“åå…§å®¹
                    cleaned_name = _clean_name_text(defendant_text)
                    result["è¢«å‘Š"] = cleaned_name if cleaned_name else "è¢«å‘Š"
                    result["è¢«å‘Šæ•¸é‡"] = 1
    
    return result

def extract_parties_fallback(text: str) -> dict:
    """ç•¶LLMæå–å¤±æ•—æ™‚çš„fallbackæ–¹æ³•ï¼ˆç°¡åŒ–ç‰ˆæ­£å‰‡ï¼‰"""
    print("âš ï¸ ä½¿ç”¨fallbackæ–¹æ³•æå–ç•¶äº‹äºº...")
    result = {"åŸå‘Š": "åŸå‘Š", "è¢«å‘Š": "è¢«å‘Š", "è¢«å‘Šæ•¸é‡": 1, "åŸå‘Šæ•¸é‡": 1}
    
    # ç°¡åŒ–çš„æ­£å‰‡è¡¨é”å¼æå–
    plaintiffs = set()
    defendants = set()
    
    # åŸºæœ¬æ¨¡å¼
    plaintiff_patterns = [
        r'åŸå‘Š([\u4e00-\u9fff]{2,4})',
        r'åŸå‘Š([ç”²ä¹™ä¸™ä¸æˆŠ])'
    ]
    
    defendant_patterns = [
        r'è¢«å‘Š([\u4e00-\u9fff]{2,4})',
        r'è¢«å‘Š([ç”²ä¹™ä¸™ä¸æˆŠ])'
    ]
    
    for pattern in plaintiff_patterns:
        matches = re.findall(pattern, text)
        plaintiffs.update(matches)
    
    for pattern in defendant_patterns:
        matches = re.findall(pattern, text)
        defendants.update(matches)
    
    # æ¸…ç†å’Œçµ„åˆçµæœ
    if plaintiffs:
        result["åŸå‘Š"] = "ã€".join(sorted(plaintiffs))
        result["åŸå‘Šæ•¸é‡"] = len(plaintiffs)
    elif "åŸå‘Š" in text:
        result["åŸå‘Š"] = "åŸå‘Š"
    
    if defendants:
        result["è¢«å‘Š"] = "ã€".join(sorted(defendants))
        result["è¢«å‘Šæ•¸é‡"] = len(defendants)
    elif "è¢«å‘Š" in text:
        result["è¢«å‘Š"] = "è¢«å‘Š"
    
    return result

def extract_parties(text: str) -> dict:
    """ä¸»è¦çš„ç•¶äº‹äººæå–å‡½æ•¸ï¼ˆå„ªå…ˆä½¿ç”¨LLMï¼‰"""
    return extract_parties_with_llm(text)

# ===== æª¢ç´¢ç›¸é—œå‡½æ•¸ =====
def embed(text: str):
    """æ–‡å­—å‘é‡åŒ–"""
    if not FULL_MODE:
        return []
    
    t = TOKENIZER(text, truncation=True, padding="max_length", max_length=512, return_tensors="pt")
    t = {k: v.to(device) for k, v in t.items()}
    with torch.no_grad():
        vec = MODEL(**t).last_hidden_state.mean(dim=1).squeeze()
    return vec.cpu().numpy().tolist()

def es_search(query_vector, case_type: str, top_k: int = 3, label: str = "Facts", quiet: bool = False):
    """ES æœå°‹ï¼ˆå«fallbackæ©Ÿåˆ¶ï¼‰"""
    if not FULL_MODE or not ES_HOST:
        return []
    
    def _search(label_filter, case_type_filter):
        must_clause = [{"match": {"label": label_filter}}]
        
        if case_type_filter:
            # å˜—è©¦å¤šç¨®å¯èƒ½çš„ case_type æ¬„ä½æ ¼å¼
            case_type_options = [
                {"term": {"case_type.keyword": case_type_filter}},
                {"term": {"case_type": case_type_filter}},
                {"match": {"case_type": case_type_filter}}
            ]
            
            # ä½¿ç”¨ should æŸ¥è©¢ï¼Œä»»ä¸€ç¬¦åˆå³å¯
            must_clause.append({
                "bool": {
                    "should": case_type_options,
                    "minimum_should_match": 1
                }
            })
            
        body = {
            "size": top_k,
            "query": {
                "script_score": {
                    "query": {"bool": {"must": must_clause}},
                    "script": {
                        "source": "cosineSimilarity(params.qv,'embedding')+1.0",
                        "params": {"qv": query_vector},
                    },
                }
            },
        }
        
        # èª¿è©¦ä¿¡æ¯ï¼šåªåœ¨éå®‰éœæ¨¡å¼ä¸‹è¼¸å‡º
        if not quiet:
            print(f"ğŸ” ESæŸ¥è©¢æ¢ä»¶: index={CHUNK_INDEX}, label={label_filter}, case_type={case_type_filter}")
        
        try:
            url = f"{ES_HOST}/{CHUNK_INDEX}/_search"
            response = requests.post(url, auth=ES_AUTH, json=body, verify=False)
            if response.status_code == 200:
                result = response.json()
                hits = result["hits"]["hits"]
                total_docs = result["hits"]["total"]["value"] if isinstance(result["hits"]["total"], dict) else result["hits"]["total"]
                if not quiet:
                    print(f"ğŸ“Š ESæŸ¥è©¢çµæœ: æ‰¾åˆ° {len(hits)} å€‹åŒ¹é…çµæœï¼Œç¸½æ–‡æª”æ•¸: {total_docs}")
                return hits
            else:
                if not quiet:
                    print(f"âŒ ESæŸ¥è©¢å¤±æ•—: {response.status_code} - {response.text}")
                return []
        except Exception as e:
            if not quiet:
                print(f"âŒ ESæŸ¥è©¢å¤±æ•—: {e}")
            return []

    if not quiet:
        print(f"ğŸ” ä½¿ç”¨ case_type='{case_type}' æœç´¢ç›¸ä¼¼æ¡ˆä¾‹...")
    hits = _search(label, case_type)
    
    if not hits:
        # å…ˆæª¢æŸ¥ç´¢å¼•æ˜ å°„å’Œå¯ç”¨çš„æ¡ˆä»¶é¡å‹
        try:
            # æª¢æŸ¥ mapping
            mapping_url = f"{ES_HOST}/{CHUNK_INDEX}/_mapping"
            mapping_response = requests.get(mapping_url, auth=ES_AUTH, verify=False)
            if mapping_response.status_code == 200:
                mapping = mapping_response.json()
                properties = mapping[CHUNK_INDEX]["mappings"]["properties"]
                has_case_type = "case_type" in properties
                print(f"ğŸ—ºï¸ case_typeæ¬„ä½å­˜åœ¨: {has_case_type}")
                
                if has_case_type:
                    # å˜—è©¦ä¸åŒçš„æ¬„ä½åç¨±
                    for field_name in ["case_type", "case_type.keyword"]:
                        try:
                            check_body = {
                                "size": 0,
                                "aggs": {
                                    "case_type_count": {"terms": {"field": field_name, "size": 20}}
                                }
                            }
                            check_url = f"{ES_HOST}/{CHUNK_INDEX}/_search"
                            check_response = requests.post(check_url, auth=ES_AUTH, json=check_body, verify=False)
                            if check_response.status_code == 200:
                                check_result = check_response.json()
                                available_types = [bucket["key"] for bucket in check_result["aggregations"]["case_type_count"]["buckets"]]
                                print(f"ğŸ“‹ ä½¿ç”¨æ¬„ä½ {field_name} æ‰¾åˆ°çš„æ¡ˆä»¶é¡å‹: {available_types}")
                                break
                        except Exception as field_e:
                            print(f"âš ï¸ æ¬„ä½ {field_name} æŸ¥è©¢å¤±æ•—: {field_e}")
                else:
                    print("âŒ case_typeæ¬„ä½ä¸å­˜åœ¨æ–¼ç´¢å¼•æ˜ å°„ä¸­")
            else:
                print(f"âŒ ç²å–æ˜ å°„å¤±æ•—: {mapping_response.status_code}")
                
        except Exception as e:
            print(f"âš ï¸ ç„¡æ³•æª¢æŸ¥ç´¢å¼•æ˜ å°„æˆ–æ¡ˆä»¶é¡å‹: {e}")
        
        fallback = CASE_TYPE_MAP.get(case_type, "å–®ç´”åŸè¢«å‘Šå„ä¸€")
        if fallback != case_type:
            print(f"âš ï¸ ä½¿ç”¨ fallback='{fallback}' é‡æ–°æœå°‹...")
            hits = _search(label, fallback)
    
    if not hits:
        print("âš ï¸ ä¸é™æ¡ˆä»¶é¡å‹é€²è¡Œæœå°‹...")
        hits = _search(label, None)
    
    return hits

def rerank_case_ids_by_paragraphs(query_text: str, case_ids: List[str], label: str = "Facts", quiet: bool = False) -> List[str]:
    """æ ¹æ“šæ®µè½ç´šè³‡æ–™é‡æ–°æ’åºæ¡ˆä¾‹"""
    if not FULL_MODE or not ES_HOST:
        return case_ids
    
    if not quiet:
        print("ğŸ“˜ å•Ÿå‹•æ®µè½ç´š rerank...")
    try:
        from sklearn.metrics.pairwise import cosine_similarity
        import numpy as np
    except ImportError:
        if not quiet:
            print("âš ï¸ sklearnæœªå®‰è£ï¼Œè·³érerank")
        return case_ids

    query_vec = embed(query_text)
    if not query_vec:
        return case_ids
    
    query_vec_np = np.array(query_vec).reshape(1, -1)
    scored_cases = []
    
    for cid in case_ids:
        try:
            # ä½¿ç”¨ requests ç›´æ¥èª¿ç”¨ ES API
            search_body = {
                "query": {
                    "bool": {
                        "must": [
                            {"term": {"case_id": cid}},
                            {"term": {"label": label}}
                        ]
                    }
                },
                "size": 1
            }
            url = f"{ES_HOST}/legal_kg_paragraphs/_search"
            response = requests.post(url, auth=ES_AUTH, json=search_body, verify=False)
            if response.status_code != 200:
                continue
            res = response.json()
            hits = res.get("hits", {}).get("hits", [])
            if hits:
                vec = hits[0]['_source']['embedding']
                para_vec_np = np.array(vec).reshape(1, -1)
                score = cosine_similarity(query_vec_np, para_vec_np)[0][0]
                scored_cases.append((cid, score))
        except Exception as e:
            print(f"âš ï¸ Case {cid} rerankå¤±æ•—: {e}")
            scored_cases.append((cid, 0.0))

    scored_cases.sort(key=lambda x: x[1], reverse=True)
    return [cid for cid, _ in scored_cases]

def get_complete_cases_content(case_ids: List[str]) -> List[str]:
    """ç²å–å®Œæ•´æ¡ˆä¾‹å…§å®¹"""
    if not FULL_MODE or not NEO4J_DRIVER:
        return []
    
    complete_cases = []
    
    try:
        with NEO4J_DRIVER.session() as session:
            for case_id in case_ids:
                # æŸ¥è©¢å®Œæ•´æ¡ˆä¾‹çš„äº‹å¯¦æ®µè½
                result = session.run("""
                    MATCH (c:Case {case_id: $case_id})-[:åŒ…å«]->(f:Facts)
                    RETURN f.description AS facts_content
                """, case_id=case_id).data()
                
                if result:
                    # çµ„åˆæ¡ˆä¾‹çš„æ‰€æœ‰äº‹å¯¦æ®µè½
                    case_content = "\n".join([record["facts_content"] for record in result if record["facts_content"]])
                    if case_content:
                        complete_cases.append(case_content)
                else:
                    print(f"âš ï¸ æ¡ˆä¾‹ {case_id} ç„¡æ³•ç²å–å®Œæ•´å…§å®¹")
                    
    except Exception as e:
        print(f"âš ï¸ ç²å–å®Œæ•´æ¡ˆä¾‹å…§å®¹å¤±æ•—: {e}")
    
    return complete_cases

def query_laws(case_ids):
    """å¾Neo4jæŸ¥è©¢æ³•æ¢è³‡è¨Š"""
    if not FULL_MODE or not NEO4J_DRIVER:
        return Counter(), {}
    
    counter = Counter()
    law_text_map = {}
    
    try:
        with NEO4J_DRIVER.session() as session:
            for cid in case_ids:
                result = session.run("""
                    MATCH (c:Case {case_id: $cid})-[:åŒ…å«]->(:Facts)-[:é©ç”¨]->(l:Laws)-[:åŒ…å«]->(ld:LawDetail)
                    RETURN collect(distinct ld.name) AS law_names, collect(distinct ld.text) AS law_texts
                """, cid=cid).single()
                if result:
                    names = result["law_names"]
                    texts = result["law_texts"]
                    counter.update(names)
                    for n, t in zip(names, texts):
                        if n not in law_text_map:
                            law_text_map[n] = t
    except Exception as e:
        print(f"âš ï¸ Neo4jæŸ¥è©¢å¤±æ•—: {e}")
    
    return counter, law_text_map

def get_similar_cases_laws_stats(case_ids):
    """ç²å–ç›¸ä¼¼æ¡ˆä¾‹çš„æ³•æ¢çµ±è¨ˆè³‡è¨Š"""
    counter, _ = query_laws(case_ids)
    return counter.most_common()

def normalize_article_number(article: str) -> str:
    """æ¢è™Ÿæ ¼å¼æ¨™æº–åŒ–ï¼šç¬¬191-2æ¢ â†’ ç¬¬191æ¢ä¹‹2"""
    # è™•ç†ç‰¹æ®Šæ ¼å¼çš„æ¢è™Ÿ
    article = re.sub(r'ç¬¬(\d+)-(\d+)æ¢', r'ç¬¬\1æ¢ä¹‹\2', article)
    return article

def detect_special_relationships(text: str, parties: dict) -> dict:
    """åµæ¸¬ç‰¹æ®Šæ³•å¾‹é—œä¿‚ï¼ˆå„ªåŒ–ç‰ˆï¼‰"""
    relationships = {
        "æœªæˆå¹´": False,
        "é›‡å‚­é—œä¿‚": False,
        "å‹•ç‰©æå®³": False,
        "å¤šè¢«å‘Š": parties.get('è¢«å‘Šæ•¸é‡', 1) > 1,
        "å¤šåŸå‘Š": parties.get('åŸå‘Šæ•¸é‡', 1) > 1   # æ–°å¢å¤šåŸå‘Šåˆ¤æ–·
    }
    
    # æ›´ç²¾ç¢ºçš„æœªæˆå¹´æª¢æ¸¬
    # 1. æ˜ç¢ºæåˆ°æœªæˆå¹´ç›¸é—œè©å½™
    explicit_minor_keywords = ["æœªæˆå¹´", "æ³•å®šä»£ç†äºº", "ç›£è­·äºº", "æœªæ»¿åå…«æ­²", "æœªæ»¿18æ­²"]
    if any(keyword in text for keyword in explicit_minor_keywords):
        relationships["æœªæˆå¹´"] = True
    
    # 2. æª¢æŸ¥å…·é«”å¹´é½¡ï¼ˆ18æ­²ä»¥ä¸‹ï¼‰
    age_pattern = r'(\d+)\s*æ­²'
    age_matches = re.findall(age_pattern, text)
    for age_str in age_matches:
        age = int(age_str)
        if age < 18:
            relationships["æœªæˆå¹´"] = True
            break
    
    # 3. å­¸æ ¡é—œéµå­—éœ€è¦æ›´è¬¹æ…
    school_keywords = ["åœ‹ä¸­ç”Ÿ", "åœ‹å°ç”Ÿ", "é«˜ä¸­ç”Ÿ"]  # ä¸æ˜¯å–®ç´”çš„"åœ‹ä¸­"ã€"é«˜ä¸­"
    if any(keyword in text for keyword in school_keywords):
        relationships["æœªæˆå¹´"] = True
    
    # æª¢æŸ¥é›‡å‚­é—œä¿‚
    employment_keywords = ["å—åƒ±", "åƒ±ç”¨", "é›‡ä¸»", "å“¡å·¥", "è·å‹™", "å·¥ä½œæ™‚é–“", "å…¬å¸è»Š", "åŸ·è¡Œè·å‹™"]
    relationships["é›‡å‚­é—œä¿‚"] = any(keyword in text for keyword in employment_keywords)
    
    # æª¢æŸ¥å‹•ç‰©æå®³
    animal_keywords = ["ç‹—", "è²“", "çŠ¬", "å‹•ç‰©", "å¯µç‰©", "å’¬å‚·", "æŠ“å‚·"]
    relationships["å‹•ç‰©æå®³"] = any(keyword in text for keyword in animal_keywords)
    
    return relationships

def determine_case_type(accident_facts: str, parties: dict) -> str:
    """åˆ¤æ–·æ¡ˆä»¶é¡å‹ï¼ˆä¸ƒå¤§åŸºæœ¬é¡å‹ï¼‰"""
    relationships = detect_special_relationships(accident_facts, parties)
    
    # å„ªå…ˆåˆ¤æ–·ç‰¹æ®Šæ³•æ¢é¡å‹ï¼ˆäº’æ–¥å„ªå…ˆç´šï¼‰
    if relationships["å‹•ç‰©æå®³"]:
        return "Â§190å‹•ç‰©æ¡ˆå‹"
    elif relationships["é›‡å‚­é—œä¿‚"]: 
        return "Â§188åƒ±ç”¨äººæ¡ˆå‹"
    elif relationships["æœªæˆå¹´"]:
        return "Â§187æœªæˆå¹´æ¡ˆå‹"
    
    # å…¶æ¬¡åˆ¤æ–·ç•¶äº‹äººæ•¸é‡é¡å‹
    elif relationships["å¤šåŸå‘Š"] and relationships["å¤šè¢«å‘Š"]:
        return "åŸè¢«å‘Šçš†æ•¸å"
    elif relationships["å¤šåŸå‘Š"]:
        return "æ•¸ååŸå‘Š"
    elif relationships["å¤šè¢«å‘Š"]:
        return "æ•¸åè¢«å‘Š"
    else:
        return "å–®ç´”åŸè¢«å‘Šå„ä¸€"

def determine_applicable_laws(accident_facts: str, injuries: str, comp_facts: str, parties: dict) -> List[str]:
    """æ ¹æ“šæ¡ˆä»¶äº‹å¯¦æ™ºèƒ½åˆ¤æ–·é©ç”¨æ³•æ¢"""
    applicable_laws = []
    
    # åµæ¸¬ç‰¹æ®Šé—œä¿‚
    relationships = detect_special_relationships(accident_facts + injuries + comp_facts, parties)
    
    # 1. ç¬¬184æ¢ç¬¬1é …å‰æ®µ - åŸºæœ¬ä¾µæ¬Šè²¬ä»»ï¼ˆå¿…é ˆï¼‰
    applicable_laws.append("æ°‘æ³•ç¬¬184æ¢ç¬¬1é …å‰æ®µ")
    
    # 2. è»Šç¦æ¡ˆä»¶ - ç¬¬191æ¢ä¹‹2ï¼ˆäº¤é€šå·¥å…·ï¼‰
    traffic_keywords = ["æ±½è»Š", "æ©Ÿè»Š", "è»Šè¼›", "é§•é§›", "äº¤é€š", "æ’", "ç¢°æ’"]
    if any(keyword in accident_facts for keyword in traffic_keywords):
        applicable_laws.append("æ°‘æ³•ç¬¬191æ¢ä¹‹2")
    
    # 3. èº«é«”å¥åº·æå®³ - ç¬¬193æ¢ç¬¬1é …
    health_damage_keywords = ["é†«ç™‚", "çœ‹è­·", "å·¥ä½œæå¤±", "è–ªè³‡", "æ”¶å…¥", "å‹å‹•èƒ½åŠ›"]
    if injuries or any(keyword in comp_facts for keyword in health_damage_keywords):
        applicable_laws.append("æ°‘æ³•ç¬¬193æ¢ç¬¬1é …")
    
    # 4. ç²¾ç¥æ…°æ’«é‡‘ - ç¬¬195æ¢ç¬¬1é …å‰æ®µ
    mental_damage_keywords = ["ç²¾ç¥", "æ…°æ’«", "ç—›è‹¦", "åè­½", "äººæ ¼"]
    if any(keyword in comp_facts for keyword in mental_damage_keywords):
        applicable_laws.append("æ°‘æ³•ç¬¬195æ¢ç¬¬1é …å‰æ®µ")
    
    # 5. ç‰¹æ®Šæƒ…æ³è™•ç†ï¼ˆäº’æ–¥è¦å‰‡ï¼‰
    if relationships["é›‡å‚­é—œä¿‚"]:
        # é›‡å‚­é—œä¿‚å„ªå…ˆé©ç”¨ç¬¬188æ¢ç¬¬1é …æœ¬æ–‡
        applicable_laws.append("æ°‘æ³•ç¬¬188æ¢ç¬¬1é …æœ¬æ–‡")
    elif relationships["å¤šè¢«å‘Š"]:
        # å¤šè¢«å‘Šä½†ç„¡é›‡å‚­é—œä¿‚æ™‚é©ç”¨ç¬¬185æ¢ç¬¬1é …
        applicable_laws.append("æ°‘æ³•ç¬¬185æ¢ç¬¬1é …")
    
    # 6. æœªæˆå¹´æ¡ˆä»¶ - ç¬¬187æ¢ç¬¬1é …
    if relationships["æœªæˆå¹´"]:
        applicable_laws.append("æ°‘æ³•ç¬¬187æ¢ç¬¬1é …")
    
    # 7. å‹•ç‰©æå®³ - ç¬¬190æ¢ç¬¬1é …
    if relationships["å‹•ç‰©æå®³"]:
        applicable_laws.append("æ°‘æ³•ç¬¬190æ¢ç¬¬1é …")
    
    # æ¨™æº–åŒ–æ¢è™Ÿæ ¼å¼
    applicable_laws = [normalize_article_number(law) for law in applicable_laws]
    
    return list(dict.fromkeys(applicable_laws))  # å»é‡ä½†ä¿æŒé †åº


    

# ===== æ··åˆæ¨¡å¼ç”Ÿæˆå™¨ =====
class HybridCoTGenerator:
    """æ··åˆæ¨¡å¼ç”Ÿæˆå™¨ï¼šäº‹å¯¦æ³•æ¢æå®³ç”¨æ¨™æº–ï¼Œçµè«–ç”¨CoT"""
    
    def __init__(self, model_name: str = DEFAULT_MODEL):
        self.model_name = model_name
        self.llm_url = LLM_URL
        
        # åˆå§‹åŒ–é‡‘é¡è™•ç†å™¨
        if STRUCTURED_PROCESSOR_AVAILABLE:
            self.structured_processor = StructuredLegalAmountProcessor()
        else:
            self.structured_processor = None
        
        if BASIC_STANDARDIZER_AVAILABLE:
            self.basic_standardizer = LegalAmountStandardizer()
        else:
            self.basic_standardizer = None
        
        # åˆå§‹åŒ–é€šç”¨æ ¼å¼è™•ç†å™¨
        if UNIVERSAL_FORMAT_HANDLER_AVAILABLE:
            self.format_handler = UniversalFormatHandler()
        else:
            self.format_handler = None
        
        # æª¢æŸ¥LLMé€£æ¥
        self.llm_available = self._check_llm_connection()
    
    def _check_llm_connection(self) -> bool:
        """æª¢æŸ¥LLMé€£æ¥"""
        try:
            response = requests.get("http://localhost:11434/api/version", timeout=5)
            return response.status_code == 200
        except:
            return False
    
    def call_llm(self, prompt: str, timeout: int = 180) -> str:
        """èª¿ç”¨LLM"""
        if not self.llm_available:
            return "âŒ LLMæœå‹™ä¸å¯ç”¨"
        
        try:
            response = requests.post(
                self.llm_url,
                json={
                    "model": self.model_name,
                    "prompt": prompt,
                    "stream": False
                },
                timeout=timeout
            )
            
            if response.status_code == 200:
                return response.json()["response"].strip()
            else:
                return f"âŒ LLM APIéŒ¯èª¤: {response.status_code}"
                
        except Exception as e:
            return f"âŒ LLMèª¿ç”¨å¤±æ•—: {str(e)}"
    
    def _chinese_num(self, num: int) -> str:
        """æ•¸å­—è½‰ä¸­æ–‡"""
        chinese = ["", "ä¸€", "äºŒ", "ä¸‰", "å››", "äº”", "å…­", "ä¸ƒ", "å…«", "ä¹", "å"]
        if num <= 10:
            return chinese[num]
        return str(num)
    
    def _extract_all_plaintiffs(self, text: str) -> List[str]:
        """æå–æ‰€æœ‰åŸå‘Šå§“å"""
        plaintiffs = []
        
        # æ–¹æ³•1ï¼šå¾æ–‡æœ¬ä¸­æ‰¾ã€ŒåŸå‘Šâ—‹â—‹â—‹ã€çš„æ¨¡å¼
        pattern = r'åŸå‘Š([^ï¼Œã€‚ï¼›ã€\s]{2,4})(?:ç‚º|å› |å—|å‰å¾€|æ”¯å‡º)'
        matches = re.findall(pattern, text)
        plaintiffs.extend(matches)
        
        # å»é‡ä¸¦ä¿æŒé †åº
        seen = set()
        unique_plaintiffs = []
        for p in plaintiffs:
            if p not in seen:
                seen.add(p)
                unique_plaintiffs.append(p)
        
        return unique_plaintiffs
    
    def generate_standard_facts(self, accident_facts: str, similar_cases: List[str] = None) -> str:
        """æ¨™æº–æ–¹å¼ç”Ÿæˆäº‹å¯¦æ®µè½ï¼ˆå«ç›¸ä¼¼æ¡ˆä¾‹åƒè€ƒï¼‰"""
        print("ğŸ“ ä½¿ç”¨æ¨™æº–æ–¹å¼ç”Ÿæˆäº‹å¯¦æ®µè½...")
        
        # çµ„åˆç›¸ä¼¼æ¡ˆä¾‹åƒè€ƒ
        reference_text = ""
        if similar_cases:
            reference_text = "\n\nåƒè€ƒç›¸ä¼¼æ¡ˆä¾‹ï¼š\n" + "\n".join([f"{i+1}. {case}" for i, case in enumerate(similar_cases[:2])])
        
        prompt = f"""ä½ æ˜¯å°ç£å¾‹å¸«ï¼Œè«‹æ ¹æ“šä»¥ä¸‹äº‹å¯¦ææ–™æ’°å¯«èµ·è¨´ç‹€çš„äº‹å¯¦æ®µè½ï¼š

äº‹å¯¦ææ–™ï¼š
{accident_facts}{reference_text}

è¦æ±‚ï¼š
1. ä»¥ã€Œç·£è¢«å‘Šã€é–‹é ­
2. ä½¿ç”¨ã€ŒåŸå‘Šã€ã€ã€Œè¢«å‘Šã€ç¨±è¬‚ï¼Œä½†å¿…é ˆä¿æŒå§“åçš„å®Œæ•´æ€§å’Œæº–ç¢ºæ€§
3. å®¢è§€æè¿°äº‹æ•…ç¶“éï¼Œç¢ºä¿èªæ³•æ­£ç¢ºæµæš¢
4. åƒè€ƒç›¸ä¼¼æ¡ˆä¾‹çš„æ•˜è¿°æ–¹å¼ï¼Œä½†ä¸å¾—æŠ„è¥²
5. æ ¼å¼ï¼šä¸€ã€äº‹å¯¦æ¦‚è¿°ï¼š[å…§å®¹]
6. **é‡è¦**ï¼šå¦‚æœäº‹å¯¦ææ–™ä¸­æœ‰å…·é«”å§“åï¼Œè«‹å®Œæ•´ä¿ç•™ï¼Œä¸è¦æˆªæ–·æˆ–æ”¹è®Šä»»ä½•å­—å…ƒ
7. **ç¦æ­¢äº‹é …**ï¼šçµ•å°ä¸å¯ä»¥åœ¨è¼¸å‡ºä¸­åŒ…å«ä»»ä½•æ‹¬è™Ÿæé†’æ–‡å­—ï¼Œå¦‚ã€Œï¼ˆå§“åï¼šè«‹å¡«å¯«...ï¼‰ã€ã€ã€Œï¼ˆè«‹å¡«å¯«...ï¼‰ã€ç­‰æç¤ºå…§å®¹
8. **ç›´æ¥è¼¸å‡º**ï¼šåªè¼¸å‡ºå®Œæ•´çš„äº‹å¯¦æ®µè½ï¼Œä¸è¦åŒ…å«ä»»ä½•éœ€è¦ç”¨æˆ¶å¡«å¯«çš„ç©ºç™½æˆ–æé†’
9. **èªæ³•è¦æ±‚**ï¼šè«‹ç‰¹åˆ¥æ³¨æ„å¥å­çµæ§‹çš„å®Œæ•´æ€§ï¼Œé¿å…å‡ºç¾èªæ³•éŒ¯èª¤æˆ–ä¸å®Œæ•´çš„å¥å­

è«‹ç›´æ¥è¼¸å‡ºå®Œæ•´çš„äº‹å¯¦æ®µè½ï¼š"""
        
        result = self.call_llm(prompt)
        
        # æ¸…ç†æ‹¬è™Ÿæé†’æ–‡å­—
        result = self._remove_bracket_reminders(result)
        
        # ä¿®æ­£èªæ³•éŒ¯èª¤
        result = self._fix_grammar_errors(result)
        
        # æå–äº‹å¯¦æ®µè½
        fact_match = re.search(r"ä¸€ã€äº‹å¯¦æ¦‚è¿°ï¼š\s*(.*?)(?:\n\n|$)", result, re.S)
        if fact_match:
            cleaned_content = fact_match.group(1).strip()
            return f"ä¸€ã€äº‹å¯¦æ¦‚è¿°ï¼š\n{cleaned_content}"
        elif "ç·£è¢«å‘Š" in result:
            # æ‰¾åˆ°åŒ…å«"ç·£è¢«å‘Š"çš„è¡Œ
            for line in result.split('\n'):
                if "ç·£è¢«å‘Š" in line:
                    cleaned_line = line.strip()
                    return f"ä¸€ã€äº‹å¯¦æ¦‚è¿°ï¼š\n{cleaned_line}"
        
        # Fallback
        facts_content = accident_facts.replace('ç·£è¢«å‘Š', '').strip()
        return f"ä¸€ã€äº‹å¯¦æ¦‚è¿°ï¼š\nç·£è¢«å‘Š{facts_content}"
    
    def generate_standard_laws(self, accident_facts: str, injuries: str, parties: dict, compensation_facts: str = "") -> str:
        """æ¨™æº–æ–¹å¼ç”Ÿæˆæ³•å¾‹ä¾æ“šï¼ˆç¬¦åˆæ³•æ¢å¼•ç”¨è¦ç¯„ï¼‰"""
        print("âš–ï¸ ä½¿ç”¨æ¨™æº–æ–¹å¼ç”Ÿæˆæ³•å¾‹ä¾æ“š...")
        
        # æ™ºèƒ½åˆ¤æ–·é©ç”¨æ³•æ¢
        applicable_laws = determine_applicable_laws(accident_facts, injuries, compensation_facts, parties)
        
        # å®Œæ•´çš„æ³•æ¢èªªæ˜å°ç…§è¡¨ï¼ˆç²¾ç¢ºåˆ°é …ã€æ®µã€ä½†æ›¸ï¼‰
        law_descriptions = {
            "æ°‘æ³•ç¬¬184æ¢ç¬¬1é …å‰æ®µ": "å› æ•…æ„æˆ–éå¤±ï¼Œä¸æ³•ä¾µå®³ä»–äººä¹‹æ¬Šåˆ©è€…ï¼Œè² æå®³è³ å„Ÿè²¬ä»»ã€‚",
            "æ°‘æ³•ç¬¬185æ¢ç¬¬1é …": "æ•¸äººå…±åŒä¸æ³•ä¾µå®³ä»–äººä¹‹æ¬Šåˆ©è€…ï¼Œé€£å¸¶è² æå®³è³ å„Ÿè²¬ä»»ã€‚",
            "æ°‘æ³•ç¬¬187æ¢ç¬¬1é …": "ç„¡è¡Œç‚ºèƒ½åŠ›äººæˆ–é™åˆ¶è¡Œç‚ºèƒ½åŠ›äººï¼Œä¸æ³•ä¾µå®³ä»–äººä¹‹æ¬Šåˆ©è€…ï¼Œä»¥è¡Œç‚ºæ™‚æœ‰è­˜åˆ¥èƒ½åŠ›ç‚ºé™ï¼Œèˆ‡å…¶æ³•å®šä»£ç†äººé€£å¸¶è² æå®³è³ å„Ÿè²¬ä»»ã€‚",
            "æ°‘æ³•ç¬¬188æ¢ç¬¬1é …æœ¬æ–‡": "å—åƒ±äººå› åŸ·è¡Œè·å‹™ï¼Œä¸æ³•ä¾µå®³ä»–äººä¹‹æ¬Šåˆ©è€…ï¼Œç”±åƒ±ç”¨äººèˆ‡è¡Œç‚ºäººé€£å¸¶è² æå®³è³ å„Ÿè²¬ä»»ã€‚",
            "æ°‘æ³•ç¬¬190æ¢ç¬¬1é …": "å‹•ç‰©åŠ æå®³æ–¼ä»–äººè€…ï¼Œç”±å…¶å æœ‰äººè² æå®³è³ å„Ÿè²¬ä»»ã€‚",
            "æ°‘æ³•ç¬¬191æ¢ä¹‹2": "æ±½è»Šã€æ©Ÿè»Šæˆ–å…¶ä»–éä¾è»Œé“è¡Œé§›ä¹‹å‹•åŠ›è»Šè¼›ï¼Œåœ¨ä½¿ç”¨ä¸­åŠ æå®³æ–¼ä»–äººè€…ï¼Œé§•é§›äººæ‡‰è³ å„Ÿå› æ­¤æ‰€ç”Ÿä¹‹æå®³ã€‚",
            "æ°‘æ³•ç¬¬193æ¢ç¬¬1é …": "ä¸æ³•ä¾µå®³ä»–äººä¹‹èº«é«”æˆ–å¥åº·è€…ï¼Œå°æ–¼è¢«å®³äººå› æ­¤å–ªå¤±æˆ–æ¸›å°‘å‹å‹•èƒ½åŠ›æˆ–å¢åŠ ç”Ÿæ´»ä¸Šä¹‹éœ€è¦æ™‚ï¼Œæ‡‰è² æå®³è³ å„Ÿè²¬ä»»ã€‚",
            "æ°‘æ³•ç¬¬195æ¢ç¬¬1é …å‰æ®µ": "ä¸æ³•ä¾µå®³ä»–äººä¹‹èº«é«”ã€å¥åº·ã€åè­½ã€è‡ªç”±ã€ä¿¡ç”¨ã€éš±ç§ã€è²æ“ï¼Œæˆ–ä¸æ³•ä¾µå®³å…¶ä»–äººæ ¼æ³•ç›Šè€Œæƒ…ç¯€é‡å¤§è€…ï¼Œè¢«å®³äººé›–éè²¡ç”¢ä¸Šä¹‹æå®³ï¼Œäº¦å¾—è«‹æ±‚è³ å„Ÿç›¸ç•¶ä¹‹é‡‘é¡ã€‚"
        }
        
        # çµ„åˆæ³•æ¢å…§å®¹ï¼ˆå…ˆåˆ—æ³•æ¢å…§å®¹ï¼‰
        law_texts = []
        valid_laws = []
        
        for law in applicable_laws:
            if law in law_descriptions:
                law_texts.append(f"ã€Œ{law_descriptions[law]}ã€")
                valid_laws.append(law)
        
        if not law_texts:
            # Fallbackï¼šè‡³å°‘åŒ…å«åŸºæœ¬ä¾µæ¬Šæ¢æ–‡
            law_texts = ["ã€Œå› æ•…æ„æˆ–éå¤±ï¼Œä¸æ³•ä¾µå®³ä»–äººä¹‹æ¬Šåˆ©è€…ï¼Œè² æå®³è³ å„Ÿè²¬ä»»ã€‚ã€"]
            valid_laws = ["æ°‘æ³•ç¬¬184æ¢ç¬¬1é …å‰æ®µ"]
        
        # æŒ‰æ¢è™Ÿæ•¸å­—æ’åºæ³•æ¢
        sorted_laws_with_content = self._sort_laws_by_article_number(valid_laws, law_descriptions)
        
        # æå–æ’åºå¾Œçš„å…§å®¹å’Œæ¢è™Ÿ
        sorted_law_texts = [item[1] for item in sorted_laws_with_content]
        sorted_article_list = [item[0] for item in sorted_laws_with_content]
        
        # æŒ‰æ­£ç¢ºæ ¼å¼çµ„åˆï¼šå…ˆæ³•æ¢å…§å®¹ï¼Œå¾Œæ¢è™Ÿ
        law_content_block = "ã€".join(sorted_law_texts)
        article_list = "ã€".join(sorted_article_list)
        
        print(f"âœ… é©ç”¨æ³•æ¢: {', '.join(sorted_article_list)}")
        
        return f"""äºŒã€æ³•å¾‹ä¾æ“šï¼š
æŒ‰{law_content_block}ï¼Œ{article_list}åˆ†åˆ¥å®šæœ‰æ˜æ–‡ã€‚æŸ¥è¢«å‘Šå› ä¸Šé–‹ä¾µæ¬Šè¡Œç‚ºï¼Œè‡´åŸå‘Šå—æœ‰ä¸‹åˆ—æå®³ï¼Œä¾å‰æ­è¦å®šï¼Œè¢«å‘Šæ‡‰è² æå®³è³ å„Ÿè²¬ä»»ï¼š"""
    
    def _parse_damage_from_sentence(self, sentence: str, plaintiff: str) -> List[dict]:
        """å¾å¥å­ä¸­è§£ææå®³é …ç›®ï¼ˆæ”¹é€²ç‰ˆï¼‰"""
        damages = []
        
        # æ“´å……æå®³é¡å‹æ¨¡å¼
        damage_patterns = [
            {
                'keywords': ['é†«ç™‚è²»ç”¨', 'æ²»ç™‚', 'é†«é™¢', 'è¨ºæ‰€', 'å°±è¨º'],
                'name': 'é†«ç™‚è²»ç”¨',
                'template': f'åŸå‘Š{plaintiff}å› æœ¬æ¬¡äº‹æ•…å—å‚·ï¼Œå‰å¾€é†«é™¢æ²»ç™‚æ‰€æ”¯å‡ºä¹‹é†«ç™‚è²»ç”¨'
            },
            {
                'keywords': ['äº¤é€šè²»'],
                'name': 'äº¤é€šè²»ç”¨',
                'template': f'åŸå‘Š{plaintiff}å› æœ¬æ¬¡äº‹æ•…æ‰€ç”Ÿä¹‹äº¤é€šè²»ç”¨'
            },
            {
                'keywords': ['å·¥è³‡æå¤±', 'ä¸èƒ½å·¥ä½œ', 'å·¥ä½œæå¤±', 'ç„¡æ³•å·¥ä½œ', 'ä¼‘é¤Š'],
                'name': 'å·¥ä½œæå¤±',
                'template': f'åŸå‘Š{plaintiff}å› æœ¬æ¬¡äº‹æ•…ç„¡æ³•å·¥ä½œä¹‹æ”¶å…¥æå¤±'
            },
            {
                'keywords': ['æ…°æ’«é‡‘', 'ç²¾ç¥'],
                'name': 'ç²¾ç¥æ…°æ’«é‡‘',
                'template': f'åŸå‘Š{plaintiff}å› æœ¬æ¬¡äº‹æ•…æ‰€å—ç²¾ç¥ä¸Šç—›è‹¦ä¹‹æ…°æ’«é‡‘'
            },
            {
                'keywords': ['è»Šè¼›è²¶å€¼', 'è²¶æ', 'åƒ¹å€¼æ¸›æ', 'äº¤æ˜“åƒ¹å€¼'],
                'name': 'è»Šè¼›è²¶å€¼æå¤±',
                'template': f'ç³»çˆ­è»Šè¼›å› æœ¬æ¬¡äº‹æ•…è²¶å€¼ä¹‹æå¤±'
            },
            {
                'keywords': ['é‘‘å®šè²»'],
                'name': 'é‘‘å®šè²»ç”¨',
                'template': f'ç‚ºè©•ä¼°è»Šè¼›æå¤±æ‰€æ”¯å‡ºä¹‹é‘‘å®šè²»ç”¨'
            }
        ]
        
        # æ›´ç²¾ç¢ºçš„é‡‘é¡æå–ï¼ˆè€ƒæ…®å‰å¾Œæ–‡ï¼‰
        # ä¸è¦åªçœ‹åˆ°é‡‘é¡å°±æå–ï¼Œè¦çœ‹é‡‘é¡å‰å¾Œçš„æè¿°
        
        return damages
    
    def generate_smart_compensation(self, injuries: str, comp_facts: str, parties: dict) -> str:
        """æ™ºèƒ½ç”Ÿæˆæå®³é …ç›®ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰"""
        print("ğŸ’° ç”Ÿæˆæå®³è³ å„Ÿ...")
        
        # ç›´æ¥ä½¿ç”¨LLMè™•ç†ï¼Œé¿å…è¤‡é›œçš„çµæ§‹åŒ–è™•ç†
        return self._generate_llm_based_compensation(comp_facts, parties)

    def _preprocess_chinese_numbers(self, text: str) -> str:
        """é è™•ç†ä¸­æ–‡æ•¸å­—ï¼Œè½‰æ›ç‚ºé˜¿æ‹‰ä¼¯æ•¸å­—"""
        import re
        
        # è™•ç† Xè¬Y,YYYå…ƒ æ ¼å¼ (å¦‚ï¼š26è¬4,379å…ƒ)
        pattern1 = r'(\d+)è¬(\d+,?\d+)å…ƒ'
        def replace1(match):
            wan = int(match.group(1))
            rest = int(match.group(2).replace(',', ''))
            total = wan * 10000 + rest
            return f"{total:,}å…ƒ"
        text = re.sub(pattern1, replace1, text)
        
        # è™•ç† Xè¬Yåƒå…ƒ æ ¼å¼ (å¦‚ï¼š30è¬5åƒå…ƒ)
        pattern2 = r'(\d+)è¬(\d+)åƒå…ƒ'
        def replace2(match):
            wan = int(match.group(1))
            qian = int(match.group(2))
            total = wan * 10000 + qian * 1000
            return f"{total:,}å…ƒ"
        text = re.sub(pattern2, replace2, text)
        
        # è™•ç† Xè¬å…ƒ æ ¼å¼ (å¦‚ï¼š20è¬å…ƒ)
        pattern3 = r'(\d+)è¬å…ƒ'
        def replace3(match):
            wan = int(match.group(1))
            total = wan * 10000
            return f"{total:,}å…ƒ"
        text = re.sub(pattern3, replace3, text)
        
        # è™•ç† Xåƒå…ƒ æ ¼å¼ (å¦‚ï¼š5åƒå…ƒ)
        pattern4 = r'(\d+)åƒå…ƒ'
        def replace4(match):
            qian = int(match.group(1))
            total = qian * 1000
            return f"{total:,}å…ƒ"
        text = re.sub(pattern4, replace4, text)
        
        return text
    
    def _remove_bracket_reminders(self, text: str) -> str:
        """ç§»é™¤æ–‡æœ¬ä¸­çš„æ‹¬è™Ÿæé†’æ–‡å­—"""
        import re
        
        # ç§»é™¤å„ç¨®æ‹¬è™Ÿæé†’æ¨¡å¼
        patterns = [
            r'\ï¼ˆ[^ï¼‰]*è«‹å¡«å¯«[^ï¼‰]*\ï¼‰',  # ï¼ˆå§“åï¼šè«‹å¡«å¯«...ï¼‰
            r'\ï¼ˆ[^ï¼‰]*è«‹[^ï¼‰]*\ï¼‰',     # ï¼ˆè«‹...ï¼‰
            r'\ï¼ˆ[^ï¼‰]*ï¼š[^ï¼‰]*\ï¼‰',     # ï¼ˆä»»ä½•ï¼šèªªæ˜ï¼‰
            r'\ï¼ˆ[^ï¼‰]*å¡«å¯«[^ï¼‰]*\ï¼‰',   # ï¼ˆ...å¡«å¯«...ï¼‰
            r'\ï¼ˆ[^ï¼‰]*è¼¸å…¥[^ï¼‰]*\ï¼‰',   # ï¼ˆ...è¼¸å…¥...ï¼‰
            r'\ï¼ˆ[^ï¼‰]*è£œå……[^ï¼‰]*\ï¼‰',   # ï¼ˆ...è£œå……...ï¼‰
        ]
        
        cleaned_text = text
        for pattern in patterns:
            cleaned_text = re.sub(pattern, '', cleaned_text)
        
        # æ¸…ç†å¤šé¤˜çš„ç©ºæ ¼
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
        
        return cleaned_text
    
    def _fix_grammar_errors(self, text: str) -> str:
        """ä¿®æ­£æ–‡æœ¬ä¸­çš„èªæ³•éŒ¯èª¤"""
        import re
        
        cleaned_text = text
        
        # ä¿®æ­£å¸¸è¦‹çš„èªæ³•éŒ¯èª¤æ¨¡å¼
        grammar_fixes = [
            # ä¿®æ­£ã€Œè‡´æ’æ“Šè»Šå¾Œä¹‹æ©Ÿè»Šé“ä¸Šï¼Œç”±åŸå‘Šæ‰€é¨ä¹˜...ã€é¡å‹çš„éŒ¯èª¤
            (r'è‡´æ’æ“Šè»Šå¾Œä¹‹æ©Ÿè»Šé“ä¸Šï¼Œç”±([^æ‰€]+)æ‰€é¨ä¹˜([^æ™®]+)æ™®é€šé‡å‹æ©Ÿè»Š', 
             r'è‡´æ’æ“Š\1æ‰€é¨ä¹˜ä¹‹\2æ™®é€šé‡å‹æ©Ÿè»Š'),
            
            # ä¿®æ­£å…¶ä»–å¸¸è¦‹çš„èªæ³•éŒ¯èª¤
            (r'ï¼Œç”±([^æ‰€]+)æ‰€é¨ä¹˜([^ä¹‹]+)ä¹‹([^ï¼Œã€‚]+)ï¼Œ', r'ï¼Œæ’æ“Š\1æ‰€é¨ä¹˜ä¹‹\2\3ï¼Œ'),
            
            # ä¿®æ­£å¥å­çµæ§‹ä¸å®Œæ•´çš„å•é¡Œ
            (r'è‡´æ’æ“Š([^ï¼Œã€‚]+)ï¼Œ$', r'è‡´æ’æ“Š\1ã€‚'),
            
            # ä¿®æ­£é‡è¤‡çš„ä»‹è©æˆ–é€£è©
            (r'ä¹‹ä¹‹', r'ä¹‹'),
            (r'æ–¼æ–¼', r'æ–¼'),
            (r'ï¼Œï¼Œ', r'ï¼Œ'),
            
            # ä¿®æ­£è»Šè¼›æè¿°çš„èªæ³•
            (r'è»Šå¾Œä¹‹æ©Ÿè»Šé“ä¸Šï¼Œç”±', r'è»Šè¼›ï¼Œè©²è»Šè¼›ç”±'),
            
            # ä¿®æ­£ä¸å®Œæ•´çš„å¥å­çµæ§‹
            (r'ç”±([^æ‰€]+)æ‰€é¨ä¹˜([^ï¼Œã€‚]+)ï¼Œ$', r'ç”±\1æ‰€é¨ä¹˜ä¹‹\2ã€‚'),
        ]
        
        for pattern, replacement in grammar_fixes:
            cleaned_text = re.sub(pattern, replacement, cleaned_text)
        
        # æ¸…ç†å¤šé¤˜çš„ç©ºæ ¼å’Œæ¨™é»
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
        cleaned_text = re.sub(r'ï¼Œã€‚', 'ã€‚', cleaned_text)
        cleaned_text = re.sub(r'ã€‚ã€‚', 'ã€‚', cleaned_text)
        
        return cleaned_text
    
    def _remove_conclusion_phrases(self, text: str) -> str:
        """ç§»é™¤æ–‡æœ¬ä¸­çš„çµè«–æ€§æ–‡å­—å’Œç¸½è¨ˆèªªæ˜"""
        import re
        
        # åˆ†è¡Œè™•ç†
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            
            # è·³éåŒ…å«çµè«–æ€§é—œéµè©çš„è¡Œï¼ˆä½†ä¸è¦èª¤åˆªç†ç”±èªªæ˜ï¼‰
            conclusion_keywords = [
                'ç¶œä¸Šæ‰€è¿°', 'ç¸½è¨ˆæ–°å°å¹£', 'åˆè¨ˆæ–°å°å¹£', 'æ³•å®šåˆ©æ¯', 
                'æŒ‰é€±å¹´åˆ©ç‡', 'æŒ‰å¹´æ¯', 'èµ·è¨´ç‹€ç¹•æœ¬é€é”',
                'æ¸…å„Ÿæ—¥æ­¢', 'è‡ª.*èµ·è‡³.*æ­¢', 'å¹´æ¯5%', 'ç¸½é¡ç‚º',
                'æ­¤æœ‰ç›¸é—œæ”¶æ“šå¯è­‰', 'æœ‰æ”¶æ“šç‚ºè­‰', 'æœ‰çµ±ä¸€ç™¼ç¥¨å¯è­‰',
                'ç¶“æŸ¥', 'æŸ¥æ˜', 'ç¶“å¯©ç†'
            ]
            
            should_skip = False
            for keyword in conclusion_keywords:
                if keyword in line:
                    should_skip = True
                    break
            
            if not should_skip and line:  # ä¿ç•™éç©ºä¸”éçµè«–æ€§çš„è¡Œ
                cleaned_lines.append(line)
        
        # é‡æ–°çµ„åˆ
        cleaned_text = '\n'.join(cleaned_lines)
        
        # é¡å¤–æ¸…ç†ï¼šç§»é™¤å¯èƒ½çš„çµè«–æ®µè½å’Œè­‰æ“šæ–‡å­—
        # ç§»é™¤å¾ã€Œç¶œä¸Šã€é–‹å§‹åˆ°çµå°¾çš„æ‰€æœ‰å…§å®¹
        cleaned_text = re.sub(r'ç¶œä¸Š.*$', '', cleaned_text, flags=re.MULTILINE | re.DOTALL)
        
        # ç§»é™¤å¥å­ä¸­çš„è­‰æ“šç›¸é—œæ–‡å­—
        evidence_patterns = [
            r'ï¼Œæ­¤æœ‰[^ã€‚]*å¯è­‰ã€‚?',
            r'ï¼Œæœ‰[^ã€‚]*æ”¶æ“š[^ã€‚]*è­‰ã€‚?',
            r'ï¼Œæœ‰[^ã€‚]*ç™¼ç¥¨[^ã€‚]*è­‰ã€‚?',
            r'ï¼Œ[^ã€‚]*ç‚ºè­‰ã€‚?'
        ]
        
        for pattern in evidence_patterns:
            cleaned_text = re.sub(pattern, 'ã€‚', cleaned_text)
        
        # æ¸…ç†å¤šé¤˜çš„å¥è™Ÿ
        cleaned_text = re.sub(r'ã€‚+', 'ã€‚', cleaned_text)
        
        return cleaned_text.strip()
    
    def _remove_defendant_damage_errors(self, text: str) -> str:
        """ç§»é™¤é—œæ–¼è¢«å‘Šæå®³çš„éŒ¯èª¤å…§å®¹"""
        import re
        
        # åˆ†è¡Œè™•ç†
        lines = text.split('\n')
        cleaned_lines = []
        skip_section = False
        
        for line in lines:
            line_stripped = line.strip()
            
            # æª¢æŸ¥æ˜¯å¦é–‹å§‹è¢«å‘Šæå®³æ®µè½
            if re.search(r'[ï¼ˆ(][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å][ï¼‰)].*è¢«å‘Š.*æå®³', line_stripped):
                print(f"ğŸ” æª¢æ¸¬åˆ°è¢«å‘Šæå®³éŒ¯èª¤æ®µè½ï¼Œé–‹å§‹è·³éï¼š{line_stripped}")
                skip_section = True
                continue
            
            # æª¢æŸ¥æ˜¯å¦é–‹å§‹æ–°çš„æ­£å¸¸æ®µè½ï¼ˆå¦‚ä¸‹ä¸€å€‹åŸå‘Šæˆ–å…¶ä»–æ®µè½ï¼‰
            if skip_section and (
                re.search(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]ã€', line_stripped) or  # æ–°çš„å¤§æ¨™é¡Œ
                re.search(r'[ï¼ˆ(][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å][ï¼‰)](?!.*è¢«å‘Š)', line_stripped) or  # æ–°çš„é …ç›®ä½†ä¸æ˜¯è¢«å‘Š
                line_stripped.startswith('å››ã€') or  # çµè«–æ®µè½
                line_stripped == ''
            ):
                skip_section = False
            
            # å¦‚æœä¸åœ¨è·³éæ¨¡å¼ä¸­ï¼Œä¿ç•™é€™è¡Œ
            if not skip_section:
                # é¡å¤–æª¢æŸ¥ï¼šç§»é™¤ä»»ä½•ç›´æ¥æåˆ°è¢«å‘Šæå®³çš„è¡Œ
                if ('è¢«å‘Š' in line_stripped and 'æå®³' in line_stripped and 
                    ('ä¹‹æå®³' in line_stripped or 'çš„æå®³' in line_stripped)):
                    print(f"ğŸ” ç§»é™¤è¢«å‘Šæå®³ç›¸é—œè¡Œï¼š{line_stripped}")
                    continue
                
                # ç§»é™¤è§£é‡‹è¢«å‘Šç‚ºä»€éº¼æ²’æœ‰æå®³çš„ç„¡ç”¨æ–‡å­—
                if ('æœªæåŠè¢«å‘Š' in line_stripped or 
                    'æ•…æ­¤éƒ¨åˆ†ç„¡æå®³é …ç›®' in line_stripped or
                    'ç”±æ–¼åŸå§‹æè¿°åƒ…æä¾›' in line_stripped):
                    print(f"ğŸ” ç§»é™¤ç„¡ç”¨è§£é‡‹æ–‡å­—ï¼š{line_stripped}")
                    continue
                
                cleaned_lines.append(line)
        
        # é‡æ–°çµ„åˆ
        cleaned_text = '\n'.join(cleaned_lines)
        
        # æ¸…ç†å¤šé¤˜çš„ç©ºè¡Œ
        cleaned_text = re.sub(r'\n\s*\n\s*\n', '\n\n', cleaned_text)
        
        return cleaned_text.strip()
    
    def _final_format_validation(self, text: str, is_single_case: bool) -> str:
        """æœ€çµ‚æ ¼å¼é©—è­‰å’Œä¿®æ­£"""
        import re
        
        if not is_single_case:
            return text  # å¤šåŸå‘Šæ¡ˆä»¶ä¸éœ€è¦ç‰¹æ®Šè™•ç†
        
        # å–®ä¸€åŸå‘Šæ¡ˆä»¶çš„ç‰¹æ®Šæ ¼å¼ä¿®æ­£
        lines = text.split('\n')
        corrected_lines = []
        
        for line in lines:
            line_stripped = line.strip()
            
            # æª¢æ¸¬ä¸¦ä¿®æ­£ã€Œï¼ˆä¸€ï¼‰åŸå‘Šä¹‹æå®³ï¼šã€æ ¼å¼
            if re.match(r'^[ï¼ˆ(][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å][ï¼‰)].*åŸå‘Š.*ä¹‹æå®³ï¼š?$', line_stripped):
                print(f"ğŸ” æª¢æ¸¬åˆ°éŒ¯èª¤æ ¼å¼ï¼Œç§»é™¤ï¼š{line_stripped}")
                continue  # è·³éé€™ç¨®éŒ¯èª¤æ ¼å¼çš„è¡Œ
            
            # æª¢æ¸¬ä¸¦ä¿®æ­£åµŒå¥—çš„æå®³é …ç›®æ ¼å¼
            # å¦‚ï¼šã€Œ1. é†«ç™‚è²»ç”¨ï¼š182,690å…ƒã€â†’ã€Œï¼ˆä¸€ï¼‰é†«ç™‚è²»ç”¨ï¼š182,690å…ƒã€
            if re.match(r'^\d+\.\s*([^ï¼š]+)ï¼š([0-9,]+å…ƒ)', line_stripped):
                match = re.match(r'^\d+\.\s*([^ï¼š]+)ï¼š([0-9,]+å…ƒ)', line_stripped)
                if match:
                    item_name = match.group(1).strip()
                    amount = match.group(2).strip()
                    # è½‰æ›ç‚ºæ­£ç¢ºçš„ä¸­æ–‡ç·¨è™Ÿæ ¼å¼
                    # ç°¡å–®çš„æ•¸å­—è½‰æ›ï¼ˆ1â†’ä¸€ï¼Œ2â†’äºŒç­‰ï¼‰
                    chinese_nums = ['', 'ä¸€', 'äºŒ', 'ä¸‰', 'å››', 'äº”', 'å…­', 'ä¸ƒ', 'å…«', 'ä¹', 'å']
                    item_num = re.match(r'^(\d+)', line_stripped).group(1)
                    try:
                        num = int(item_num)
                        if num <= 10:
                            chinese_num = chinese_nums[num]
                            corrected_line = f"ï¼ˆ{chinese_num}ï¼‰{item_name}ï¼š{amount}"
                            print(f"ğŸ” æ ¼å¼ä¿®æ­£ï¼š{line_stripped} â†’ {corrected_line}")
                            corrected_lines.append(corrected_line)
                            continue
                    except:
                        pass
            
            corrected_lines.append(line)
        
        # é‡æ–°çµ„åˆä¸¦æ¸…ç†
        corrected_text = '\n'.join(corrected_lines)
        
        # ç¢ºä¿æ ¼å¼ä¸€è‡´æ€§
        if not corrected_text.startswith("ä¸‰ã€æå®³é …ç›®ï¼š"):
            corrected_text = "ä¸‰ã€æå®³é …ç›®ï¼š\n" + corrected_text
        
        return corrected_text.strip()
    
    def _clean_evidence_language(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬ä¸­çš„è­‰æ“šèªè¨€"""
        import re
        
        # ç§»é™¤è­‰æ“šç›¸é—œæ–‡å­—
        evidence_patterns = [
            r'ï¼Œæ­¤æœ‰[^ã€‚]*å¯è­‰ã€‚?',
            r'ï¼Œæœ‰[^ã€‚]*æ”¶æ“š[^ã€‚]*è­‰ã€‚?',
            r'ï¼Œæœ‰[^ã€‚]*ç™¼ç¥¨[^ã€‚]*è­‰ã€‚?',
            r'ï¼Œæœ‰[^ã€‚]*è­‰æ˜[^ã€‚]*è­‰ã€‚?',
            r'ï¼Œ[^ã€‚]*ç‚ºè­‰ã€‚?',
            r'æ­¤æœ‰[^ã€‚]*å¯è­‰ã€‚?',
            r'æœ‰[^ã€‚]*æ”¶æ“š[^ã€‚]*è­‰ã€‚?',
            r'æœ‰[^ã€‚]*ç™¼ç¥¨[^ã€‚]*è­‰ã€‚?',
            r'æœ‰[^ã€‚]*è­‰æ˜[^ã€‚]*è­‰ã€‚?',
            r'[^ã€‚]*ç‚ºè­‰ã€‚?'
        ]
        
        cleaned_text = text
        for pattern in evidence_patterns:
            cleaned_text = re.sub(pattern, 'ã€‚', cleaned_text)
        
        # æ¸…ç†å¤šé¤˜çš„å¥è™Ÿå’Œç©ºæ ¼
        cleaned_text = re.sub(r'ã€‚+', 'ã€‚', cleaned_text)
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text)
        cleaned_text = cleaned_text.strip().rstrip('ã€‚')
        
        return cleaned_text

    def _ensure_reason_completeness(self, text: str, original_facts: str) -> str:
        """ç¢ºä¿æ¯å€‹æå®³é …ç›®éƒ½æœ‰ç†ç”±èªªæ˜"""
        import re
        
        lines = text.split('\n')
        enhanced_lines = []
        
        # è§£æåŸå§‹æè¿°ï¼Œå»ºç«‹é …ç›®åˆ°ç†ç”±çš„å°æ‡‰
        reason_map = {}
        original_lines = original_facts.split('\n')
        
        current_item = None
        current_reason = []
        
        for line in original_lines:
            line = line.strip()
            if not line:
                continue
            
            # æª¢æŸ¥æ˜¯å¦æ˜¯æ–°çš„é …ç›®é–‹å§‹
            item_match = re.match(r'^\d+\.\s*([^ï¼š]+)ï¼š', line)
            if item_match:
                # ä¿å­˜å‰ä¸€å€‹é …ç›®çš„ç†ç”±
                if current_item and current_reason:
                    reason_text = ' '.join(current_reason)
                    # æ¸…ç†è­‰æ“šèªè¨€
                    reason_text = self._clean_evidence_language(reason_text)
                    reason_map[current_item] = reason_text
                
                # é–‹å§‹æ–°é …ç›®
                current_item = item_match.group(1).strip()
                current_reason = []
                
                # æª¢æŸ¥åŒä¸€è¡Œæ˜¯å¦æœ‰ç†ç”±
                reason_part = line.split('ï¼š', 1)[1] if 'ï¼š' in line else ''
                if reason_part and not re.match(r'^\d+[,\d]*å…ƒ', reason_part.strip()):
                    current_reason.append(reason_part.strip())
            else:
                # é€™æ˜¯ç†ç”±èªªæ˜è¡Œ
                if current_item and line:
                    current_reason.append(line)
        
        # ä¿å­˜æœ€å¾Œä¸€å€‹é …ç›®
        if current_item and current_reason:
            reason_text = ' '.join(current_reason)
            # æ¸…ç†è­‰æ“šèªè¨€
            reason_text = self._clean_evidence_language(reason_text)
            reason_map[current_item] = reason_text
        
        # è™•ç†è¼¸å‡ºæ–‡æœ¬ï¼Œè£œå……ç¼ºå¤±çš„ç†ç”±
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            enhanced_lines.append(lines[i])
            
            # æª¢æŸ¥æ˜¯å¦æ˜¯æå®³é …ç›®è¡Œ
            item_match = re.match(r'^[ï¼ˆ(][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å][ï¼‰)]([^ï¼š]+)ï¼š([0-9,]+å…ƒ)', line)
            if item_match:
                item_name = item_match.group(1).strip()
                amount = item_match.group(2).strip()
                
                # æª¢æŸ¥ä¸‹ä¸€è¡Œæ˜¯å¦æœ‰ç†ç”±
                has_reason = False
                if i + 1 < len(lines):
                    next_line = lines[i + 1].strip()
                    if (next_line and 
                        not re.match(r'^[ï¼ˆ(][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å][ï¼‰)]', next_line) and
                        len(next_line) > 10):
                        has_reason = True
                
                # å¦‚æœæ²’æœ‰ç†ç”±ï¼Œå¾åŸå§‹æè¿°ä¸­æ‰¾ç†ç”±ä¸¦æ·»åŠ 
                if not has_reason:
                    reason = None
                    
                    # ç²¾ç¢ºåŒ¹é…é …ç›®åç¨±
                    for key, value in reason_map.items():
                        if item_name in key or key in item_name:
                            reason = value
                            break
                    
                    # å¦‚æœæ‰¾ä¸åˆ°ç²¾ç¢ºåŒ¹é…ï¼Œç”¨ç°¡æ½”çš„é€šç”¨ç†ç”±
                    if not reason:
                        if 'é†«ç™‚' in item_name:
                            reason = f"åŸå‘Šå› æœ¬æ¬¡äº‹æ•…å—å‚·ï¼Œæ”¯å‡º{item_name}{amount}ã€‚"
                        elif 'çœ‹è­·' in item_name:
                            reason = f"åŸå‘Šå› æœ¬æ¬¡äº‹æ•…å—æœ‰é‡å‚·ï¼Œéœ€å°ˆäººçœ‹è­·ï¼Œæ”¯å‡º{item_name}{amount}ã€‚"
                        elif 'äº¤é€š' in item_name:
                            reason = f"åŸå‘Šå› æœ¬æ¬¡äº‹æ•…å—å‚·å°±é†«æœŸé–“ï¼Œæ”¯å‡º{item_name}{amount}ã€‚"
                        elif 'å·¥ä½œ' in item_name or 'æå¤±' in item_name:
                            reason = f"åŸå‘Šå› æœ¬æ¬¡äº‹æ•…å—å‚·ç„¡æ³•å·¥ä½œï¼Œå—æœ‰{item_name}{amount}ã€‚"
                        elif 'æ…°æ’«' in item_name:
                            reason = f"åŸå‘Šå› æœ¬æ¬¡äº‹æ•…å—æœ‰å‚·å®³ï¼Œé€ æˆèº«å¿ƒç—›è‹¦ï¼Œè«‹æ±‚{item_name}{amount}ã€‚"
                        else:
                            reason = f"åŸå‘Šå› æœ¬æ¬¡äº‹æ•…æ”¯å‡º{item_name}{amount}ã€‚"
                    
                    if reason:
                        # æ·»åŠ ç†ç”±è¡Œ
                        enhanced_lines.append(reason)
                        print(f"ğŸ” è£œå……ç†ç”±ï¼š{item_name} -> {reason}")
            
            i += 1
        
        return '\n'.join(enhanced_lines)
    
    def _sort_laws_by_article_number(self, laws: List[str], law_descriptions: dict) -> List[tuple]:
        """æŒ‰æ¢è™Ÿæ•¸å­—å¤§å°æ’åºæ³•æ¢ä¸¦è¿”å›(æ¢è™Ÿ, å…§å®¹)å…ƒçµ„åˆ—è¡¨"""
        import re
        
        def extract_article_number(law: str) -> tuple:
            """æå–æ¢è™Ÿæ•¸å­—ç”¨æ–¼æ’åº"""
            # åŒ¹é…å¦‚ï¼šæ°‘æ³•ç¬¬184æ¢ç¬¬1é …å‰æ®µ
            match = re.search(r'ç¬¬(\d+)æ¢(?:ä¹‹(\d+))?', law)
            if match:
                main_num = int(match.group(1))
                sub_num = int(match.group(2)) if match.group(2) else 0
                return (main_num, sub_num)
            return (999, 0)  # ç„¡æ³•è§£æçš„æ”¾åˆ°æœ€å¾Œ
        
        # å‰µå»ºåŒ…å«æ¢è™Ÿã€å…§å®¹å’Œæ’åºéµçš„åˆ—è¡¨
        law_items = []
        for law in laws:
            if law in law_descriptions:
                content = f"ã€Œ{law_descriptions[law]}ã€"
                sort_key = extract_article_number(law)
                law_items.append((law, content, sort_key))
        
        # æŒ‰æ¢è™Ÿæ•¸å­—æ’åº
        law_items.sort(key=lambda x: x[2])
        
        # è¿”å›(æ¢è™Ÿ, å…§å®¹)å…ƒçµ„åˆ—è¡¨
        return [(item[0], item[1]) for item in law_items]
    
    def _detect_structure_type(self, text: str) -> str:
        """æª¢æ¸¬æ–‡æœ¬çµæ§‹é¡å‹"""
        # æª¢æŸ¥çµæ§‹åŒ–æ¨¡å¼
        structured_patterns = [
            r'\d+\.\s*[^ï¼š:]+[ï¼š:]\s*[0-9,]+å…ƒ',  # 1. é …ç›®ï¼šé‡‘é¡
            r'ï¼ˆ[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ï¼‰.*?[ï¼š:]\s*[0-9,]+å…ƒ',  # ï¼ˆä¸€ï¼‰é …ç›®ï¼šé‡‘é¡
            r'èªªæ˜ï¼š.*?[0-9,]+å…ƒ'  # èªªæ˜ï¼š...é‡‘é¡
        ]
        
        structured_matches = sum(1 for pattern in structured_patterns 
                               if re.search(pattern, text))
        
        if structured_matches >= 2:
            return "structured"
        elif "å…ƒ" in text:
            return "semi_structured"
        else:
            return "unstructured"
    
    def _generate_structured_compensation(self, comp_facts: str, parties: dict) -> str:
        """ä½¿ç”¨çµæ§‹åŒ–è™•ç†å™¨ç”Ÿæˆæå®³é …ç›®"""
        print("ğŸ—ï¸ ä½¿ç”¨çµæ§‹åŒ–è™•ç†å™¨åˆ†ææå®³é …ç›®...")
        
        # ä½¿ç”¨çµæ§‹åŒ–è™•ç†å™¨
        result = self.structured_processor.process_structured_document(comp_facts)
        
        # æª¢æŸ¥æ˜¯å¦æœ‰è¨ˆç®—éŒ¯èª¤
        validation = result.get('validation', {})
        if validation.get('claimed_total') and not validation.get('match', True):
            print(f"âš ï¸ ç™¼ç¾è¨ˆç®—éŒ¯èª¤ï¼šåŸèµ·è¨´ç‹€è²ç¨±{validation['claimed_total']:,}å…ƒï¼Œå¯¦éš›æ‡‰ç‚º{validation['calculated_total']:,}å…ƒ")
            print(f"ğŸ“Š å·®é¡ï¼š{abs(validation['difference']):,}å…ƒ")
        
        # ç”Ÿæˆä¿®æ­£å¾Œçš„æå®³é …ç›®
        structured_text = "ä¸‰ã€æå®³é …ç›®ï¼š\n\n"
        
        # æŒ‰åŸå‘Šåˆ†çµ„é¡¯ç¤º
        current_plaintiff = None
        plaintiff_index = 0
        item_counter = 1
        
        for item in result['structured_items']:
            # åˆ¤æ–·æ˜¯å¦ç‚ºæ–°çš„åŸå‘Š
            item_title = item['item_title']
            if 'åŸå‘Š' in item_title:
                # æå–åŸå‘Šå§“å
                plaintiff_match = re.search(r'åŸå‘Š([^ä¹‹çš„]+)', item_title)
                if plaintiff_match:
                    plaintiff_name = plaintiff_match.group(1).strip()
                    if plaintiff_name != current_plaintiff:
                        current_plaintiff = plaintiff_name
                        plaintiff_index += 1
                        chinese_num = self._chinese_num(plaintiff_index)
                        structured_text += f"ï¼ˆ{chinese_num}ï¼‰åŸå‘Š{current_plaintiff}ä¹‹æå®³ï¼š\n"
                        item_counter = 1
            
            # æ·»åŠ æå®³é …ç›®
            structured_text += f"{item_counter}. {item['item_title']}ï¼š{item['formatted_amount']}\n"
            if item['description']:
                structured_text += f"   {item['description']}\n"
            item_counter += 1
            structured_text += "\n"
        
        # æ·»åŠ æˆ‘èªªæ˜
        total_amount = result['calculation']['total']
        structured_text += f"\nğŸ’° æå®³ç¸½è¨ˆï¼šæ–°å°å¹£{total_amount:,}å…ƒæ•´\n"
        
        # å¦‚æœæœ‰è¨ˆç®—éŒ¯èª¤ï¼Œæ·»åŠ èªªæ˜
        if validation.get('claimed_total') and not validation.get('match', True):
            structured_text += f"\nï¼ˆè¨»ï¼šç¶“é‡æ–°è¨ˆç®—ï¼Œæ­£ç¢ºç¸½é¡ç‚º{total_amount:,}å…ƒï¼Œ"
            if validation['difference'] > 0:
                structured_text += f"åŸèµ·è¨´ç‹€å°‘ç®—{validation['difference']:,}å…ƒï¼‰"
            else:
                structured_text += f"åŸèµ·è¨´ç‹€å¤šç®—{abs(validation['difference']):,}å…ƒï¼‰"
        
        return structured_text
    
    def generate_cot_conclusion_with_smart_amount_calculation(self, accident_facts: str, compensation_text: str, parties: dict) -> str:
        """ä½¿ç”¨æ™ºèƒ½é‡‘é¡è¨ˆç®—ç”ŸæˆCoTçµè«–ï¼ˆé˜²æ­¢é‡è¤‡å’ŒéŒ¯èª¤è¨ˆç®—ï¼‰"""
        print("ğŸ§  ç”ŸæˆCoTçµè«–ï¼ˆå«ç¸½é‡‘é¡è¨ˆç®—ï¼‰...")
        
        # æå–æ‰€æœ‰é‡‘é¡ç”¨æ–¼è¨ˆç®—
        amounts = self._extract_valid_claim_amounts(compensation_text)
        total_amount = sum(amounts) if amounts else 0
        
        # æ§‹å»ºåŒ…å«é‡‘é¡è¨ˆç®—çš„CoTæç¤ºè©
        plaintiff = parties.get("åŸå‘Š", "åŸå‘Š")
        defendant = parties.get("è¢«å‘Š", "è¢«å‘Š")
        
        prompt = f"""ä½ æ˜¯å°ç£è³‡æ·±å¾‹å¸«ï¼Œè«‹é‹ç”¨Chain of Thoughtæ¨ç†æ–¹å¼ç”Ÿæˆå°ˆæ¥­çš„èµ·è¨´ç‹€çµè«–æ®µè½ã€‚

ğŸ‘¥ ç•¶äº‹äººè³‡è¨Šï¼š
åŸå‘Šï¼š{plaintiff}
è¢«å‘Šï¼š{defendant}

ğŸ“„ æ¡ˆä»¶äº‹å¯¦ï¼š
{accident_facts}

ğŸ“„ æå®³è³ å„Ÿå…§å®¹ï¼š
{compensation_text}

ğŸ’° æ™ºèƒ½é‡‘é¡åˆ†æçµæœï¼š
æå–åˆ°çš„æœ‰æ•ˆæ±‚å„Ÿé‡‘é¡ï¼š{amounts}
æ­£ç¢ºç¸½è¨ˆï¼š{total_amount:,}å…ƒ

ğŸ§  è«‹ä½¿ç”¨Chain of Thoughtæ–¹å¼åˆ†æï¼š

æ­¥é©Ÿ1: åˆ†ææ¡ˆä»¶æ€§è³ªå’Œç•¶äº‹äººè²¬ä»»
æ­¥é©Ÿ2: å¾æå®³è³ å„Ÿå…§å®¹ä¸­è­˜åˆ¥å„é …æå®³é …ç›®
æ­¥é©Ÿ3: é©—è­‰å„é …é‡‘é¡çš„åˆç†æ€§ï¼ˆä½¿ç”¨æ™ºèƒ½åˆ†æçµæœï¼‰
æ­¥é©Ÿ4: å½¢æˆç°¡æ½”ç²¾ç¢ºçš„çµè«–

ğŸ›ï¸ æœ€å¾Œè«‹ç”Ÿæˆå°ˆæ¥­çš„çµè«–æ®µè½ï¼Œæ ¼å¼è¦æ±‚ï¼š
- é–‹é ­ï¼šã€Œå››ã€çµè«–ï¼šã€
- æ ¼å¼ï¼šä¸€æ®µå¼é€£çºŒæ–‡å­—ï¼Œä¸è¦æ¢åˆ—å¼
- å…§å®¹ï¼šã€Œç¶œä¸Šæ‰€é™³ï¼Œè¢«å‘Šæ‡‰è³ å„ŸåŸå‘Šä¹‹æå®³ï¼ŒåŒ…å«[é …ç›®1][é‡‘é¡1]å…ƒã€[é …ç›®2][é‡‘é¡2]å…ƒ...ç­‰ï¼Œç¸½è¨ˆ{total_amount:,}å…ƒï¼Œä¸¦è‡ªèµ·è¨´ç‹€å‰¯æœ¬é€é”ç¿Œæ—¥èµ·è‡³æ¸…å„Ÿæ—¥æ­¢ï¼ŒæŒ‰å¹´æ¯5%è¨ˆç®—ä¹‹åˆ©æ¯ã€‚ã€
- âš ï¸ é‡è¦ï¼šé¿å…é‡è¤‡èªªæ˜åŒä¸€é …ç›®ï¼Œæ¯é …æå®³åªèªªæ˜ä¸€æ¬¡
- âš ï¸ é‡è¦ï¼šå¿…é ˆä½¿ç”¨æä¾›çš„æ­£ç¢ºç¸½è¨ˆ{total_amount:,}å…ƒ
- âš ï¸ é‡è¦ï¼šè«‹ç”¨ä¸€æ®µé€£çºŒæ–‡å­—ï¼Œä¸è¦åˆ†æ¢åˆ—å‡º

è«‹ç›´æ¥è¼¸å‡ºçµè«–æ®µè½ï¼š"""

        result = self.call_llm(prompt, timeout=180)
        
        return result if result else "å››ã€çµè«–ï¼š\nï¼ˆLLMç”Ÿæˆå¤±æ•—ï¼Œè«‹æª¢æŸ¥è¼¸å…¥å…§å®¹ï¼‰"

    def generate_cot_conclusion_with_structured_analysis(self, accident_facts: str, compensation_text: str, parties: dict) -> str:
        """ä½¿ç”¨æ™ºèƒ½é‡‘é¡è¨ˆç®—ç”ŸæˆCoTçµè«–ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰"""
        print("ğŸ§  ç”ŸæˆCoTçµè«–ï¼ˆä½¿ç”¨æ™ºèƒ½é‡‘é¡è¨ˆç®—ï¼‰...")
        
        # ç›´æ¥ä½¿ç”¨æ™ºèƒ½é‡‘é¡è¨ˆç®—æ–¹å¼ï¼Œé¿å…è¤‡é›œçš„çµæ§‹åŒ–è™•ç†
        return self.generate_cot_conclusion_with_smart_amount_calculation(
            accident_facts, compensation_text, parties
        )
    
    def _build_structured_cot_prompt(self, accident_facts: str, compensation_text: str, parties: dict, analysis_result: dict) -> str:
        """æ§‹å»ºåŸºæ–¼çµæ§‹åŒ–åˆ†æçš„CoTæç¤ºè©"""
        
        plaintiff = parties.get("åŸå‘Š", "åŸå‘Š")
        defendant = parties.get("è¢«å‘Š", "è¢«å‘Š")
        
        # æå–çµæ§‹åŒ–åˆ†æçµæœ
        structured_items = analysis_result.get('structured_items', [])
        calculation = analysis_result.get('calculation', {})
        validation = analysis_result.get('validation', {})
        
        # æ§‹å»ºé …ç›®æ‘˜è¦
        items_summary = "\nğŸ“‹ å·²è­˜åˆ¥çš„æå®³é …ç›®ï¼š"
        for item in structured_items:
            items_summary += f"\nâ€¢ {item['item_title']}: {item['formatted_amount']}"
        
        items_summary += f"\n\nğŸ’° è¨ˆç®—åˆ†æï¼š"
        items_summary += f"\nâ€¢ æ­£ç¢ºç¸½è¨ˆ: {calculation.get('total', 0):,}å…ƒ"
        
        if validation.get('claimed_total'):
            items_summary += f"\nâ€¢ åŸèµ·è¨´ç‹€è²ç¨±: {validation['claimed_total']:,}å…ƒ"
            if validation.get('difference', 0) != 0:
                if validation['difference'] < 0:
                    items_summary += f"\nâ€¢ âŒ åŸèµ·è¨´ç‹€å°‘ç®—äº†: {abs(validation['difference']):,}å…ƒ"
                else:
                    items_summary += f"\nâ€¢ âŒ åŸèµ·è¨´ç‹€å¤šç®—äº†: {validation['difference']:,}å…ƒ"
                items_summary += f"\nâ€¢ âœ… è«‹ä½¿ç”¨æ­£ç¢ºé‡‘é¡: {calculation.get('total', 0):,}å…ƒ"
        
        prompt = f"""ä½ æ˜¯å°ç£è³‡æ·±å¾‹å¸«ï¼Œè«‹é‹ç”¨Chain of Thoughtæ¨ç†æ–¹å¼ï¼Œæ ¹æ“šçµæ§‹åŒ–åˆ†æçµæœç”Ÿæˆå°ˆæ¥­çš„èµ·è¨´ç‹€çµè«–æ®µè½ã€‚

ğŸ¯ é‡è¦æŒ‡ç¤ºï¼š
1. å¿…é ˆä½¿ç”¨æ­£ç¢ºçš„é‡‘é¡è¨ˆç®—ï¼Œé¿å…é‡è¤‡è¨ˆç®—èªªæ˜æ–‡å­—ä¸­çš„é‡‘é¡
2. ä½¿ç”¨é€æ­¥æ¨ç†æ–¹å¼åˆ†ææå®³é …ç›®
3. çµè«–å¿…é ˆåŒ…å«å®Œæ•´çš„é …ç›®æ˜ç´°
4. ç¸½é‡‘é¡å¿…é ˆæº–ç¢ºç„¡èª¤
5. æ¡ç”¨æ¨™æº–çš„æ³•å¾‹æ–‡æ›¸æ ¼å¼

ğŸ‘¥ ç•¶äº‹äººè³‡è¨Šï¼š
åŸå‘Šï¼š{plaintiff}
è¢«å‘Šï¼š{defendant}

ğŸ“„ æ¡ˆä»¶äº‹å¯¦ï¼š
{accident_facts}

ğŸ“„ æå®³è³ å„ŸåŸå§‹å…§å®¹ï¼š
{compensation_text}

{items_summary}

ğŸ§  è«‹ä½¿ç”¨Chain of Thoughtæ–¹å¼åˆ†æï¼š

æ­¥é©Ÿ1: åˆ†ææ¡ˆä»¶æ€§è³ªå’Œç•¶äº‹äººè²¬ä»»
æ­¥é©Ÿ2: æª¢è¦–å„é …æå®³çš„åˆç†æ€§å’Œæ³•å¾‹ä¾æ“š
æ­¥é©Ÿ3: é©—è­‰é‡‘é¡è¨ˆç®—çš„æº–ç¢ºæ€§
æ­¥é©Ÿ4: ç¶œåˆåˆ†æä¸¦å½¢æˆçµè«–

ğŸ›ï¸ æœ€å¾Œè«‹ç”Ÿæˆå°ˆæ¥­çš„çµè«–æ®µè½ï¼ŒåŒ…æ‹¬ï¼š
1. æå®³é …ç›®æ˜ç´°åˆ—è¡¨
2. æ­£ç¢ºçš„ç¸½é‡‘é¡è¨ˆç®—  
3. æ¨™æº–çš„çµè«–æ ¼å¼
4. åˆ©æ¯è¨ˆç®—æ¢æ¬¾

æ ¼å¼è¦æ±‚ï¼š
- é–‹é ­ï¼šã€Œå››ã€çµè«–ï¼šã€
- ä¸­é–“ï¼šåˆ—èˆ‰å„é …æå®³æ˜ç´°
- çµå°¾ï¼šç¸½è¨ˆé‡‘é¡å’Œåˆ©æ¯è«‹æ±‚
- âš ï¸ é‡è¦ï¼šé¿å…é‡è¤‡èªªæ˜åŒä¸€é …ç›®ï¼Œæ¯é …æå®³åªèªªæ˜ä¸€æ¬¡
- âš ï¸ é‡è¦ï¼šä¸è¦é‡è¤‡åˆ—èˆ‰é‡‘é¡ï¼Œä½¿ç”¨çµæ§‹åŒ–åˆ†æçš„æ­£ç¢ºç¸½é¡

é‡è¦ï¼šè«‹ç¢ºä¿é‡‘é¡è¨ˆç®—çµ•å°æ­£ç¢ºï¼Œä½¿ç”¨çµæ§‹åŒ–åˆ†æçš„æ­£ç¢ºç¸½é¡ï¼"""

        return prompt
    
    def _build_traditional_cot_prompt(self, accident_facts: str, compensation_text: str, parties: dict) -> str:
        """æ§‹å»ºå‚³çµ±CoTæç¤ºè©"""
        
        plaintiff = parties.get("åŸå‘Š", "åŸå‘Š")
        defendant = parties.get("è¢«å‘Š", "è¢«å‘Š")
        
        prompt = f"""ä½ æ˜¯å°ç£è³‡æ·±å¾‹å¸«ï¼Œè«‹é‹ç”¨Chain of Thoughtæ¨ç†æ–¹å¼ç”Ÿæˆå°ˆæ¥­çš„èµ·è¨´ç‹€çµè«–æ®µè½ã€‚

ğŸ‘¥ ç•¶äº‹äººè³‡è¨Šï¼š
åŸå‘Šï¼š{plaintiff}
è¢«å‘Šï¼š{defendant}

ğŸ“„ æ¡ˆä»¶äº‹å¯¦ï¼š
{accident_facts}

ğŸ“„ æå®³è³ å„Ÿå…§å®¹ï¼š
{compensation_text}

ğŸ§  è«‹ä½¿ç”¨Chain of Thoughtæ–¹å¼åˆ†æï¼š

æ­¥é©Ÿ1: åˆ†ææ¡ˆä»¶æ€§è³ªå’Œç•¶äº‹äººè²¬ä»»
æ­¥é©Ÿ2: æª¢è¦–å„é …æå®³çš„åˆç†æ€§å’Œæ³•å¾‹ä¾æ“š  
æ­¥é©Ÿ3: è¨ˆç®—ç¸½æå®³é‡‘é¡
æ­¥é©Ÿ4: ç¶œåˆåˆ†æä¸¦å½¢æˆçµè«–

ğŸ›ï¸ æœ€å¾Œè«‹ç”Ÿæˆå°ˆæ¥­çš„çµè«–æ®µè½ï¼Œæ ¼å¼è¦æ±‚ï¼š
- é–‹é ­ï¼šã€Œå››ã€çµè«–ï¼šã€
- ä¸­é–“ï¼šåˆ—èˆ‰å„é …æå®³æ˜ç´°
- çµå°¾ï¼šç¸½è¨ˆé‡‘é¡å’Œåˆ©æ¯è«‹æ±‚
- âš ï¸ é‡è¦ï¼šé¿å…é‡è¤‡èªªæ˜åŒä¸€é …ç›®ï¼Œæ¯é …æå®³åªèªªæ˜ä¸€æ¬¡
- âš ï¸ é‡è¦ï¼šä¸è¦é‡è¤‡åˆ—èˆ‰ç›¸åŒçš„é‡‘é¡å’Œé …ç›®"""

        return prompt
    
    def _post_process_structured_conclusion(self, conclusion: str, analysis_result: dict) -> str:
        """å¾Œè™•ç†çµæ§‹åŒ–çµè«–"""
        
        # æ·»åŠ è™•ç†è³‡è¨Š
        processing_info = f"\n\nğŸ’¡ è™•ç†è³‡è¨Šï¼š\n"
        processing_info += f"è™•ç†æ–¹æ³•ï¼šçµæ§‹åŒ–åˆ†æ\n"
        
        calculation = analysis_result.get('calculation', {})
        validation = analysis_result.get('validation', {})
        
        processing_info += f"æ­£ç¢ºç¸½é¡ï¼š{calculation.get('total', 0):,}å…ƒ\n"
        if validation.get('claimed_total'):
            processing_info += f"åŸè²ç¨±é¡ï¼š{validation['claimed_total']:,}å…ƒ\n"
            if validation.get('difference', 0) != 0:
                processing_info += f"å·®é¡ä¿®æ­£ï¼š{abs(validation['difference']):,}å…ƒ\n"
        
        return conclusion + processing_info

    def _generate_complex_compensation(self, comp_facts: str, parties: dict) -> str:
        """è™•ç†è¤‡é›œæå®³é …ç›®æ–‡æœ¬çš„åˆ†æ­¥æ–¹æ³•"""
        print("ğŸ”§ ä½¿ç”¨åˆ†æ­¥è™•ç†è¤‡é›œæå®³æ–‡æœ¬...")
        
        # æ­¥é©Ÿ1ï¼šé è™•ç†ä¸­æ–‡æ•¸å­—
        preprocessed_facts = self._preprocess_chinese_numbers(comp_facts)
        
        # ç›´æ¥æ ¼å¼åŒ–ç‚ºæ¨™æº–æå®³é …ç›®
        format_prompt = f"""è«‹å°‡ä»¥ä¸‹æå®³è³ å„Ÿå…§å®¹é‡æ–°æ•´ç†ç‚ºæ¨™æº–çš„æ³•å¾‹æ–‡æ›¸æ ¼å¼ï¼š

ã€ç•¶äº‹äººã€‘
åŸå‘Šï¼š{parties.get('åŸå‘Š', 'æœªæåŠ')}ï¼ˆå…±{parties.get('åŸå‘Šæ•¸é‡', 1)}åï¼‰

ã€æå®³æè¿°ã€‘
{preprocessed_facts}

ã€æ¨™æº–æ ¼å¼è¦æ±‚ã€‘
ä¸‰ã€æå®³é …ç›®ï¼š

ï¼ˆä¸€ï¼‰åŸå‘Š[å§“å]ä¹‹æå®³ï¼š
1. [é …ç›®åç¨±]ï¼š[é‡‘é¡]å…ƒ
   èªªæ˜ï¼šåŸå‘Š[å§“å]å› æœ¬æ¬¡è»Šç¦[æå®³æ€§è³ª]
2. [é …ç›®åç¨±]ï¼š[é‡‘é¡]å…ƒ
   èªªæ˜ï¼šåŸå‘Š[å§“å]å› æœ¬æ¬¡è»Šç¦[æå®³æ€§è³ª]

ï¼ˆäºŒï¼‰åŸå‘Š[å§“å]ä¹‹æå®³ï¼š
1. [é …ç›®åç¨±]ï¼š[é‡‘é¡]å…ƒ
   èªªæ˜ï¼šåŸå‘Š[å§“å]å› æœ¬æ¬¡è»Šç¦[æå®³æ€§è³ª]

ã€é‡è¦è¦æ±‚ã€‘
- ç›´æ¥æ•´ç†æå®³é …ç›®ï¼Œä¸è¦é¡¯ç¤ºåˆ†ææˆ–è¨ˆç®—éç¨‹
- å…±åŒè²»ç”¨è¦å¹³å‡åˆ†æ”¤çµ¦ç›¸é—œåŸå‘Š
- æ‰€æœ‰é‡‘é¡ä½¿ç”¨åƒåˆ†ä½é€—è™Ÿæ ¼å¼
- æ¯é …æå®³éƒ½è¦æœ‰å…·é«”èªªæ˜
- ç¢ºä¿æ ¼å¼æ•´é½Šçµ±ä¸€

è«‹ç›´æ¥è¼¸å‡ºæ¨™æº–æ ¼å¼çš„æå®³é …ç›®ï¼š"""

        return self.call_llm(format_prompt, timeout=120)

    def _verify_calculation(self, result_text: str) -> dict:
        """å¾çµæœä¸­æå–ä¸¦é©—è­‰è¨ˆç®—æº–ç¢ºæ€§"""
        import re
        
        verification = {
            "correct": True,
            "errors": [],
            "corrected_total": None
        }
        
        # æå–æ‰€æœ‰é‡‘é¡æ•¸å­—
        amounts = re.findall(r'(\d{1,3}(?:,\d{3})*)', result_text)
        amounts = [int(amt.replace(',', '')) for amt in amounts if amt]
        
        if len(amounts) >= 10:  # å¦‚æœæœ‰è¶³å¤ çš„é‡‘é¡é€²è¡Œé©—è­‰
            # å˜—è©¦æ‰¾åˆ°å…©å€‹å°è¨ˆå’Œç¸½è¨ˆ
            try:
                # å‡è¨­æœ€å¾Œä¸‰å€‹å¤§æ•¸å­—æ˜¯ï¼šå°è¨ˆ1ã€å°è¨ˆ2ã€ç¸½è¨ˆ
                if len(amounts) >= 3:
                    subtotal1 = amounts[-3]
                    subtotal2 = amounts[-2] 
                    reported_total = amounts[-1]
                    
                    # é©—è­‰ç¸½è¨ˆ
                    actual_total = subtotal1 + subtotal2
                    if actual_total != reported_total:
                        verification["correct"] = False
                        verification["errors"].append(f"ç¸½è¨ˆéŒ¯èª¤ï¼š{subtotal1} + {subtotal2} = {actual_total}ï¼Œä½†å ±å‘Šç‚º{reported_total}")
                        verification["corrected_total"] = actual_total
                        
            except Exception as e:
                verification["errors"].append(f"é©—è­‰éç¨‹å‡ºéŒ¯ï¼š{e}")
        
        return verification

    def _generate_llm_based_compensation(self, comp_facts: str, parties: dict) -> str:
        """ä½¿ç”¨LLMå®Œå…¨è™•ç†æå®³é …ç›®ç”Ÿæˆ - æ”¯æŒæ ¼å¼è‡ªé©æ‡‰"""
        
        # å…ˆé è™•ç†ä¸­æ–‡æ•¸å­—
        preprocessed_facts = self._preprocess_chinese_numbers(comp_facts)
        
        # æª¢æ¸¬è¼¸å…¥æ ¼å¼é¡å‹
        if self.format_handler:
            format_info = self.format_handler.detect_format(preprocessed_facts)
            detected_format = format_info.get('primary_format')
            confidence = format_info.get('confidence', 0.0)
            print(f"ğŸ” ã€æ ¼å¼æª¢æ¸¬ã€‘æª¢æ¸¬åˆ°æ ¼å¼: {detected_format} (ç½®ä¿¡åº¦: {confidence:.2f})")
        else:
            detected_format = None
            confidence = 0.0
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºå–®ä¸€åŸå‘Šæƒ…æ³ï¼ˆè¢«å‘Šæ•¸é‡ä¸å½±éŸ¿æ ¼å¼é¸æ“‡ï¼‰
        plaintiff_count = parties.get('åŸå‘Šæ•¸é‡', 1)
        defendant_count = parties.get('è¢«å‘Šæ•¸é‡', 1)
        is_single_case = plaintiff_count == 1  # åªçœ‹åŸå‘Šæ•¸é‡
        
        # é™¤éŒ¯è¼¸å‡º
        print(f"ğŸ” DEBUG: parties = {parties}")
        print(f"ğŸ” DEBUG: plaintiff_count = {plaintiff_count}, defendant_count = {defendant_count}")
        print(f"ğŸ” DEBUG: is_single_case = {is_single_case}")
        
        # æ ¹æ“šæ ¼å¼æª¢æ¸¬çµæœé¸æ“‡åˆé©çš„æç¤ºè©ç­–ç•¥
        if detected_format == 'multi_plaintiff_narrative' or (format_info.get('is_multi_plaintiff', False) and plaintiff_count > 1):
            # å¤šåŸå‘Šæ¡ˆä¾‹ - ä½¿ç”¨å¤šåŸå‘Šå°ˆé–€è™•ç†
            prompt = self._build_adaptive_prompt(preprocessed_facts, parties, 'multi_plaintiff_narrative')
        elif detected_format == 'free_format' or confidence < 0.5:
            # è‡ªç”±æ ¼å¼æˆ–æ ¼å¼ä¸æ˜ç¢º - ä½¿ç”¨é©æ‡‰æ€§æç¤ºè©
            prompt = self._build_adaptive_prompt(preprocessed_facts, parties, detected_format)
        elif is_single_case:
            # å–®ä¸€åŸå‘Šæ™‚ï¼Œä½¿ç”¨ç°¡åŒ–çš„ä¸­æ–‡ç·¨è™Ÿæ ¼å¼ï¼ˆä¸è«–è¢«å‘Šæ•¸é‡ï¼‰
            prompt = f"""è«‹å°‡ä»¥ä¸‹æå®³å…§å®¹æ•´ç†æˆæå®³é …ç›®æ¸…å–®ï¼Œç†ç”±ç°¡æ½”æ˜ç­ã€‚

ã€åŸå§‹å…§å®¹ã€‘
{preprocessed_facts}

ã€è¼¸å‡ºæ ¼å¼ã€‘
ï¼ˆä¸€ï¼‰é …ç›®åç¨±ï¼šé‡‘é¡å…ƒ
åŸå‘Šå› æœ¬æ¬¡äº‹æ•…[ç°¡æ½”ç†ç”±]ï¼Œæ”¯å‡º/å—æœ‰[é …ç›®åç¨±][é‡‘é¡]å…ƒã€‚

ã€ç¯„ä¾‹ã€‘
ï¼ˆä¸€ï¼‰é†«ç™‚è²»ç”¨ï¼š182,690å…ƒ
åŸå‘Šå› æœ¬æ¬¡äº‹æ•…å—å‚·ï¼Œæ–¼å—é–€é†«é™¢ã€é›²æ—åŸºç£æ•™é†«é™¢å°±é†«æ²»ç™‚ï¼Œæ”¯å‡ºé†«ç™‚è²»ç”¨å…±è¨ˆ182,690å…ƒã€‚

ï¼ˆäºŒï¼‰çœ‹è­·è²»ç”¨ï¼š246,000å…ƒ
åŸå‘Šå› æœ¬æ¬¡äº‹æ•…å—æœ‰é‡å‚·ï¼Œéœ€å°ˆäººçœ‹è­·ï¼Œæ”¯å‡ºçœ‹è­·è²»ç”¨246,000å…ƒã€‚

ï¼ˆä¸‰ï¼‰å·¥ä½œæå¤±ï¼š485,000å…ƒ
åŸå‘Šæ–¼è»Šç¦ç™¼ç”Ÿæ™‚ä»»è·æ–¼å‡±æ’’å¤§é£¯åº—æˆ¿å‹™éƒ¨é ˜ç­ï¼Œå› æœ¬æ¬¡äº‹æ•…å—å‚·éœ€ä¼‘é¤Šä¸€å¹´ï¼Œè‡´ç„¡æ³•å·¥ä½œï¼Œå—æœ‰å·¥ä½œæå¤±485,000å…ƒã€‚

ã€è¦æ±‚ã€‘
- ç†ç”±1-2å¥è©±ï¼Œç°¡æ½”æ˜ç­
- ä¿ç•™é‡è¦é†«é™¢åç¨±ã€å·¥ä½œå–®ä½ç­‰é—œéµè³‡è¨Š
- çµ±ä¸€ä½¿ç”¨"æœ¬æ¬¡äº‹æ•…"è€Œé"æœ¬ä»¶è»Šç¦"
- å·¥ä½œæå¤±é …ç›®åç¨±ç›´æ¥ç”¨"å·¥ä½œæå¤±"
- ç²¾ç¥æ…°æ’«é‡‘é …ç›®åç¨±ç›´æ¥ç”¨"æ…°æ’«é‡‘"
- ä¸è¦éåº¦è§£é‡‹æˆ–åˆ†æ

è«‹è¼¸å‡ºï¼š"""
        else:
            # å¤šåŸå‘Šæ™‚ï¼Œä½¿ç”¨å®Œæ•´æ ¼å¼ï¼Œæ¯ä½åŸå‘Šåˆ†åˆ¥åˆ—å‡ºæå®³
            prompt = f"""ä½ æ˜¯å°ç£å¾‹å¸«ï¼Œè«‹æ ¹æ“šè»Šç¦æ¡ˆä»¶çš„æå®³è³ å„Ÿå…§å®¹ï¼Œåˆ†æä¸¦é‡æ–°æ•´ç†æˆæ¨™æº–çš„èµ·è¨´ç‹€æå®³é …ç›®æ ¼å¼ï¼š

ã€ç•¶äº‹äººè³‡è¨Šã€‘
åŸå‘Šï¼š{parties.get('åŸå‘Š', 'æœªæåŠ')}ï¼ˆå…±{parties.get('åŸå‘Šæ•¸é‡', 1)}åï¼‰
è¢«å‘Šï¼š{parties.get('è¢«å‘Š', 'æœªæåŠ')}ï¼ˆå…±{parties.get('è¢«å‘Šæ•¸é‡', 1)}åï¼‰

ã€åŸå§‹æå®³æè¿°ã€‘
{preprocessed_facts}

ã€åˆ†æè¦æ±‚ã€‘
è«‹ä»”ç´°åˆ†æä¸Šè¿°å…§å®¹ï¼Œå¾ä¸­æå–å‡ºï¼š
1. æ¯ä½åŸå‘Šçš„å…·é«”æå®³é …ç›®é¡å‹å’Œç¢ºåˆ‡é‡‘é¡
2. **æ¯é …æå®³çš„è©³ç´°äº‹å¯¦æ ¹æ“šå’Œæ³•å¾‹ç†ç”±**ï¼šé€™æ˜¯æœ€é‡è¦çš„éƒ¨åˆ†ï¼
3. **é‡è¦**ï¼šåªèƒ½ä½¿ç”¨åŸå§‹æè¿°ä¸­å·²æåŠçš„äº‹å¯¦ï¼Œçµ•å°ä¸å¯ä»¥è‡ªè¡Œæ·»åŠ æˆ–ç·¨é€ ä»»ä½•å…§å®¹
4. **å°±é†«ç´€éŒ„æ•´åˆ**ï¼šå¦‚æœåŸå§‹æè¿°ä¸­æœ‰å…·é«”çš„é†«é™¢åç¨±ï¼Œè«‹å°‡é€™äº›å°±é†«ç´€éŒ„æ•´åˆåˆ°é†«ç™‚ç›¸é—œæå®³é …ç›®çš„ç†ç”±èªªæ˜ä¸­
5. **ç†ç”±å®Œæ•´æ€§**ï¼šæ¯å€‹æå®³é …ç›®éƒ½å¿…é ˆæœ‰å®Œæ•´çš„ç†ç”±èªªæ˜ï¼ŒåŒ…æ‹¬ï¼š
   - æå®³ç™¼ç”Ÿçš„åŸå› ï¼ˆå› æœ¬æ¬¡è»Šç¦...ï¼‰
   - å…·é«”çš„äº‹å¯¦ä¾æ“šï¼ˆæ”¯å‡ºè²»ç”¨ã€å—å‚·æƒ…æ³ã€å½±éŸ¿ç­‰ï¼‰
   - ç›¸é—œçš„è©³ç´°èªªæ˜ï¼ˆå°±é†«æƒ…æ³ã€å·¥ä½œå½±éŸ¿ã€ç”Ÿæ´»å½±éŸ¿ç­‰ï¼‰

ã€æ¨™æº–è¼¸å‡ºæ ¼å¼ã€‘
ä¸‰ã€æå®³é …ç›®ï¼š

ï¼ˆä¸€ï¼‰åŸå‘Šå³éº—å¨Ÿä¹‹æå®³ï¼š
1. é†«ç™‚è²»ç”¨ï¼š6,720å…ƒ
åŸå‘Šå³éº—å¨Ÿå› æœ¬æ¬¡è»Šç¦å—å‚·ï¼Œæ–¼è‡ºåŒ—æ¦®æ°‘ç¸½é†«é™¢ã€é¦¬å•ç´€å¿µé†«é™¢ã€å…§æ¹–èè‹±è¨ºæ‰€åŠä¸­é†«è¨ºæ‰€å°±é†«æ²»ç™‚ï¼Œæ”¯å‡ºé†«ç™‚è²»ç”¨è¨ˆ6,720å…ƒã€‚

2. æœªä¾†æ‰‹è¡“è²»ç”¨ï¼š264,379å…ƒ
åŸå‘Šå³éº—å¨Ÿå› æœ¬æ¬¡è»Šç¦ç¶“æ¦®æ°‘ç¸½é†«é™¢ç¢ºè¨ºç™¼ç”Ÿè…°æ¤ç¬¬ä¸€ã€äºŒç¯€è„Šæ¤æ»‘è„«ï¼Œé è¨ˆæœªä¾†æ‰‹è¡“è²»ç”¨ç‚º264,379å…ƒã€‚

3. çœ‹è­·è²»ç”¨ï¼š152,500å…ƒ
åŸå‘Šå³éº—å¨Ÿå› æœ¬æ¬¡è»Šç¦èº«é«”å—çŒ›çƒˆæ’æ“Šéœ‡ç›ªï¼Œé¤Šå‚·æœŸé–“ç„¡ç”Ÿæ´»è‡ªä¸»èƒ½åŠ›ï¼Œè‡ª107å¹´7æœˆ24æ—¥èµ·è‡³107å¹´11æœˆ23æ—¥æ­¢ï¼Œå¹³å‡åˆ†æ”¤çœ‹è­·è²»ç”¨å…±è¨ˆ305,000å…ƒä¹‹åŠæ•¸ã€‚

4. æ…°æ’«é‡‘ï¼š200,000å…ƒ
åŸå‘Šå³éº—å¨Ÿå› æœ¬æ¬¡è»Šç¦é™¤å—å¤–å‚·å¤–ï¼Œå°šå› å—æ’æ“Šæ‹‰æ‰¯ï¼Œé ˆé•·æœŸæ²»ç™‚åŠå¾©å¥ï¼Œä¸”æœªä¾†å°šé ˆè² æ“”æ²‰é‡æ‰‹è¡“è²»ç”¨ï¼Œæ•…è«‹æ±‚æ…°æ’«é‡‘200,000å…ƒã€‚

ï¼ˆäºŒï¼‰åŸå‘Šé™³ç¢§ç¿”ä¹‹æå®³ï¼š
1. é†«ç™‚è²»ç”¨ï¼š12,180å…ƒ
åŸå‘Šé™³ç¢§ç¿”å› æœ¬æ¬¡è»Šç¦å—å‚·ï¼Œæ–¼è‡ºåŒ—æ¦®æ°‘ç¸½é†«é™¢ã€é¦¬å•ç´€å¿µé†«é™¢åŠä¸­é†«è¨ºæ‰€å°±é†«æ²»ç™‚ï¼Œæ”¯å‡ºé†«ç™‚è²»ç”¨è¨ˆ12,180å…ƒã€‚

2. å‡ç‰™è£ç½®è²»ç”¨ï¼š24,000å…ƒ
åŸå‘Šé™³ç¢§ç¿”å› æœ¬æ¬¡è»Šç¦é ­éƒ¨å³å´é­å—é‡æ“Šï¼Œå‡ç‰™è„«è½ï¼Œéœ€é‡æ–°å®‰è£å‡ç‰™è£ç½®ï¼Œè²»ç”¨ç‚º24,000å…ƒã€‚

ã€é—œéµè¦æ±‚ã€‘
- æ¯ä½åŸå‘Šå…ˆç”¨ï¼ˆä¸€ï¼‰ï¼ˆäºŒï¼‰ç­‰ç·¨è™Ÿå€åˆ†
- æ¯ä½åŸå‘Šå…§éƒ¨çš„æå®³é …ç›®ä½¿ç”¨ 1. 2. 3. ç­‰æ•¸å­—ç·¨è™Ÿ
- **æ¯é …å¿…é ˆæ ¼å¼**ï¼šæ•¸å­—ç·¨è™Ÿ. é …ç›®åç¨±ï¼šé‡‘é¡ + è©³ç´°å®Œæ•´çš„æ³•å¾‹ç†ç”±èªªæ˜
- **ç†ç”±å®Œæ•´æ€§è¦æ±‚**ï¼šæ¯å€‹æå®³é …ç›®éƒ½å¿…é ˆåŒ…å«å®Œæ•´çš„ç†ç”±èªªæ˜ï¼Œä¸å¯åªæœ‰é‡‘é¡
- **ç†ç”±å…§å®¹è¦æ±‚**ï¼š
  - æå®³ç™¼ç”Ÿçš„åŸå› ï¼ˆå¦‚ï¼šå› æœ¬æ¬¡è»Šç¦å—å‚·...ï¼‰
  - å…·é«”çš„äº‹å¯¦ä¾æ“šï¼ˆå¦‚ï¼šæ”¯å‡ºè²»ç”¨ã€å—å‚·æƒ…æ³ã€å·¥ä½œå½±éŸ¿ç­‰ï¼‰
  - ç›¸é—œçš„è©³ç´°èªªæ˜ï¼ˆå¦‚ï¼šå°±é†«æƒ…æ³ã€æ²»ç™‚éœ€è¦ã€ç”Ÿæ´»å½±éŸ¿ç­‰ï¼‰
- ç†ç”±èªªæ˜å¿…é ˆåŸºæ–¼åŸå§‹æè¿°ä¸­çš„å…·é«”äº‹å¯¦ï¼Œå……åˆ†æå–åŸå§‹æè¿°çš„è©³ç´°è³‡è¨Š
- ä¸å¯è‡ªè¡Œç·¨é€ ä»»ä½•é†«ç™‚è¨ºæ–·ã€å‚·å‹¢æè¿°æˆ–å…¶ä»–ç´°ç¯€
- **å°±é†«ç´€éŒ„è™•ç†**ï¼šå¦‚æœåŸå§‹æè¿°æåŠå…·é«”é†«é™¢åç¨±ï¼Œè«‹åœ¨é†«ç™‚ç›¸é—œé …ç›®ä¸­åŠ å…¥ã€Œæ–¼[é†«é™¢åç¨±]å°±é†«æ²»ç™‚ã€
- ç†ç”±è¦æ¡ç”¨æ­£å¼çš„æ³•å¾‹æ–‡æ›¸èªè¨€
- ä½¿ç”¨åƒåˆ†ä½é€—è™Ÿæ ¼å¼é¡¯ç¤ºé‡‘é¡

ã€åš´æ ¼ç¦æ­¢äº‹é …ã€‘
- çµ•å°ä¸å¯åœ¨è¼¸å‡ºä¸­åŒ…å«ã€Œç¶œä¸Šæ‰€è¿°ã€ã€ã€Œç¸½è¨ˆã€ã€ã€Œåˆè¨ˆã€ã€ã€Œå…±è¨ˆã€ç­‰çµè«–æ€§æ–‡å­—
- ä¸è¦åŒ…å«ä»»ä½•ç¸½é‡‘é¡è¨ˆç®—æˆ–åŒ¯ç¸½èªªæ˜
- ä¸è¦åŒ…å«ä»»ä½•æ³•å®šåˆ©æ¯çš„èªªæ˜
- ä¸è¦åŒ…å«ä»»ä½•çµè«–æ®µè½æˆ–ç¸½çµæ–‡å­—
- ä¸è¦åŒ…å«è­‰æ“šç›¸é—œæ–‡å­—ï¼šã€Œæ­¤æœ‰ç›¸é—œæ”¶æ“šå¯è­‰ã€ã€ã€Œæœ‰æ”¶æ“šç‚ºè­‰ã€ã€ã€Œæœ‰çµ±ä¸€ç™¼ç¥¨å¯è­‰ã€ã€ã€Œå¯è­‰ã€ç­‰
- ä¸è¦åŒ…å«åˆ¤æ±ºæ›¸ç”¨èªï¼šã€Œç¶“æŸ¥ã€ã€ã€ŒæŸ¥æ˜ã€ã€ã€Œç¶“å¯©ç†ã€ç­‰
- **çµ•å°ç¦æ­¢è¢«å‘Šæå®³**ï¼šä¸å¯ä»¥åŒ…å«ä»»ä½•é—œæ–¼è¢«å‘Šæå®³çš„å…§å®¹ï¼Œè¢«å‘Šæ˜¯è³ å„Ÿç¾©å‹™äººä¸æœƒæœ‰æå®³
- **çµ•å°ç¦æ­¢**ï¼šä¸å¯ä»¥å‡ºç¾ã€Œï¼ˆäºŒï¼‰è¢«å‘Š...ä¹‹æå®³ã€æˆ–ä»»ä½•è¢«å‘Šæå®³çš„æè¿°
- åªè¼¸å‡ºç´”ç²¹çš„æå®³é …ç›®æ¢åˆ—ï¼Œæ¯é …åŒ…å«ç·¨è™Ÿã€åç¨±ã€é‡‘é¡ã€ç†ç”±èªªæ˜
- **æ˜ç¢ºåŸå‰‡**ï¼šèµ·è¨´ç‹€ä¸­åªèƒ½æœ‰åŸå‘Šçš„æå®³ï¼Œçµ•å°ä¸èƒ½å‰µé€ è™›å‡çš„è¢«å‘Šæå®³é …ç›®

è«‹åš´æ ¼æŒ‰ç…§ä¸Šè¿°æ ¼å¼å’Œè¦æ±‚ï¼ŒåŸºæ–¼åŸå§‹æè¿°çš„äº‹å¯¦åˆ†æä¸¦è¼¸å‡ºæå®³é …ç›®ï¼š"""

        result = self.call_llm(prompt, timeout=120)
        
        # æ¸…ç†çµè«–æ€§æ–‡å­—
        result = self._remove_conclusion_phrases(result)
        
        # ç§»é™¤ä»»ä½•é—œæ–¼è¢«å‘Šæå®³çš„éŒ¯èª¤å…§å®¹
        result = self._remove_defendant_damage_errors(result)
        
        # æª¢æŸ¥ä¸¦è£œå……ç¼ºå¤±çš„ç†ç”±
        result = self._ensure_reason_completeness(result, preprocessed_facts)
        
        # æœ€çµ‚æ¸…ç†è­‰æ“šèªè¨€ï¼ˆä½†ä¿ç•™ç†ç”±èªªæ˜ï¼‰
        result = self._remove_conclusion_phrases(result)
        
        # æœ€çµ‚æ ¼å¼é©—è­‰å’Œä¿®æ­£ï¼ˆä½†ä¸è¦ç ´å£ç†ç”±ï¼‰
        # result = self._final_format_validation(result, is_single_case)
        
        # æª¢æŸ¥çµæœæ˜¯å¦åŒ…å«é æœŸæ ¼å¼
        if "ï¼ˆä¸€ï¼‰" in result:
            # ç§»é™¤å¯èƒ½çš„æ¨™é¡Œï¼Œåªè¿”å›æå®³é …ç›®æ¸…å–®
            if result.startswith("ä¸‰ã€æå®³é …ç›®ï¼š"):
                result = result.replace("ä¸‰ã€æå®³é …ç›®ï¼š", "").strip()
            return result
        else:
            # Fallbackï¼šè¿”å›åŸºæœ¬æ ¼å¼
            return comp_facts

    def _build_adaptive_prompt(self, preprocessed_facts: str, parties: dict, detected_format: str) -> str:
        """æ ¹æ“šæª¢æ¸¬åˆ°çš„æ ¼å¼æ§‹å»ºé©æ‡‰æ€§æç¤ºè©"""
        plaintiff = parties.get("åŸå‘Š", "åŸå‘Š")
        plaintiff_count = parties.get('åŸå‘Šæ•¸é‡', 1)
        
        if detected_format == 'multi_plaintiff_narrative':
            # å¤šåŸå‘Šæ•˜è¿°æ ¼å¼ - å°ˆé–€è™•ç†å¤šååŸå‘Šå„è‡ªçš„æå®³
            return f"""ä½ æ˜¯å°ç£è³‡æ·±å¾‹å¸«ï¼Œè«‹å¾ä»¥ä¸‹å¤šåŸå‘Šæå®³æè¿°ä¸­åˆ†åˆ¥æå–æ¯ä½åŸå‘Šçš„æå®³é …ç›®ã€‚

ã€å¤šåŸå‘Šæå®³æè¿°ã€‘
{preprocessed_facts}

ã€ç•¶äº‹äººä¿¡æ¯ã€‘
åŸå‘Šï¼š{plaintiff}ï¼ˆå…±{plaintiff_count}åï¼‰

ã€é‡è¦åˆ†ææŒ‡å°ã€‘
é€™æ˜¯å¤šåŸå‘Šæ¡ˆä¾‹ï¼Œéœ€è¦ï¼š
1. åˆ†åˆ¥è­˜åˆ¥æ¯ä½åŸå‘Šçš„æå®³é …ç›®å’Œé‡‘é¡
2. å€åˆ†è¨ˆç®—åŸºæº–vsæœ€çµ‚æ±‚å„Ÿé‡‘é¡
3. æŒ‰åŸå‘Šåˆ†çµ„æ•´ç†æå®³é …ç›®

ã€è¼¸å‡ºæ ¼å¼ã€‘
ï¼ˆä¸€ï¼‰åŸå‘Š[å§“å]ä¹‹æå®³ï¼š
1. æå®³é¡å‹ï¼šé‡‘é¡å…ƒ
åŸå‘Š[å§“å]å› æœ¬æ¬¡äº‹æ•…[ç°¡æ½”ç†ç”±]ï¼Œæ”¯å‡º/å—æœ‰[æå®³é¡å‹]é‡‘é¡å…ƒã€‚

ï¼ˆäºŒï¼‰åŸå‘Š[å§“å]ä¹‹æå®³ï¼š
1. æå®³é¡å‹ï¼šé‡‘é¡å…ƒ
åŸå‘Š[å§“å]å› æœ¬æ¬¡äº‹æ•…[ç°¡æ½”ç†ç”±]ï¼Œæ”¯å‡º/å—æœ‰[æå®³é¡å‹]é‡‘é¡å…ƒã€‚

ã€ç¯„ä¾‹ã€‘
ï¼ˆä¸€ï¼‰åŸå‘Šç¾…é–å´´ä¹‹æå®³ï¼š
1. é†«ç™‚è²»ç”¨ï¼š2,443å…ƒ
åŸå‘Šç¾…é–å´´å› æœ¬æ¬¡äº‹æ•…å—å‚·å°±é†«ï¼Œæ”¯å‡ºé†«ç™‚è²»ç”¨2,443å…ƒã€‚
2. äº¤é€šè²»ï¼š1,235å…ƒ
åŸå‘Šç¾…é–å´´å› æœ¬æ¬¡äº‹æ•…å°±é†«ï¼Œæ”¯å‡ºäº¤é€šè²»ç”¨1,235å…ƒã€‚
3. å·¥ä½œæå¤±ï¼š20,148å…ƒ
åŸå‘Šç¾…é–å´´å› æœ¬æ¬¡äº‹æ•…ç„¡æ³•å·¥ä½œ16æ—¥ï¼Œå—æœ‰å·¥ä½œæå¤±20,148å…ƒã€‚
4. æ…°æ’«é‡‘ï¼š10,000å…ƒ
åŸå‘Šç¾…é–å´´å› æœ¬æ¬¡äº‹æ•…å—æœ‰èº«å¿ƒç—›è‹¦ï¼Œè«‹æ±‚ç²¾ç¥æ…°æ’«é‡‘10,000å…ƒã€‚

ï¼ˆäºŒï¼‰åŸå‘Šé‚±å“å¦ä¹‹æå®³ï¼š
1. é†«ç™‚è²»ç”¨ï¼š57,550å…ƒ
åŸå‘Šé‚±å“å¦å› æœ¬æ¬¡äº‹æ•…å—å‚·å°±é†«ï¼Œæ”¯å‡ºé†«ç™‚è²»ç”¨57,550å…ƒã€‚
2. äº¤é€šè²»ï¼š22,195å…ƒ
åŸå‘Šé‚±å“å¦å› æœ¬æ¬¡äº‹æ•…å°±é†«ï¼Œæ”¯å‡ºäº¤é€šè²»ç”¨22,195å…ƒã€‚
3. å·¥ä½œæå¤±ï¼š66,768å…ƒ
åŸå‘Šé‚±å“å¦å› æœ¬æ¬¡äº‹æ•…ç„¡æ³•å·¥ä½œ1æœˆ28æ—¥ï¼Œå—æœ‰å·¥ä½œæå¤±66,768å…ƒã€‚
4. è»Šè¼›æå¤±ï¼š36,000å…ƒ
åŸå‘Šé‚±å“å¦å› æœ¬æ¬¡äº‹æ•…è»Šè¼›å—æï¼Œæ”¯å‡ºä¿®å¾©åŠé‘‘å®šè²»ç”¨36,000å…ƒã€‚
5. æ…°æ’«é‡‘ï¼š60,000å…ƒ
åŸå‘Šé‚±å“å¦å› æœ¬æ¬¡äº‹æ•…å—æœ‰èº«å¿ƒç—›è‹¦ï¼Œè«‹æ±‚ç²¾ç¥æ…°æ’«é‡‘60,000å…ƒã€‚

ã€åš´æ ¼è¦æ±‚ã€‘
- å¿…é ˆæŒ‰åŸå‘Šåˆ†çµ„ï¼Œæ¯ä½åŸå‘Šå–®ç¨åˆ—å‡º
- æ¯ä½åŸå‘Šå…ˆç”¨ï¼ˆä¸€ï¼‰ï¼ˆäºŒï¼‰ç­‰ä¸­æ–‡ç·¨è™Ÿå€åˆ†
- æ¯ä½åŸå‘Šå…§éƒ¨ç”¨1. 2. 3.ç­‰æ•¸å­—ç·¨è™Ÿ
- åªæå–æœ€çµ‚æ±‚å„Ÿé‡‘é¡ï¼Œæ’é™¤è¨ˆç®—åŸºæº–ï¼ˆå¦‚æœˆè–ªã€æ—¥è–ªç­‰ï¼‰
- æ¯é …ç†ç”±1-2å¥è©±ï¼Œç°¡æ½”æ˜ç­
- ä½¿ç”¨åƒåˆ†ä½é€—è™Ÿæ ¼å¼
- ä¸è¦åŒ…å«ç¸½è¨ˆã€ç¶œä¸Šæ‰€è¿°ç­‰çµè«–æ€§æ–‡å­—

è«‹åˆ†æä¸¦è¼¸å‡ºï¼š"""
            
        elif detected_format == 'free_format':
            # è‡ªç”±æ–‡æœ¬æ ¼å¼ - å°ˆé–€è™•ç†éçµæ§‹åŒ–æè¿°
            return f"""ä½ æ˜¯å°ç£è³‡æ·±å¾‹å¸«ï¼Œè«‹å¾ä»¥ä¸‹è‡ªç”±æ–‡æœ¬ä¸­æå–ä¸¦æ•´ç†æå®³è³ å„Ÿé …ç›®ã€‚

ã€åŸå§‹è‡ªç”±æ–‡æœ¬ã€‘
{preprocessed_facts}

ã€é‡è¦åˆ†ææŒ‡å°ã€‘
ä»¥ä¸‹æ–‡æœ¬å¯èƒ½åŒ…å«ï¼š
1. æ··åˆæè¿°çš„æå®³é …ç›®å’Œé‡‘é¡
2. è¨ˆç®—éç¨‹ä¸­çš„åŸºæº–æ•¸æ“šï¼ˆå¦‚åŸºæœ¬å·¥è³‡ã€æ—¥è–ªç­‰ï¼‰- é€™äº›ä¸æ˜¯æœ€çµ‚æ±‚å„Ÿé‡‘é¡
3. æœ€çµ‚çš„æ±‚å„Ÿé‡‘é¡

ã€æå–åŸå‰‡ã€‘
1. è­˜åˆ¥çœŸæ­£çš„æå®³é¡å‹ï¼šé†«ç™‚è²»ç”¨ã€äº¤é€šè²»ã€çœ‹è­·è²»ã€å·¥ä½œæå¤±ã€ç²¾ç¥æ…°æ’«é‡‘ç­‰
2. å€åˆ†è¨ˆç®—åŸºæº–vsæœ€çµ‚æ±‚å„Ÿï¼š
   - è¨ˆç®—åŸºæº–ï¼šã€Œä»¥æ¯æ—¥XXå…ƒä½œç‚ºè¨ˆç®—åŸºæº–ã€ã€ã€Œæ¯æœˆåŸºæœ¬å·¥è³‡XXå…ƒè¨ˆç®—ã€
   - æœ€çµ‚æ±‚å„Ÿï¼šã€Œå…±è«‹æ±‚XXå…ƒã€ã€ã€Œè³ å„ŸXXå…ƒã€ã€ã€Œæå¤±ç‚ºXXå…ƒã€
3. åªæ¡ç”¨æœ€çµ‚æ±‚å„Ÿé‡‘é¡ï¼Œæ’é™¤è¨ˆç®—éç¨‹ä¸­çš„åŸºæº–æ•¸æ“š

ã€è¼¸å‡ºæ ¼å¼ã€‘
ï¼ˆä¸€ï¼‰æå®³é¡å‹ï¼šé‡‘é¡å…ƒ
åŸå‘Šå› æœ¬æ¬¡äº‹æ•…[ç°¡æ½”ç†ç”±èªªæ˜]ã€‚

ã€ç¯„ä¾‹ã€‘
ï¼ˆä¸€ï¼‰é†«ç™‚è²»ç”¨ï¼š255,830å…ƒ
åŸå‘Šå› æœ¬æ¬¡äº‹æ•…å—å‚·å°±é†«ï¼Œæ”¯å‡ºé†«ç™‚åŠäº¤é€šè²»ç”¨å…±è¨ˆ255,830å…ƒã€‚

ï¼ˆäºŒï¼‰çœ‹è­·è²»ç”¨ï¼š270,000å…ƒ
åŸå‘Šå› æœ¬æ¬¡äº‹æ•…éœ€å°ˆäººç…§é¡§ï¼Œæ”¯å‡ºçœ‹è­·è²»ç”¨å…±è¨ˆ270,000å…ƒã€‚

ï¼ˆä¸‰ï¼‰å·¥ä½œæå¤±ï¼š113,625å…ƒ
åŸå‘Šå› æœ¬æ¬¡äº‹æ•…ç„¡æ³•å·¥ä½œï¼Œå—æœ‰è–ªè³‡æå¤±å…±è¨ˆ113,625å…ƒã€‚

ï¼ˆå››ï¼‰æ…°æ’«é‡‘ï¼š300,000å…ƒ
åŸå‘Šå› æœ¬æ¬¡äº‹æ•…å—æœ‰èº«å¿ƒç—›è‹¦ï¼Œè«‹æ±‚ç²¾ç¥æ…°æ’«é‡‘300,000å…ƒã€‚

ã€åš´æ ¼è¦æ±‚ã€‘
- åªè¼¸å‡ºçœŸæ­£çš„æ±‚å„Ÿé …ç›®ï¼Œæ’é™¤è¨ˆç®—åŸºæº–
- æ¯é …ç†ç”±1-2å¥è©±ï¼Œç°¡æ½”æ˜ç­
- ä½¿ç”¨åƒåˆ†ä½é€—è™Ÿæ ¼å¼
- ä¸è¦åŒ…å«ç¸½è¨ˆã€ç¶œä¸Šæ‰€è¿°ç­‰çµè«–æ€§æ–‡å­—

è«‹åˆ†æä¸¦è¼¸å‡ºï¼š"""
            
        else:
            # å…¶ä»–æ ¼å¼æˆ–æœªçŸ¥æ ¼å¼ - ä½¿ç”¨é€šç”¨ç­–ç•¥
            if plaintiff_count == 1:
                return f"""è«‹å°‡ä»¥ä¸‹æå®³å…§å®¹æ•´ç†æˆæ¨™æº–æ ¼å¼çš„æå®³é …ç›®æ¸…å–®ã€‚

ã€åŸå§‹å…§å®¹ã€‘
{preprocessed_facts}

ã€è¼¸å‡ºæ ¼å¼ã€‘
ï¼ˆä¸€ï¼‰é …ç›®åç¨±ï¼šé‡‘é¡å…ƒ
åŸå‘Šå› æœ¬æ¬¡äº‹æ•…[ç°¡æ½”ç†ç”±]ï¼Œæ”¯å‡º/å—æœ‰[é …ç›®åç¨±]é‡‘é¡å…ƒã€‚

ã€è¦æ±‚ã€‘
- æ¯é …ç†ç”±1-2å¥è©±ï¼Œç°¡æ½”æ˜ç­
- ä¿ç•™é‡è¦é†«é™¢åç¨±ã€å·¥ä½œå–®ä½ç­‰é—œéµè³‡è¨Š  
- çµ±ä¸€ä½¿ç”¨"æœ¬æ¬¡äº‹æ•…"
- å·¥ä½œæå¤±é …ç›®åç¨±ç›´æ¥ç”¨"å·¥ä½œæå¤±"
- ç²¾ç¥æ…°æ’«é‡‘é …ç›®åç¨±ç›´æ¥ç”¨"æ…°æ’«é‡‘"
- ä½¿ç”¨åƒåˆ†ä½é€—è™Ÿæ ¼å¼é¡¯ç¤ºé‡‘é¡

è«‹è¼¸å‡ºï¼š"""
            else:
                # å¤šåŸå‘Šæƒ…æ³
                return f"""ä½ æ˜¯å°ç£å¾‹å¸«ï¼Œè«‹æ ¹æ“šè»Šç¦æ¡ˆä»¶çš„æå®³è³ å„Ÿå…§å®¹ï¼Œåˆ†æä¸¦é‡æ–°æ•´ç†æˆæ¨™æº–çš„èµ·è¨´ç‹€æå®³é …ç›®æ ¼å¼ï¼š

ã€ç•¶äº‹äººè³‡è¨Šã€‘
åŸå‘Šï¼š{parties.get('åŸå‘Š', 'æœªæåŠ')}ï¼ˆå…±{plaintiff_count}åï¼‰
è¢«å‘Šï¼š{parties.get('è¢«å‘Š', 'æœªæåŠ')}ï¼ˆå…±{parties.get('è¢«å‘Šæ•¸é‡', 1)}åï¼‰

ã€åŸå§‹æå®³æè¿°ã€‘
{preprocessed_facts}

ã€æ¨™æº–è¼¸å‡ºæ ¼å¼ã€‘
ï¼ˆä¸€ï¼‰åŸå‘Š[å§“å]ä¹‹æå®³ï¼š
1. é†«ç™‚è²»ç”¨ï¼šé‡‘é¡å…ƒ
åŸå‘Š[å§“å]å› æœ¬æ¬¡è»Šç¦å—å‚·ï¼Œ[è©³ç´°å°±é†«æƒ…æ³]ï¼Œæ”¯å‡ºé†«ç™‚è²»ç”¨è¨ˆé‡‘é¡å…ƒã€‚

ã€è¦æ±‚ã€‘
- æ¯ä½åŸå‘Šå…ˆç”¨ï¼ˆä¸€ï¼‰ï¼ˆäºŒï¼‰ç­‰ç·¨è™Ÿå€åˆ†
- æ¯ä½åŸå‘Šå…§éƒ¨çš„æå®³é …ç›®ä½¿ç”¨ 1. 2. 3. ç­‰æ•¸å­—ç·¨è™Ÿ
- æ¯é …å¿…é ˆåŒ…å«ï¼šæ•¸å­—ç·¨è™Ÿ. é …ç›®åç¨±ï¼šé‡‘é¡ + è©³ç´°ç†ç”±èªªæ˜
- ç†ç”±è¦å®Œæ•´ï¼ŒåŒ…å«æå®³åŸå› ã€äº‹å¯¦ä¾æ“šã€è©³ç´°èªªæ˜
- ä¸å¯åŒ…å«ç¸½è¨ˆã€ç¶œä¸Šæ‰€è¿°ç­‰çµè«–æ€§æ–‡å­—

è«‹åˆ†æä¸¦è¼¸å‡ºï¼š"""

    def _comprehensive_number_preprocessing(self, text: str) -> str:
        """å…¨é¢é è™•ç†ä¸­æ–‡æ•¸å­—å’Œç‰¹æ®Šæ ¼å¼"""
        import re
        
        # è™•ç† Xè¬Y,YYYå…ƒ æ ¼å¼ (å¦‚ï¼š26è¬4,379å…ƒ)
        pattern1 = r'(\d+)è¬(\d+,?\d+)å…ƒ'
        def replace1(match):
            wan = int(match.group(1))
            rest = int(match.group(2).replace(',', ''))
            total = wan * 10000 + rest
            return f"{total}å…ƒ"
        text = re.sub(pattern1, replace1, text)
        
        # è™•ç†å…¶ä»–ä¸­æ–‡æ•¸å­—æ ¼å¼
        text = re.sub(r'(\d+)è¬(\d+)åƒå…ƒ', lambda m: f"{int(m.group(1))*10000 + int(m.group(2))*1000}å…ƒ", text)
        text = re.sub(r'(\d+)è¬å…ƒ', lambda m: f"{int(m.group(1))*10000}å…ƒ", text)
        text = re.sub(r'(\d+)åƒå…ƒ', lambda m: f"{int(m.group(1))*1000}å…ƒ", text)
        
        return text

    def _is_same_damage_type(self, context1: str, context2: str) -> bool:
        """åˆ¤æ–·å…©å€‹ä¸Šä¸‹æ–‡æ˜¯å¦ç‚ºç›¸åŒçš„æå®³é¡å‹"""
        damage_types = [
            ['é†«ç™‚', 'æ²»ç™‚', 'å°±è¨º'],
            ['çœ‹è­·', 'ç…§é¡§'],
            ['ç‰™é½’', 'å‡ç‰™'],
            ['æ…°æ’«', 'ç²¾ç¥', 'ç—›è‹¦'],
            ['äº¤é€š', 'è»Šè³‡'],
            ['å·¥ä½œ', 'æ”¶å…¥', 'è–ªè³‡'],
            ['ä¿®å¾©', 'ä¿®ç†', 'ç¶­ä¿®']
        ]
        
        # æ‰¾å‡ºæ¯å€‹ä¸Šä¸‹æ–‡çš„æå®³é¡å‹
        type1 = None
        type2 = None
        
        for i, keywords in enumerate(damage_types):
            if any(keyword in context1 for keyword in keywords):
                type1 = i
            if any(keyword in context2 for keyword in keywords):
                type2 = i
        
        return type1 is not None and type1 == type2

    def _extract_valid_claim_amounts(self, text: str) -> list:
        """é€šç”¨æ™ºèƒ½é‡‘é¡æå– - ä½¿ç”¨å¤šå±¤æ¬¡ç­–ç•¥é©æ‡‰å„ç¨®æ ¼å¼"""
        print(f"ğŸ” ã€é€šç”¨é‡‘é¡æå–ã€‘åŸå§‹æ–‡æœ¬: {text[:200]}...")

        # ç­–ç•¥1: ä½¿ç”¨é€šç”¨æ ¼å¼è™•ç†å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.format_handler:
            try:
                damage_items = self.format_handler.extract_damage_items(text)
                if damage_items and len(damage_items) >= 2:
                    amounts = [item.amount for item in damage_items]
                    print(f"ğŸ” ã€é€šç”¨æ ¼å¼è™•ç†å™¨ã€‘æˆåŠŸæå– {len(amounts)} å€‹é‡‘é¡: {amounts}")
                    print(f"ğŸ” ã€é€šç”¨æ ¼å¼è™•ç†å™¨ã€‘ç¸½è¨ˆ: {sum(amounts):,}å…ƒ")
                    return amounts
                else:
                    print("ğŸ” ã€é€šç”¨æ ¼å¼è™•ç†å™¨ã€‘æå–çµæœä¸è¶³ï¼Œä½¿ç”¨fallbackç­–ç•¥")
            except Exception as e:
                print(f"ğŸ” ã€é€šç”¨æ ¼å¼è™•ç†å™¨ã€‘è™•ç†å¤±æ•—: {e}ï¼Œä½¿ç”¨fallbackç­–ç•¥")

        # ç­–ç•¥2: Fallbackåˆ°åŸæœ‰é‚è¼¯ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
        return self._extract_amounts_legacy_method(text)

    def _extract_amounts_legacy_method(self, text: str) -> list:
        """åŸæœ‰çš„é‡‘é¡æå–é‚è¼¯ï¼ˆä½œç‚ºfallbackï¼‰"""
        import re

        print(f"ğŸ” ã€å‚³çµ±é‡‘é¡æå–ã€‘Fallbackåˆ°åŸæœ‰é‚è¼¯...")

        # 1. å…ˆé è™•ç†ä¸­æ–‡æ•¸å­—
        processed_text = self._comprehensive_number_preprocessing(text)
        clean_text = processed_text.replace(',', '')

        # 2. å®šç¾©æœ‰æ•ˆçš„æ±‚å„Ÿé—œéµè©
        valid_claim_keywords = [
            'è²»ç”¨', 'æå¤±', 'æ…°æ’«é‡‘', 'è³ å„Ÿ', 'æ”¯å‡º', 'èŠ±è²»',
            'é†«ç™‚', 'ä¿®å¾©', 'ä¿®ç†', 'äº¤é€š', 'çœ‹è­·', 'æ‰‹è¡“',
            'å‡ç‰™', 'å¾©å¥', 'æ²»ç™‚', 'å·¥ä½œæ”¶å…¥', 'é ä¼°', 'æœªä¾†', 'é è¨ˆ', 'ç”¨å“'
        ]

        # 3. å®šç¾©æ’é™¤çš„é—œéµè©ï¼ˆéæ±‚å„Ÿé …ç›®ï¼‰- æ›´ç²¾ç¢ºçš„åŒ¹é…
        exclude_keywords = [
            'æ—¥è–ª', 'å¹´åº¦æ‰€å¾—', 'æœˆæ”¶å…¥', 'æ™‚è–ª', 'å­¸æ­·', 'ç•¢æ¥­',
            'åä¸‹', 'å‹•ç”¢', 'ç¸½è¨ˆæ–°å°å¹£', 'åˆè¨ˆæ–°å°å¹£', 'å°è¨ˆæ–°å°å¹£',
            'åŒ…æ‹¬', 'å…¶ä¸­', 'åŒ…å«',  # æ·»åŠ ç´°é …åˆ†è§£é—œéµè©
            'æ¯æ—¥', 'ä¸€æ—¥', 'æ—¥è¨ˆ', 'ä»¥æ¯æ—¥',  # æ—¥è–ªç›¸é—œé—œéµè©
            'æ¯æœˆ', 'æœˆè¨ˆ', 'ä»¥æ¯æœˆ', 'åŸºæœ¬å·¥è³‡', 'æœˆè–ª', 'åº•è–ª',  # è–ªè³‡åƒè€ƒæ•¸æ“š
            'æ‰€å¾—', 'è–ªè³‡æ‰€å¾—', 'å¹´æ”¶å…¥',  # è–ªè³‡åƒè€ƒæ•¸æ“š
            'æ­¤æœ‰', 'å¯è­‰', 'ç‚ºè­‰', 'æ”¶æ“š', 'ç™¼ç¥¨', 'è­‰æ˜',  # è­‰æ“šç›¸é—œ
            'ç¶“æŸ¥', 'æŸ¥æ˜', 'ç¶“å¯©ç†',  # åˆ¤æ±ºæ›¸ç”¨èª
            'ä½œç‚ºè¨ˆç®—åŸºæº–', 'è¨ˆç®—åŸºæº–', 'åŸºæº–'  # è¨ˆç®—åŸºæº–ç›¸é—œ
        ]

        amounts = []
        lines = clean_text.split('\n')

        for line in lines:
            # æ‰¾å‡ºè©²è¡Œä¸­çš„æ‰€æœ‰é‡‘é¡
            line_amounts = re.findall(r'(\d+)\s*å…ƒ', line)

            for amt_str in line_amounts:
                try:
                    amount = int(amt_str)
                    if amount < 100:  # è·³éå°é¡ï¼ˆå¯èƒ½æ˜¯ç·¨è™Ÿç­‰ï¼‰
                        continue

                    # æª¢æŸ¥é‡‘é¡å‘¨åœçš„ä¸Šä¸‹æ–‡
                    # æ‰¾åˆ°é‡‘é¡åœ¨åŸæ–‡ä¸­çš„ä½ç½®
                    amount_pos = line.find(amt_str + 'å…ƒ')
                    if amount_pos == -1:
                        continue

                    # æå–é‡‘é¡å‰å¾Œ100å€‹å­—ç¬¦çš„ä¸Šä¸‹æ–‡ï¼ˆæ“´å¤§ç¯„åœï¼‰
                    start = max(0, amount_pos - 100)
                    end = min(len(line), amount_pos + 100)
                    context = line[start:end]
                    
                    # ä¹Ÿæª¢æŸ¥æ•´å€‹æ–‡æœ¬ä¸­è©²é‡‘é¡çš„ä¸Šä¸‹æ–‡ï¼ˆç”¨æ–¼æ›´æº–ç¢ºçš„åŸºæº–æª¢æ¸¬ï¼‰
                    full_text_pos = clean_text.find(amt_str + 'å…ƒ')
                    if full_text_pos != -1:
                        full_start = max(0, full_text_pos - 150)
                        full_end = min(len(clean_text), full_text_pos + 150)
                        full_context = clean_text[full_start:full_end]
                    else:
                        full_context = context

                    # å…ˆæª¢æŸ¥æ˜¯å¦åŒ…å«æœ‰æ•ˆæ±‚å„Ÿé—œéµè©
                    is_valid_claim = any(keyword in context for keyword in valid_claim_keywords)
                    
                    if is_valid_claim:
                        # å¦‚æœæ˜¯æœ‰æ•ˆæ±‚å„Ÿé …ç›®ï¼Œå†æª¢æŸ¥æ˜¯å¦éœ€è¦æ’é™¤
                        should_exclude = any(keyword in context for keyword in exclude_keywords)
                        
                        # æª¢æŸ¥æ˜¯å¦ç‚ºè¨ˆç®—åŸºæº–ï¼ˆä¸­é–“å€¼ï¼‰- ä½¿ç”¨å®Œæ•´ä¸Šä¸‹æ–‡
                        calculation_pattern = f'{amount}å…ƒè¨ˆç®—'
                        clean_full_context = full_context.replace(',', '')
                        is_calculation_base = (
                            'ä½œç‚ºè¨ˆç®—åŸºæº–' in full_context or
                            ('ä»¥æ¯æ—¥' in full_context and 'ä½œç‚ºè¨ˆç®—åŸºæº–' in full_context) or
                            ('åŸºæœ¬å·¥è³‡' in full_context and calculation_pattern in clean_full_context)  # å¦‚æœæ˜¯ "XXXå…ƒè¨ˆç®—" æ ¼å¼å°±æ˜¯åŸºæº–
                        )
                        
                        # æª¢æŸ¥æ˜¯å¦ç‚ºæœ€çµ‚æ±‚å„Ÿé‡‘é¡ï¼ˆé‡‘é¡ç›´æ¥è·Ÿåœ¨é—œéµè©å¾Œé¢ï¼‰
                        claim_patterns = [
                            f'æå¤±ç‚º{amount}å…ƒ',  # å—æœ‰ä¹‹è–ªè³‡æå¤±ç‚º113625å…ƒ
                            f'å…±è«‹æ±‚{amount}å…ƒ',  # å…±è«‹æ±‚270000å…ƒ
                            f'æ”¯å‡º.*{amount}å…ƒ',  # æ”¯å‡ºé†«ç™‚è²»ç”¨255830å…ƒ
                            f'è«‹æ±‚.*{amount}å…ƒ',  # è«‹æ±‚æ…°æ’«é‡‘300000å…ƒ
                            f'è³ å„Ÿ.*{amount}å…ƒ'   # è³ å„Ÿé‡‘é¡
                        ]
                        # æª¢æŸ¥æ™‚ç¢ºä¿é‡‘é¡ç·Šè·Ÿåœ¨æè¿°å¾Œé¢ï¼Œä¸æ˜¯ä½œç‚ºè¨ˆç®—åŸºæº–
                        context_clean = context.replace(',', '')
                        is_final_claim = False
                        for pattern in claim_patterns:
                            if re.search(pattern, context_clean):
                                # é€²ä¸€æ­¥æª¢æŸ¥ï¼šå¦‚æœåŒæ™‚åŒ…å«"è¨ˆç®—"é—œéµè©ï¼Œå¯èƒ½æ˜¯åŸºæº–è€Œéæœ€çµ‚é‡‘é¡
                                if 'è¨ˆç®—' not in context or f'{amount}å…ƒè¨ˆç®—' not in context_clean:
                                    is_final_claim = True
                                    break
                        
                        
                        # å¦‚æœæ˜¯è¨ˆç®—åŸºæº–ä½†ä¸æ˜¯æœ€çµ‚æ±‚å„Ÿï¼Œæ’é™¤
                        if is_calculation_base and not is_final_claim:
                            should_exclude = True
                        # å¦‚æœæ˜¯æœ€çµ‚æ±‚å„Ÿï¼Œå³ä½¿åŒ…å«å…¶ä»–æ’é™¤é—œéµè©ä¹Ÿä¸æ’é™¤    
                        elif is_final_claim:
                            should_exclude = False
                        
                        if should_exclude:
                            print(f"ğŸ” ã€æ’é™¤ã€‘{amount:,}å…ƒ - åŒ…å«æ’é™¤é—œéµè©: {context[:50]}...")
                        else:
                            print(f"ğŸ” ã€æœ‰æ•ˆã€‘{amount:,}å…ƒ - ä¸Šä¸‹æ–‡: {context[:50]}...")
                            amounts.append(amount)
                    else:
                        print(f"ğŸ” ã€è·³éã€‘{amount:,}å…ƒ - ç„¡æ˜ç¢ºæ±‚å„Ÿé—œéµè©: {context[:50]}...")

                except ValueError:
                    continue

        # 4. æ”¹é€²çš„å»é‡é‚è¼¯ï¼ˆæŒ‰é …ç›®é¡å‹åˆ†çµ„ï¼‰
        damage_items = {}  # æŒ‰é¡å‹åˆ†çµ„ï¼š{é¡å‹: [é‡‘é¡åˆ—è¡¨]}
        
        for line in clean_text.split('\n'):
            # è­˜åˆ¥æå®³é …ç›®æ¨™é¡Œè¡Œï¼ˆå¦‚ï¼šï¼ˆä¸€ï¼‰é†«ç™‚è²»ç”¨38,073å…ƒ æˆ– 1. é†«ç™‚è²»ç”¨38,073å…ƒï¼‰
            if (re.match(r'^[ï¼ˆ][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å][ï¼‰]', line.strip()) or 
                re.match(r'^[ãˆ ãˆ¡ãˆ¢ãˆ£ãˆ¤ãˆ¥ãˆ¦ãˆ§ãˆ¨ãˆ©]', line.strip()) or 
                re.match(r'^\d+\.\s*[^\d]*\d+å…ƒ', line.strip())):
                line_amounts = re.findall(r'(\d+)\s*å…ƒ', line)
                for amt_str in line_amounts:
                    try:
                        amount = int(amt_str)
                        if amount >= 100:  # æ’é™¤å°é¡
                            # åˆ¤æ–·æå®³é¡å‹
                            damage_type = "å…¶ä»–"
                            if 'é ä¼°é†«ç™‚' in line or 'æœªä¾†é†«ç™‚' in line or 'é è¨ˆé†«ç™‚' in line:
                                damage_type = "é ä¼°é†«ç™‚è²»ç”¨"
                            elif 'é†«ç™‚ç”¨å“' in line:
                                damage_type = "é†«ç™‚ç”¨å“è²»ç”¨"
                            elif 'é†«ç™‚å™¨æ' in line:
                                damage_type = "é†«ç™‚å™¨æè²»ç”¨"
                            elif 'é†«ç™‚' in line:
                                damage_type = "é†«ç™‚è²»ç”¨"
                            elif 'çœ‹è­·' in line:
                                damage_type = "çœ‹è­·è²»ç”¨"
                            elif 'ç‰™é½’' in line or 'å‡ç‰™' in line:
                                damage_type = "ç‰™é½’æå®³"
                            elif 'æ…°æ’«' in line or 'ç²¾ç¥' in line:
                                damage_type = "ç²¾ç¥æ…°æ’«é‡‘"
                            elif 'äº¤é€š' in line:
                                damage_type = "äº¤é€šè²»ç”¨"
                            elif 'è»Šè¼›' in line or 'æ©Ÿè»Š' in line or 'ä¿®å¾©' in line or 'ä¿®ç†' in line or 'ç¶­ä¿®' in line:
                                damage_type = "è»Šè¼›ä¿®å¾©è²»ç”¨"
                            elif 'ç„¡æ³•å·¥ä½œ' in line or 'å·¥ä½œæå¤±' in line:
                                damage_type = "ç„¡æ³•å·¥ä½œæå¤±"
                            elif 'å·¥ä½œ' in line or 'æ”¶å…¥' in line or 'æå¤±' in line:
                                damage_type = "å·¥ä½œæå¤±"
                            
                            if damage_type not in damage_items:
                                damage_items[damage_type] = []
                            damage_items[damage_type].append(amount)
                            print(f"ğŸ” ã€ç¢ºèªé …ç›®ã€‘{damage_type}: {amount:,}å…ƒ")
                    except ValueError:
                        continue
        
        # æ¯ç¨®æå®³é¡å‹åªå–ä¸€å€‹é‡‘é¡ï¼ˆé€šå¸¸æ¨™é¡Œè¡Œçš„é‡‘é¡æ˜¯æ­£ç¢ºçš„ï¼‰
        final_amounts = []
        for damage_type, amounts_list in damage_items.items():
            if amounts_list:
                # å–è©²é¡å‹çš„ç¬¬ä¸€å€‹é‡‘é¡ï¼ˆæ¨™é¡Œè¡Œï¼‰
                final_amounts.append(amounts_list[0])
                print(f"âœ… ã€æ¡ç”¨ã€‘{damage_type}: {amounts_list[0]:,}å…ƒ")

        # å¦‚æœæ²’æœ‰æ‰¾åˆ°çµæ§‹åŒ–é …ç›®ï¼Œä½¿ç”¨ç¬¬ä¸€éšæ®µæå–çš„æ‰€æœ‰æœ‰æ•ˆé‡‘é¡ï¼ˆå»é‡ï¼‰
        if not final_amounts and amounts:
            print("ğŸ” ã€å‚™ç”¨æ–¹æ¡ˆã€‘æœªæ‰¾åˆ°çµæ§‹åŒ–é …ç›®ï¼Œä½¿ç”¨ç¬¬ä¸€éšæ®µçš„æœ‰æ•ˆé‡‘é¡")
            # ç°¡å–®å»é‡ï¼šä¿ç•™ä¸åŒçš„é‡‘é¡
            seen_amounts = set()
            for amount in amounts:
                if amount not in seen_amounts:
                    final_amounts.append(amount)
                    seen_amounts.add(amount)
                    print(f"âœ… ã€æ¡ç”¨ã€‘é‡‘é¡: {amount:,}å…ƒ")

        print(f"ğŸ” ã€å‚³çµ±é‡‘é¡æå–ã€‘å»é‡å¾Œæœ‰æ•ˆé‡‘é¡: {final_amounts}")
        print(f"ğŸ” ã€å‚³çµ±é‡‘é¡æå–ã€‘æœ€çµ‚ç¸½è¨ˆ: {sum(final_amounts):,}å…ƒ")

        return final_amounts

    def _extract_damage_items_from_text(self, text: str) -> Dict[str, List[Dict]]:
        """å¾æ–‡æœ¬ä¸­ç²¾ç¢ºæå–æå®³é …ç›®"""
        # æŒ‰åŸå‘Šåˆ†çµ„
        plaintiff_damages = {}
        
        # åˆ†å¥è™•ç†
        sentences = re.split(r'[ã€‚]', text)
        
        for sentence in sentences:
            # è­˜åˆ¥åŸå‘Š
            plaintiff_match = re.search(r'åŸå‘Š([^ï¼Œã€‚ï¼›ã€\s]{2,4})', sentence)
            if not plaintiff_match:
                continue
                
            plaintiff = plaintiff_match.group(1)
            if plaintiff not in plaintiff_damages:
                plaintiff_damages[plaintiff] = []
            
            # ç²¾ç¢ºåŒ¹é…å„ç¨®æå®³é¡å‹
            # é†«ç™‚è²»ç”¨
            if 'é†«ç™‚è²»ç”¨' in sentence:
                amount_match = re.search(r'é†«ç™‚è²»ç”¨\s*(\d+(?:,\d{3})*)\s*å…ƒ', sentence)
                if amount_match:
                    plaintiff_damages[plaintiff].append({
                        'name': 'é†«ç™‚è²»ç”¨',
                        'amount': int(amount_match.group(1).replace(',', '')),
                        'description': f'åŸå‘Š{plaintiff}å› æœ¬æ¬¡äº‹æ•…å—å‚·å°±é†«ä¹‹é†«ç™‚è²»ç”¨'
                    })
            
            # äº¤é€šè²»
            if 'äº¤é€šè²»' in sentence:
                amount_match = re.search(r'äº¤é€šè²»\s*(\d+(?:,\d{3})*)\s*å…ƒ', sentence)
                if amount_match:
                    plaintiff_damages[plaintiff].append({
                        'name': 'äº¤é€šè²»ç”¨',
                        'amount': int(amount_match.group(1).replace(',', '')),
                        'description': f'åŸå‘Š{plaintiff}å› æœ¬æ¬¡äº‹æ•…æ‰€ç”Ÿä¹‹äº¤é€šè²»ç”¨'
                    })
            
            # å·¥ä½œæå¤±
            if any(keyword in sentence for keyword in ['å·¥è³‡æå¤±', 'ä¸èƒ½å·¥ä½œ', 'ç„¡æ³•å·¥ä½œ']):
                amount_match = re.search(r'(?:æå¤±|è«‹æ±‚)\s*(\d+(?:,\d{3})*)\s*å…ƒ', sentence)
                if amount_match:
                    plaintiff_damages[plaintiff].append({
                        'name': 'å·¥ä½œæå¤±',
                        'amount': int(amount_match.group(1).replace(',', '')),
                        'description': f'åŸå‘Š{plaintiff}å› æœ¬æ¬¡äº‹æ•…ç„¡æ³•å·¥ä½œä¹‹æ”¶å…¥æå¤±'
                    })
            
            # ç²¾ç¥æ…°æ’«é‡‘
            if 'æ…°æ’«é‡‘' in sentence:
                amount_match = re.search(r'æ…°æ’«é‡‘\s*(\d+(?:,\d{3})*)\s*å…ƒ', sentence)
                if amount_match:
                    plaintiff_damages[plaintiff].append({
                        'name': 'ç²¾ç¥æ…°æ’«é‡‘',
                        'amount': int(amount_match.group(1).replace(',', '')),
                        'description': f'åŸå‘Š{plaintiff}å› æœ¬æ¬¡äº‹æ•…æ‰€å—ç²¾ç¥ç—›è‹¦ä¹‹æ…°æ’«é‡‘'
                    })
            
            # è»Šè¼›è²¶å€¼
            if any(keyword in sentence for keyword in ['è²¶å€¼', 'è²¶æ', 'åƒ¹å€¼æ¸›æ']):
                amount_match = re.search(r'(?:è²¶æ|æ¸›æ)\s*(\d+(?:,\d{3})*)\s*å…ƒ', sentence)
                if amount_match:
                    plaintiff_damages[plaintiff].append({
                        'name': 'è»Šè¼›è²¶å€¼æå¤±',
                        'amount': int(amount_match.group(1).replace(',', '')),
                        'description': 'ç³»çˆ­è»Šè¼›å› æœ¬æ¬¡äº‹æ•…ä¹‹åƒ¹å€¼æ¸›æ'
                    })
            
            # é‘‘å®šè²»
            if 'é‘‘å®šè²»' in sentence:
                amount_match = re.search(r'é‘‘å®šè²»\s*(\d+(?:,\d{3})*)\s*å…ƒ', sentence)
                if amount_match:
                    plaintiff_damages[plaintiff].append({
                        'name': 'é‘‘å®šè²»ç”¨',
                        'amount': int(amount_match.group(1).replace(',', '')),
                        'description': 'è»Šè¼›æå®³é‘‘å®šè²»ç”¨'
                    })
        
        return plaintiff_damages
    
    def _format_damage_items(self, damage_items: Dict[str, List[Dict]]) -> str:
        """æ ¼å¼åŒ–æå®³é …ç›®"""
        if not damage_items:
            return ""
        
        result = "ä¸‰ã€æå®³é …ç›®ï¼š\n"
        for idx, (plaintiff, damages) in enumerate(damage_items.items()):
            chinese_num = self._chinese_num(idx + 1)
            result += f"\nï¼ˆ{chinese_num}ï¼‰åŸå‘Š{plaintiff}ä¹‹æå®³ï¼š\n"
            
            for i, damage in enumerate(damages, 1):
                result += f"{i}. {damage['name']}ï¼š{damage['amount']:,}å…ƒ\n"
                result += f"   èªªæ˜ï¼š{damage['description']}\n"
            
            # å°è¨ˆ
            subtotal = sum(d['amount'] for d in damages)
            result += f"\nå°è¨ˆï¼š{subtotal:,}å…ƒ\n"
        
        # ç¸½è¨ˆ
        total = sum(sum(d['amount'] for d in damages) for damages in damage_items.values())
        result += f"\næå®³ç¸½è¨ˆï¼šæ–°å°å¹£{total:,}å…ƒæ•´"
        
        return result

# ===== ä¸»è¦äº’å‹•åŠŸèƒ½ =====

def interactive_generate_lawsuit():
    """äº’å‹•å¼èµ·è¨´ç‹€ç”Ÿæˆï¼ˆæ¢å¾©å¤šè¡Œè¼¸å…¥ç‰ˆæœ¬ï¼‰"""
    print("=" * 80)
    print("ğŸ›ï¸  è»Šç¦èµ·è¨´ç‹€ç”Ÿæˆå™¨ - æ··åˆç‰ˆæœ¬ï¼ˆæ•´åˆçµæ§‹åŒ–é‡‘é¡è™•ç†ï¼‰")
    print("=" * 80)
    print("ğŸ‘‹ æ­¡è¿ä½¿ç”¨ï¼æˆ‘æœƒç‚ºæ‚¨ç”Ÿæˆå°ˆæ¥­çš„èµ·è¨´ç‹€ï¼ŒåŒ…å«ï¼š")
    print("   ğŸ“„ ç›¸ä¼¼æ¡ˆä¾‹æª¢ç´¢")
    print("   âš–ï¸ é©ç”¨æ³•æ¢åˆ†æ")
    print("   ğŸ“‹ å®Œæ•´èµ·è¨´ç‹€ç”Ÿæˆ")
    print("ğŸ’¡ æ”¯æ´çµæ§‹åŒ–é‡‘é¡è™•ç†ï¼Œè‡ªå‹•ä¿®æ­£è¨ˆç®—éŒ¯èª¤")
    print()
    
    print("ğŸ“ ä½¿ç”¨æ–¹æ³•ï¼š")
    print("   1. è«‹ä¸€æ¬¡æ€§è¼¸å…¥å®Œæ•´çš„ä¸‰æ®µå…§å®¹")
    print("   2. å¯ä»¥å¤šè¡Œè¼¸å…¥ï¼Œæ›è¡Œç¹¼çºŒ")
    print("   3. è¼¸å…¥å®Œæˆå¾Œè¼¸å…¥ 'END' ç¢ºèª")
    print("   4. è¼¸å…¥ 'quit' å¯é€€å‡ºç¨‹å¼")
    print()
    
    # åˆå§‹åŒ–ç”Ÿæˆå™¨
    generator = HybridCoTGenerator()
    
    print("ğŸ“ è«‹è¼¸å…¥å®Œæ•´çš„è»Šç¦æ¡ˆä»¶è³‡æ–™ï¼š")
    print("ğŸ“‹ è«‹åŒ…å«ä»¥ä¸‹ä¸‰å€‹éƒ¨åˆ†ï¼š")
    print("   ä¸€ã€äº‹æ•…ç™¼ç”Ÿç·£ç”±ï¼š[è©³è¿°è»Šç¦ç¶“é]")
    print("   äºŒã€åŸå‘Šå—å‚·æƒ…å½¢ï¼š[æè¿°å‚·å‹¢]")
    print("   ä¸‰ã€è«‹æ±‚è³ å„Ÿçš„äº‹å¯¦æ ¹æ“šï¼š[åˆ—å‡ºæå®³é …ç›®å’Œé‡‘é¡]")
    print()
    print("ğŸ’¡ æç¤ºï¼šå¯ä»¥æ›è¡Œè¼¸å…¥ï¼Œå®Œæˆå¾Œè¼¸å…¥ 'END' ç¢ºèª")
    print("=" * 60)
    print("ğŸ¯ è«‹é–‹å§‹è¼¸å…¥ï¼ˆå®Œæˆå¾Œè¼¸å…¥ 'END' æˆ– 'end' ç¢ºèªï¼‰ï¼š")
    
    # å¤šè¡Œè¼¸å…¥æ¨¡å¼
    user_input_lines = []
    while True:
        try:
            line = input()
            if line.strip().upper() in ['END', 'QUIT', 'EXIT', 'é€€å‡º']:
                if line.strip().upper() == 'QUIT' or line.strip().upper() == 'EXIT' or line.strip() == 'é€€å‡º':
                    print("ğŸ‘‹ æ„Ÿè¬ä½¿ç”¨ï¼Œå†è¦‹ï¼")
                    return
                break
            user_input_lines.append(line)
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ç”¨æˆ¶ä¸­æ–·ï¼Œç¨‹åºé€€å‡º")
            return
        except EOFError:
            break
    
    user_query = '\n'.join(user_input_lines).strip()
    
    if not user_query:
        print("âš ï¸ è«‹è¼¸å…¥æœ‰æ•ˆçš„å…§å®¹")
        return
    
    print("ğŸ”„ æ­£åœ¨è™•ç†...")
    
    try:
        # åˆ†æ®µæå–è³‡è¨Š
        sections = extract_sections(user_query)
        
        # æå–ç•¶äº‹äºº
        parties = extract_parties(user_query)
        
        # æ¡ˆä»¶åˆ†é¡å’Œæª¢ç´¢ç›¸ä¼¼æ¡ˆä¾‹
        accident_facts = sections.get("accident_facts", user_query)
        case_type = determine_case_type(accident_facts, parties)
        
        print("âœ… LLMæœå‹™æ­£å¸¸")
        print()
        
        # ç›¸ä¼¼æ¡ˆä¾‹æª¢ç´¢ï¼ˆè©³ç´°æ¨¡å¼ï¼‰
        similar_cases = []
        if FULL_MODE:
            print("ğŸ” æª¢ç´¢ç›¸ä¼¼æ¡ˆä¾‹...")
            # ä½¿ç”¨å›ºå®šåƒæ•¸é€²è¡Œæª¢ç´¢
            k_final = 3
            initial_retrieve_count = 15
            use_multi_stage = True
            
            print(f"ğŸ”§ æª¢ç´¢ç­–ç•¥: ç›®æ¨™{k_final}å€‹æ¡ˆä¾‹ â†’ æª¢ç´¢{initial_retrieve_count}å€‹æ®µè½")
            
            try:
                # è©³ç´°åŸ·è¡Œæª¢ç´¢
                query_vector = embed(accident_facts)
                if query_vector:
                    hits = es_search(query_vector, case_type, top_k=initial_retrieve_count, label="Facts", quiet=False)
                    if hits:
                        print(f"ğŸ” ESåŸå§‹çµæœ: æ‰¾åˆ°{len(hits)}å€‹æ®µè½")
                        
                        # å¤šéšæ®µæ¨¡å¼
                        candidate_case_ids = []
                        for hit in hits:
                            case_id = hit['_source'].get('case_id')
                            if case_id and case_id not in candidate_case_ids:
                                candidate_case_ids.append(case_id)
                        
                        print(f"ğŸ”„ å»é‡å¾Œçµæœ: {len(candidate_case_ids)}å€‹å”¯ä¸€æ¡ˆä¾‹")
                        final_case_ids = candidate_case_ids[:k_final]
                        print(f"ğŸ“Œ æœ€çµ‚é¸å–: {len(final_case_ids)}å€‹æ¡ˆä¾‹ {final_case_ids}")
                        
                        if candidate_case_ids:
                            reranked_case_ids = rerank_case_ids_by_paragraphs(
                                accident_facts, 
                                candidate_case_ids[:k_final*2],
                                label="Facts",
                                quiet=False
                            )
                            final_case_ids = reranked_case_ids[:k_final]
                            print(f"ğŸ“˜ Rerankå¾Œæœ€çµ‚é †åº: {final_case_ids}")
                            
                            similar_cases = get_complete_cases_content(final_case_ids)
                            
                            # é¡¯ç¤ºè©³ç´°æ¡ˆä¾‹åˆ†æ
                            print()
                            print(f"ğŸ“‹ è©³ç´°æ¡ˆä¾‹åˆ†æ (åƒ…é¡¯ç¤ºå‰ {len(similar_cases)} å€‹æœ€ç›¸é—œæ¡ˆä¾‹):")
                            print("=" * 80)
                            print()
                            
                            for i, (case_content, case_id) in enumerate(zip(similar_cases, final_case_ids)):
                                # å¾hitsä¸­æ‰¾åˆ°å°æ‡‰çš„åˆ†æ•¸
                                score = 0.0
                                for hit in hits:
                                    if hit['_source'].get('case_id') == case_id:
                                        score = hit['_score']
                                        break
                                
                                print(f"ğŸ“„ ç›¸ä¼¼æ¡ˆä¾‹ {i+1}: Case ID {case_id}")
                                print(f"ğŸ¯ ESç›¸ä¼¼åº¦åˆ†æ•¸: {score:.4f}")
                                print("-" * 50)
                                case_preview = case_content[:500] + "..." if len(case_content) > 500 else case_content
                                print(case_preview)
                                print()
                                if i < len(similar_cases) - 1:
                                    print()
                        else:
                            # ç°¡å–®æ¨¡å¼
                            if hits:
                                first_hit = hits[0]['_source']
                                content_fields = ['original_text', 'content', 'text', 'facts_content', 'chunk_content', 'body']
                                content_field = None
                                for field in content_fields:
                                    if field in first_hit and first_hit[field]:
                                        content_field = field
                                        break
                                
                                if content_field:
                                    similar_cases = [hit['_source'].get(content_field, '') for hit in hits[:k_final] if hit['_source'].get(content_field)]
                                else:
                                    # Fallback
                                    similar_cases = []
                                    for hit in hits[:k_final]:
                                        all_text = " ".join([str(v) for k, v in hit['_source'].items() 
                                                           if isinstance(v, str) and len(v) > 50])
                                        if all_text:
                                            similar_cases.append(all_text)
            except Exception as e:
                similar_cases = []
        
        # ===== é–‹å§‹æ··åˆæ¨¡å¼ç”Ÿæˆ =====
        print(f"\nğŸ¯ é–‹å§‹æ··åˆæ¨¡å¼ç”Ÿæˆ...")
        print("ğŸ“ äº‹å¯¦ + æ³•æ¢ + æå®³ï¼šæ¨™æº–æ–¹å¼")
        print("ğŸ§  çµè«–ï¼šCoTæ–¹å¼ï¼ˆè¨ˆç®—ç¸½é‡‘é¡ï¼‰")
        print()
        
        # ç”Ÿæˆäº‹å¯¦æ®µè½
        print("ğŸ“ ç”Ÿæˆäº‹å¯¦æ®µè½...")
        facts = generator.generate_standard_facts(accident_facts, similar_cases)
        print("âœ… äº‹å¯¦æ®µè½ç”Ÿæˆå®Œæˆ")
        
        # ç”Ÿæˆæ³•å¾‹ä¾æ“š
        print("âš–ï¸ ç”Ÿæˆæ³•å¾‹ä¾æ“š...")
        
        # çµ±è¨ˆç›¸ä¼¼æ¡ˆä¾‹ä½¿ç”¨çš„æ³•æ¢
        if similar_cases and 'final_case_ids' in locals():
            try:
                print("ğŸ“Š åˆ†æç›¸ä¼¼æ¡ˆä¾‹ä½¿ç”¨çš„æ³•æ¢...")
                similar_laws_stats = get_similar_cases_laws_stats(final_case_ids)
                if similar_laws_stats:
                    print("ğŸ“‹ ç›¸ä¼¼æ¡ˆä¾‹å¸¸ç”¨æ³•æ¢çµ±è¨ˆ:")
                    for law_name, count in similar_laws_stats[:5]:  # é¡¯ç¤ºå‰5å€‹æœ€å¸¸ç”¨çš„
                        print(f"   â€¢ {law_name}: {count}æ¬¡")
                    print()
            except Exception as e:
                print(f"âš ï¸ æ³•æ¢çµ±è¨ˆåˆ†æå¤±æ•—: {e}")
        
        laws = generator.generate_standard_laws(
            sections.get("accident_facts", user_query),
            sections.get("injuries", ""),
            parties,
            sections.get("compensation_facts", "")
        )
        print("âœ… æ³•å¾‹ä¾æ“šç”Ÿæˆå®Œæˆ")
        
        # ç”Ÿæˆæå®³è³ å„Ÿ
        print("ğŸ’° ç”Ÿæˆæå®³è³ å„Ÿ...")
        compensation_text = sections.get("compensation_facts", user_query)
        damages = generator.generate_smart_compensation(
            sections.get("injuries", ""),
            compensation_text, 
            parties
        )
        print("âœ… æå®³è³ å„Ÿç”Ÿæˆå®Œæˆ")
        
        # ç”ŸæˆCoTçµè«–
        print("ğŸ§  ç”ŸæˆCoTçµè«–ï¼ˆå«ç¸½é‡‘é¡è¨ˆç®—ï¼‰...")
        conclusion = generator.generate_cot_conclusion_with_structured_analysis(
            sections.get("accident_facts", user_query),
            compensation_text,
            parties
        )
        print("âœ… CoTçµè«–ç”Ÿæˆå®Œæˆ")
        print()
        print("âœ… æ‰€æœ‰ç”Ÿæˆæ­¥é©Ÿå®Œæˆï¼")
        
        # æå–é©ç”¨æ³•æ¢
        applicable_laws = determine_applicable_laws(
            sections.get("accident_facts", user_query),
            sections.get("injuries", ""),
            sections.get("compensation_facts", ""),
            parties
        )
        
        # ===== è¼¸å‡ºæ ¸å¿ƒçµæœ =====
        print("\n" + "=" * 60)
        print("âš–ï¸ é©ç”¨æ³•æ¢")
        print("=" * 60)
        
        for i, law in enumerate(applicable_laws, 1):
            print(f"{i}. {law}")
        
        print("\n" + "=" * 60)
        print("ğŸ“‹ ç”Ÿæˆçš„èµ·è¨´ç‹€")
        print("=" * 60)
        
        print(f"\n{facts}")
        print(f"\n{laws}")
        print(f"\n{damages}")
        print(f"\n{conclusion}")
        
        print("\n" + "=" * 60)
        print("âœ… èµ·è¨´ç‹€ç”Ÿæˆå®Œæˆ!")
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆéç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}")
        print("è«‹æª¢æŸ¥è¼¸å…¥æ ¼å¼æˆ–è¯ç¹«ç³»çµ±ç®¡ç†å“¡")

def main():
    """ä¸»ç¨‹åºå…¥å£"""
    try:
        # æª¢æŸ¥ä¾è³´
        print("ğŸ”§ æª¢æŸ¥ç³»çµ±ä¾è³´...")
        print(f"ğŸ“Š æª¢ç´¢æ¨¡å¼ï¼š{'å®Œæ•´æ¨¡å¼' if FULL_MODE else 'ç°¡åŒ–æ¨¡å¼'}")
        print(f"ğŸ—ï¸ çµæ§‹åŒ–è™•ç†å™¨ï¼š{'å¯ç”¨' if STRUCTURED_PROCESSOR_AVAILABLE else 'ä¸å¯ç”¨'}")
        print(f"ğŸ“ åŸºæœ¬æ¨™æº–åŒ–å™¨ï¼š{'å¯ç”¨' if BASIC_STANDARDIZER_AVAILABLE else 'ä¸å¯ç”¨'}")
        
        # å•Ÿå‹•äº’å‹•ç•Œé¢
        interactive_generate_lawsuit()
        
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ ç”¨æˆ¶ä¸­æ–·ï¼Œç¨‹åºé€€å‡º")
    except Exception as e:
        print(f"\nâŒ ç¨‹åºåŸ·è¡ŒéŒ¯èª¤ï¼š{str(e)}")

if __name__ == "__main__":
    main()