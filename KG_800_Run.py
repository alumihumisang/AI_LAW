#!/usr/bin/env python3
"""
èµ·è¨´ç‹€ç”Ÿæˆå™¨ (ç²¾ç°¡å„ªåŒ–ç‰ˆæœ¬)
"""

import os
import re
import pandas as pd
import warnings
from typing import List, Dict, Any
from dotenv import load_dotenv

warnings.filterwarnings("ignore")

from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_ollama import OllamaLLM
from Elasticsearch_utils import ElasticsearchManager
from KG_6_Vectorize_And_Store_ES import KGVectorizer
from Neo4j_manager_utils import Neo4jManager
from text_processor import TextProcessor  # è‹¥æœ‰å…¶ä»–éœ€æ±‚ï¼Œæ­¤è™•å¯ä»¥ç•™ç©ºæˆ–è‡ªè¨‚

# --------------------
# è¼”åŠ©å‡½å¼ï¼šæ•¸å­—æ ¼å¼è½‰æ›
# --------------------
def fullwidth_to_halfwidth(s: str) -> str:
    """å°‡å…¨å½¢æ•¸å­—è½‰æ›ç‚ºåŠå½¢"""
    return ''.join(chr(ord(char) - 0xFEE0) if 'ï¼' <= char <= 'ï¼™' else char for char in s)

def chinese_to_arabic(cn: str) -> int:
    """
    ç°¡å–®å°‡ä¸­æ–‡æ•¸å­—è½‰æ›ç‚ºé˜¿æ‹‰ä¼¯æ•¸å­—ï¼ˆåƒ…æ”¯æ´å¸¸è¦‹æ ¼å¼ï¼‰ã€‚
    æ³¨æ„ï¼šè¼ƒè¤‡é›œçš„ä¸­æ–‡æ•¸å­—è½‰æ›å¯èƒ½éœ€è¦æ›´å®Œæ•´çš„å¯¦ç¾ã€‚
    """
    cn = cn.strip()
    num_map = {'é›¶': 0, 'ä¸€': 1, 'äºŒ': 2, 'ä¸‰': 3, 'å››': 4, 'äº”': 5, 'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9}
    unit_map = {'å': 10, 'ç™¾': 100, 'åƒ': 1000, 'è¬': 10000, 'å„„': 100000000}
    result = 0
    tmp = 0
    unit = 1
    # å¾å³å‘å·¦è§£æ
    for char in reversed(cn):
        if char in num_map:
            tmp += num_map[char] * unit
        elif char in unit_map:
            unit = unit_map[char]
            if unit >= 10000:
                result += tmp * unit
                tmp = 0
                unit = 1
            else:
                if tmp == 0:
                    tmp = 1 * unit
                else:
                    tmp = tmp * unit
                unit = 1
    result += tmp
    return result

def convert_amount(amount_str: str) -> int:
    """
    å°‡é‡‘é¡å­—ä¸²è½‰æ›ç‚ºé˜¿æ‹‰ä¼¯æ•¸å­—ï¼Œæ”¯æ´åŠå½¢ã€å…¨å½¢ä»¥åŠä¸­æ–‡æ•¸å­—æ ¼å¼ã€‚
    """
    s = fullwidth_to_halfwidth(amount_str)
    # å¦‚æœå«æœ‰ä¸­æ–‡æ•¸å­—ï¼Œå‰‡ä½¿ç”¨ä¸­æ–‡è½‰æ›
    if re.search(r"[é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒè¬å„„]", s):
        try:
            return chinese_to_arabic(s)
        except Exception as e:
            return 0
    else:
        try:
            return int(s.replace(",", ""))
        except Exception as e:
            return 0

# --------------------
# æ–°å¢å‡½å¼ï¼šé è™•ç†ã€Œè«‹æ±‚è³ å„Ÿçš„äº‹å¯¦æ ¹æ“šã€æ–‡æœ¬
# --------------------
def preprocess_compensation_claim(text: str) -> str:
    """
    å°ã€Œè«‹æ±‚è³ å„Ÿçš„äº‹å¯¦æ ¹æ“šã€æ–‡æœ¬é€²è¡Œé è™•ç†èˆ‡æ ¼å¼æ¨™æº–åŒ–ï¼š
    1. æ¸…é™¤å¤šé¤˜ç©ºç™½èˆ‡æ›è¡Œç¬¦è™Ÿã€‚
    2. å°‡æ–‡æœ¬ä¾æ“šæ›è¡Œæ‹†åˆ†æˆå¤šè¡Œï¼Œä¸¦è‡ªå‹•è£œä¸Šé †åºç·¨è™Ÿï¼ˆè‹¥è©²è¡Œæ²’æœ‰ä»¥æ•¸å­—èˆ‡å¥é»é–‹é ­ï¼‰ã€‚
    3. å¿½ç•¥ä»¥ã€Œç¸½è¨ˆã€é–‹é ­çš„è¡Œã€‚
    å›å‚³æ¨™æº–åŒ–å¾Œçš„æ–‡æœ¬ã€‚
    """
    text = re.sub(r'[ \t]+', ' ', text)
    text = re.sub(r'\r\n|\r', '\n', text)
    text = text.strip()
    lines = text.split('\n')
    standardized_lines = []
    counter = 1
    for line in lines:
        line = line.strip()
        if not line:
            continue
        if line.startswith("ç¸½è¨ˆ"):
            continue
        if not re.match(r'^\d+\.\s*', line):
            line = f"{counter}. {line}"
            counter += 1
        else:
            line = re.sub(r'^\d+\.\s*', f"{counter}. ", line)
            counter += 1
        standardized_lines.append(line)
    standardized_text = "\n".join(standardized_lines)
    return standardized_text

# --------------------
# æ–°å¢å‡½å¼ï¼šè§£ææå®³é …ç›®æ˜ç´°
# --------------------
def parse_damage_items_details(damage_text: str) -> Dict[str, List[Dict[str, Any]]]:
    """
    è§£ææå®³é …ç›®æ–‡æœ¬ï¼Œæå–æ¯ä½åŸå‘Šçš„æå®³é …ç›®æ˜ç´°ã€‚
    
    é æœŸæ ¼å¼ï¼š
      åŸå‘ŠAï¼š
      1. é …ç›®åç¨±ï¼šé‡‘é¡å…ƒ [ï¼ˆè«‹æ±‚ç†ç”±ï¼‰]
      2. é …ç›®åç¨±ï¼šé‡‘é¡å…ƒ [ï¼ˆè«‹æ±‚ç†ç”±ï¼‰]
      
      åŸå‘ŠBï¼š
      1. é …ç›®åç¨±ï¼šé‡‘é¡å…ƒ [ï¼ˆè«‹æ±‚ç†ç”±ï¼‰]
      ...
    
    è‹¥å¾‹å¸«è¼¸å…¥çš„åŸå‘Šåªæœ‰ä¸€åä¸”æœªæä¾›åç¨±ï¼Œå‰‡è‡ªå‹•è¨­å®šç‚º "åŸå‘Š"ã€‚
    
    è¼¸å‡ºçµæ§‹ï¼š
      {
         "åŸå‘ŠA": [
             {"item": "é …ç›®åç¨±", "amount": æ•¸å€¼, "reason": "è«‹æ±‚ç†ç”±"},
             ...
         ],
         "åŸå‘Š": [ ... ]
      }
    """
    result = {}
    sections = re.findall(r"åŸå‘Š\s*([^ï¼š:]*?)[ï¼š:](.*?)(?=åŸå‘Š\s*[^ï¼š:]*[ï¼š:]|$)", damage_text, re.S)
    for plaintiff, content in sections:
        name = plaintiff.strip() if plaintiff.strip() else "åŸå‘Š"
        items = []
        lines = content.strip().splitlines()
        i = 0
        while i < len(lines):
            line = lines[i].strip()
            if line.startswith("ç¸½è¨ˆ"):
                i += 1
                continue
            m = re.match(r"^\d+\.\s*(.+?)[ï¼š:]\s*([\d,ï¼-ï¼™]+)å…ƒ(?:\s*(.*))?$", line)
            if m:
                item_name = m.group(1).strip()
                amount_str = m.group(2).strip()
                reason = m.group(3).strip() if m.group(3) else ""
                if not reason and i + 1 < len(lines):
                    next_line = lines[i + 1].strip()
                    if next_line.startswith("æŸ¥"):
                        reason = next_line
                        i += 1
                amount = convert_amount(amount_str)
                items.append({
                    "item": item_name,
                    "amount": amount,
                    "reason": reason
                })
            i += 1
        result[name] = items
    return result

# --------------------
# LegalDocumentGenerator é¡åˆ¥
# --------------------
class LegalDocumentGenerator:
    def __init__(self):
        load_dotenv()
        try:
            self.embedding_model = KGVectorizer()
        except Exception as e:
            raise Exception(f"Error initializing KGVectorizer: {e}")
        try:
            self.es_manager = ElasticsearchManager(
                host="http://localhost:9201",
                username=os.getenv("ELASTIC_USER", "elastic"),
                password=os.getenv("ELASTIC_PASSWORD", "*3D7+GFIdBOT-glzYMLx"),
                verify_certs=False
            )
        except Exception as e:
            raise Exception(f"Error initializing ElasticsearchManager: {e}")
        try:
            self.neo4j_manager = Neo4jManager(
                uri=os.getenv("NEO4J_URI"),
                user=os.getenv("NEO4J_USER"),
                password=os.getenv("NEO4J_PASSWORD")
            )
        except Exception as e:
            raise Exception(f"Error initializing Neo4jManager: {e}")
        try:
            self.llm = OllamaLLM(
                model="kenneth85/llama-3-taiwan:8b-instruct-dpo",
                temperature=0.1,
                keep_alive=0
            )
        except Exception as e:
            raise Exception(f"Error initializing LLM: {e}")
        self.lawsuit_template = (
            "ä¸€ã€äº‹å¯¦æ¦‚è¿°ï¼š\n{case_facts}\n\n"
            "äºŒã€æ³•å¾‹ä¾æ“šï¼š\n{legal_reference}\n\n"
            "ä¸‰ã€æå®³é …ç›®ï¼š\n{damage_items}\n\n"
            "å››ã€çµè«–ï¼š\n{conclusion}"
        )
        self.damage_prompt = """
è«‹æ ¹æ“šä»¥ä¸‹è¼¸å…¥è³‡æ–™ï¼Œå‹•æ…‹ç”Ÿæˆã€æå®³é …ç›®ã€‘æ®µè½ã€‚è«‹å°‡å„é …æå®³æ¢åˆ—å¼åˆ—å‡ºï¼Œä¸¦æ³¨æ„ä»¥ä¸‹è¦æ±‚ï¼š
1. è‹¥æœ‰å¤šä½åŸå‘Šï¼Œè«‹ä»¥ã€ŒåŸå‘ŠOOOï¼šã€å€åˆ†ï¼Œæ¥è‘—é€é …æå®³æ¢åˆ—ã€‚
2. æ¯é …æå®³è«‹ç›´æ¥çµ¦å‡ºé‡‘é¡ï¼Œæ–°å°å¹£é‡‘é¡æ ¼å¼ä¾‹å¦‚ã€Œ38,706å…ƒã€æˆ–ã€Œ38,706å…ƒã€ã€‚
3. è‹¥æœªæåŠæŸæå®³é¡å‹ï¼Œè«‹å‹¿è‡†é€ ã€‚
4. è«‹å‹¿é‡è¤‡æ•˜è¿°åŒä¸€å€‹æå®³é …ç›®ã€‚
ä»¥ä¸‹ç‚ºç›¸é—œè¼¸å…¥è³‡æ–™ï¼š
{input_text}
"""
        self.conclusion_prompt = """
è«‹æ ¹æ“šä¸‹åˆ—ã€ŒåŸå‘Šåç¨±èˆ‡æå®³é‡‘é¡ã€ç”Ÿæˆã€çµè«–ã€‘æ®µè½ï¼š
{plaintiffs_info}

è«‹ä¾ä»¥ä¸‹æ ¼å¼èˆ‡è¦æ±‚æ’°å¯«ï¼š
1. ä»¥ã€Œç¶œä¸Šæ‰€é™³ã€é–‹é ­ï¼Œä¸¦æ¦‚æ‹¬å„åŸå‘Šè«‹æ±‚é‡‘é¡ã€‚
2. è‹¥åƒ…æœ‰ä¸€ä½åŸå‘Šï¼Œæ ¼å¼ç‚ºã€ŒåŸå‘Šè«‹æ±‚è¢«å‘Šè³ å„Ÿç¸½è¨ˆâ€¦â€¦å…ƒã€ã€‚
3. è‹¥å¤šä½åŸå‘Šï¼Œæ ¼å¼ç‚ºã€Œå…±Xä½åŸå‘Šè«‹æ±‚è¢«å‘Šé€£å¸¶è³ å„Ÿç¸½è¨ˆâ€¦â€¦å…ƒã€ã€‚
4. æœ€å¾Œé™„è¨»ã€Œè‡ªèµ·è¨´ç‹€é€é”ç¿Œæ—¥èµ·è‡³æ¸…å„Ÿæ—¥æ­¢ï¼ŒæŒ‰å¹´åˆ©ç‡5%è¨ˆç®—æ³•å®šåˆ©æ¯ã€ã€‚
5. è«‹å‹¿é‡è¤‡é‡é»å¥å­ï¼Œä¾‹å¦‚ã€Œè¢«å‘Šæ‡‰è² è³ å„Ÿè²¬ä»»ã€ç­‰åœ¨æœ¬æ®µä¸­åƒ…å‡ºç¾ä¸€æ¬¡å³å¯ã€‚
"""

    def split_input(self, user_input: str) -> Dict[str, str]:
        patterns = {
            "case_facts": r"ä¸€[ã€\.ï¼]\s*äº‹æ•…ç™¼ç”Ÿç·£ç”±[:ï¼š]\s*(.+?)(?=\n\s*äºŒ[ã€\.ï¼]|$)",
            "injury_details": r"äºŒ[ã€\.ï¼]\s*åŸå‘Šå—å‚·æƒ…å½¢[:ï¼š]\s*(.+?)(?=\n\s*ä¸‰[ã€\.ï¼]|$)",
            "compensation_request": r"ä¸‰[ã€\.ï¼]\s*è«‹æ±‚è³ å„Ÿçš„äº‹å¯¦æ ¹æ“š[:ï¼š]\s*(.+)"
        }
        result = {}
        for key, pattern in patterns.items():
            match = re.search(pattern, user_input, re.S | re.M)
            result[key] = match.group(1).strip() if match else ""
        return result

    def generate_dynamic_legal_reference(self, case_facts: str, injury_details: str, compensation_request: str) -> str:
        legal_mapping = {
            "æ°‘æ³•ç¬¬184æ¢ç¬¬1é …å‰æ®µ": ["æœªæ³¨æ„", "éå¤±", "æå®³è³ å„Ÿ", "ä¾µå®³ä»–äººä¹‹æ¬Šåˆ©"],
            "æ°‘æ³•ç¬¬185æ¢": ["å…±åŒä¾µå®³", "å…±åŒè¡Œç‚º", "æ•¸äººä¾µå®³", "é€ æ„äºº"],
            "æ°‘æ³•ç¬¬187æ¢": ["ç„¡è¡Œç‚ºèƒ½åŠ›", "é™åˆ¶è¡Œç‚ºèƒ½åŠ›", "æ³•å®šä»£ç†äºº", "è­˜åˆ¥èƒ½åŠ›", "æœªæˆå¹´"],
            "æ°‘æ³•ç¬¬188æ¢": ["å—åƒ±äºº", "åƒ±ç”¨äºº", "é›‡å‚­", "é€£å¸¶è³ å„Ÿ"],
            "æ°‘æ³•ç¬¬191-2æ¢": ["æ±½è»Š", "æ©Ÿè»Š", "äº¤é€šäº‹æ•…", "å‚·å®³", "æå®³"],
            "æ°‘æ³•ç¬¬193æ¢ç¬¬1é …": ["æå¤±", "é†«ç™‚è²»ç”¨", "å·¥ä½œ", "è–ªè³‡", "å°±é†«", "å‚·"],
            "æ°‘æ³•ç¬¬195æ¢ç¬¬1é …å‰æ®µ": ["ç²¾ç¥", "æ…°æ’«é‡‘", "ç—›è‹¦", "åè­½", "å¥åº·", "éš±ç§", "è²æ“"],
            "æ°‘æ³•ç¬¬213æ¢": ["å›å¾©åŸç‹€", "çµ¦ä»˜é‡‘éŒ¢", "æå®³ç™¼ç”Ÿ"],
            "æ°‘æ³•ç¬¬216æ¢": ["å¡«è£œæå®³", "æ‰€å¤±åˆ©ç›Š", "é æœŸåˆ©ç›Š"],
            "æ°‘æ³•ç¬¬217æ¢": ["è¢«å®³äººèˆ‡æœ‰éå¤±", "è³ å„Ÿé‡‘æ¸›è¼•", "é‡å¤§æå®³åŸå› "]
        }
        legal_references = []
        for law, keywords in legal_mapping.items():
            if any(term in case_facts for term in keywords):
                legal_references.append(law)
            if any(term in injury_details for term in keywords):
                legal_references.append(law)
            if any(term in compensation_request for term in keywords):
                legal_references.append(law)
        legal_references = list(set(legal_references))
        if not legal_references:
            return "ç›®å‰æœªæ‰¾åˆ°ç›¸é—œæ³•å¾‹æ¢æ–‡ï¼Œè«‹æª¢æŸ¥è¼¸å…¥å…§å®¹æ˜¯å¦æ­£ç¢ºã€‚"
        law_descriptions = {
            "æ°‘æ³•ç¬¬184æ¢ç¬¬1é …å‰æ®µ": "ã€Œå› æ•…æ„æˆ–éå¤±ï¼Œä¸æ³•ä¾µå®³ä»–äººä¹‹æ¬Šåˆ©è€…ï¼Œè² æå®³è³ å„Ÿè²¬ä»»ã€‚ã€",
            "æ°‘æ³•ç¬¬185æ¢": "ã€Œæ•¸äººå…±åŒä¸æ³•ä¾µå®³ä»–äººä¹‹æ¬Šåˆ©è€…ï¼Œé€£å¸¶è² æå®³è³ å„Ÿè²¬ä»»ã€‚ã€",
            "æ°‘æ³•ç¬¬187æ¢": "ã€Œç„¡è¡Œç‚ºèƒ½åŠ›äººæˆ–é™åˆ¶è¡Œç‚ºèƒ½åŠ›äººï¼Œä¸æ³•ä¾µå®³ä»–äººä¹‹æ¬Šåˆ©è€…ï¼Œç”±å…¶æ³•å®šä»£ç†äººè² è³ å„Ÿè²¬ä»»ã€‚ã€",
            "æ°‘æ³•ç¬¬188æ¢": "ã€Œå—åƒ±äººå› åŸ·è¡Œè·å‹™ï¼Œä¸æ³•ä¾µå®³ä»–äººä¹‹æ¬Šåˆ©è€…ï¼Œç”±åƒ±ç”¨äººèˆ‡è¡Œç‚ºäººé€£å¸¶è² æå®³è³ å„Ÿè²¬ä»»ã€‚ã€",
            "æ°‘æ³•ç¬¬191-2æ¢": "ã€Œæ±½è»Šã€æ©Ÿè»Šæˆ–å…¶ä»–éä¾è»Œé“è¡Œé§›ä¹‹å‹•åŠ›è»Šè¼›ï¼Œåœ¨ä½¿ç”¨ä¸­åŠ æå®³æ–¼ä»–äººè€…ï¼Œé§•é§›äººæ‡‰è³ å„Ÿå› æ­¤æ‰€ç”Ÿä¹‹æå®³ã€‚ã€",
            "æ°‘æ³•ç¬¬193æ¢ç¬¬1é …": "ã€Œä¸æ³•ä¾µå®³ä»–äººä¹‹èº«é«”æˆ–å¥åº·è€…ï¼Œæ‡‰è² æå®³è³ å„Ÿè²¬ä»»ã€‚ã€",
            "æ°‘æ³•ç¬¬195æ¢ç¬¬1é …å‰æ®µ": "ã€Œä¸æ³•ä¾µå®³ä»–äººä¹‹èº«é«”ã€å¥åº·ã€åè­½ã€è‡ªç”±ã€ä¿¡ç”¨ã€éš±ç§ã€è²æ“ï¼Œæˆ–ä¸æ³•ä¾µå®³å…¶ä»–äººæ ¼æ³•ç›Šè€Œæƒ…ç¯€é‡å¤§è€…ï¼Œè¢«å®³äººäº¦å¾—è«‹æ±‚è³ å„Ÿç›¸ç•¶ä¹‹é‡‘é¡ã€‚ã€",
            "æ°‘æ³•ç¬¬213æ¢": "ã€Œè² æå®³è³ å„Ÿè²¬ä»»è€…ï¼Œæ‡‰å›å¾©æå®³ç™¼ç”Ÿå‰ä¹‹åŸç‹€ã€‚ã€",
            "æ°‘æ³•ç¬¬216æ¢": "ã€Œæå®³è³ å„Ÿä»¥å¡«è£œå—æå®³è€…æ‰€å—æå®³åŠæ‰€å¤±åˆ©ç›Šç‚ºé™ã€‚ã€",
            "æ°‘æ³•ç¬¬217æ¢": "ã€Œæå®³ä¹‹ç™¼ç”Ÿæˆ–æ“´å¤§ï¼Œè¢«å®³äººèˆ‡æœ‰éå¤±è€…ï¼Œæ³•é™¢å¾—æ¸›è¼•è³ å„Ÿé‡‘é¡ã€‚ã€"
        }
        references_text = "ï¼›".join([law_descriptions[law] for law in legal_references])
        references_text += "ï¼›" + "ã€".join(legal_references) + "åˆ†åˆ¥å®šæœ‰æ˜æ–‡ã€‚"
        references_text += "æŸ¥è¢«å‘Šå› ä¸Šé–‹ä¾µæ¬Šè¡Œç‚ºï¼Œè‡´åŸå‘Šå—æœ‰ä¸‹åˆ—æå®³ï¼Œä¾å‰æ­è¦å®šï¼Œè¢«å‘Šæ‡‰è² æå®³è³ å„Ÿè²¬ä»»ï¼š"
        print("Matched Legal References:", legal_references)
        print("Generated References Text:", references_text)
        return references_text
    
    def get_lawyer_input_by_case_id(self, case_id: int) -> str:
        """å¾ ES è£¡æŸ¥å‡ºæŒ‡å®š case_id çš„æ¨¡æ“¬å¾‹å¸«è¼¸å…¥ï¼ˆlabel=LawyerInputï¼‰"""
        query = {
            "query": {
                "bool": {
                    "must": [
                        {"term": {"case_id": case_id}},
                        {"term": {"label": "LawyerInput"}}
                    ]
                }
            }
        }
        try:
            result = self.es_manager.client.search(index="legal_knowledge_graph", body=query, size=1)
            hits = result.get("hits", {}).get("hits", [])
            if hits:
                return hits[0]["_source"].get("text", "ç„¡")  # âœ… æ”¹ç‚ºæŠ“ text æ¬„ä½
            return "ç„¡"
        except Exception as e:
            print(f"âš ï¸ æŸ¥è©¢ lawyer_input å¤±æ•—ï¼ˆcase_id={case_id}ï¼‰ï¼š{e}")
            return "ç„¡"



    def generate_dynamic_legal_reference_combined(self, case_facts: str, injury_details: str, compensation_request: str) -> str:
        """
        æ··åˆé—œéµå­—åŒ¹é…èˆ‡ RAG çµæœï¼ŒRAG åªç”¨æ–¼ç›¸ä¼¼æ¡ˆä¾‹é¡¯ç¤ºï¼Œä¸å½±éŸ¿æ³•å¾‹ä¾æ“šæ®µè½ã€‚
        """
        keyword_result = self.generate_dynamic_legal_reference(case_facts, injury_details, compensation_request)
        combined_text = " ".join([case_facts, injury_details, compensation_request])
        embedding = self.embedding_model.embed_texts([combined_text])[0]
        try:
            rag_results = self.es_manager.search_similar(embedding, top_k=5)
        except Exception as e:
            print(f"RAG æª¢ç´¢å¤±æ•—ï¼š{e}")
            return keyword_result

        similar_cases = []
        for r in rag_results:
            source = r.get("_source", {})
            text = source.get("text", "").strip()
            case_id = source.get("case_id", "æœªçŸ¥")
            lawyer_input = self.get_lawyer_input_by_case_id(case_id)
            if text:
                similar_cases.append({
                    "case_id": case_id,
                    "lawyer_input": lawyer_input,
                    "excerpt": text
                })

        # é¡¯ç¤ºå‰ä¸‰ç­†ç›¸ä¼¼æ¡ˆä¾‹
        if similar_cases:
            print("å‰ä¸‰ç­†ç›¸ä¼¼æ¡ˆä¾‹ï¼š")
            for idx, case in enumerate(similar_cases[:3], 1):
                print(f"æ¡ˆä¾‹ {idx}:")
                print(f"  case_id: {case['case_id']}")
                print(f"  æ¨¡æ“¬å¾‹å¸«è¼¸å…¥: {case['lawyer_input']}")
                print(f"  èµ·è¨´æ›¸ç¯€éŒ„: {case['excerpt']}")

        return keyword_result

    def generate_damage_items(self, input_data: Dict[str, str]) -> str:
        compensation_request_raw = input_data.get("compensation_request", "")
        compensation_request_std = preprocess_compensation_claim(compensation_request_raw)
        
        input_text = "\n".join([
            "äº‹æ•…ç™¼ç”Ÿç¶“éï¼š",
            input_data.get("case_facts", ""),
            "\nåŸå‘Šå—å‚·æƒ…å½¢ï¼š",
            input_data.get("injury_details", ""),
            "\nè«‹æ±‚ä¾æ“šï¼š",
            compensation_request_std
        ])

        prompt_text = self.damage_prompt.format(input_text=input_text)
        chain = LLMChain(
            llm=self.llm,
            prompt=PromptTemplate(input_variables=["input_text"], template=prompt_text)
        )
        result = chain.run({"input_text": input_text})
        result = re.sub(r"[\*\#]+", "", result)
        return result.strip()

    def generate_conclusion(self, damage_text: str) -> str:
        """
        ç”Ÿæˆçµè«–ï¼š
        1. åˆ©ç”¨ parse_damage_items_details è§£æå‡ºæ¯ä½åŸå‘Šçš„æå®³é …ç›®æ˜ç´°ï¼ˆåƒ…å–é‡‘é¡ï¼‰ã€‚
        2. åˆ†åˆ¥è¨ˆç®—å„åŸå‘Šå„é …é‡‘é¡ç¸½å’Œï¼Œä¸¦è¨ˆç®—æ‰€æœ‰åŸå‘Šç¸½è¨ˆã€‚
        æ•¸å­—éƒ¨åˆ†ä»¥åƒä½é€—è™Ÿæ ¼å¼é¡¯ç¤ºï¼Œä¾‹å¦‚ 123,456å…ƒã€‚
        è‹¥ç„¡æ³•è§£æé‡‘é¡å‰‡çµ¦å‡ºé è¨­çµè«–ã€‚
        """
        details = parse_damage_items_details(damage_text)
        if not details or all(len(items) == 0 for items in details.values()):
            return (
                "ç¶œä¸Šæ‰€é™³ï¼ŒåŸå‘Šè«‹æ±‚è¢«å‘Šæ‡‰è² æå®³è³ å„Ÿè²¬ä»»ï¼Œä¸¦è‡ªèµ·è¨´ç‹€é€é”ç¿Œæ—¥èµ·è‡³æ¸…å„Ÿæ—¥æ­¢ï¼Œ"
                "æŒ‰å¹´åˆ©ç‡5%è¨ˆç®—æ³•å®šåˆ©æ¯ï¼Œç‰¹æ­¤æ•˜æ˜ã€‚"
            )
        lines = []
        overall_total = 0
        for plaintiff, items in details.items():
            total = sum(item["amount"] for item in items)
            overall_total += total
            item_amounts = " + ".join(f"{format(item['amount'], ',')}å…ƒ" for item in items if item["amount"] > 0)
            lines.append(f"åŸå‘Š {plaintiff} ä¹‹æå®³é‡‘é¡ï¼š{item_amounts} = {format(total, ',')}å…ƒ")
        if len(details) > 1:
            lines.append(f"å…±{len(details)}ä½åŸå‘Šè«‹æ±‚è¢«å‘Šé€£å¸¶è³ å„Ÿç¸½è¨ˆï¼š{format(overall_total, ',')}å…ƒ")
        else:
            lines.append(f"åŸå‘Šè«‹æ±‚è¢«å‘Šè³ å„Ÿç¸½è¨ˆï¼š{format(overall_total, ',')}å…ƒ")
        prompt_text = self.conclusion_prompt.format(
            plaintiffs_info="\n".join(lines)
        )
        chain = LLMChain(
            llm=self.llm,
            prompt=PromptTemplate(input_variables=["plaintiffs_info"], template=prompt_text)
        )
        result = chain.run({"plaintiffs_info": "\n".join(lines)})
        result = re.sub(r"[\*\#]+", "", result)
        return result.strip()

    def generate_full_lawsuit(self, user_input: str) -> str:
        input_data = self.split_input(user_input)
        case_facts = input_data.get("case_facts", "").strip()
        if not case_facts:
            case_facts = "ï¼ˆæœªæä¾›äº‹æ•…ç™¼ç”Ÿç·£ç”±ï¼Œè«‹è£œå……ï¼‰"
        else:
            if not case_facts.startswith("ç·£"):
                case_facts = "ç·£" + case_facts
        legal_reference = self.generate_dynamic_legal_reference_combined(
            case_facts,
            input_data.get("injury_details", ""),
            input_data.get("compensation_request", "")
        )
        if "è«‹è‡ªè¡Œæª¢æ ¸" not in legal_reference and "ç›®å‰æœªæ‰¾åˆ°" not in legal_reference:
            legal_reference += "\n\nç¶œä¸Šï¼Œä¾ä¸Šè¿°è¦å®šè¢«å‘Šæ‡‰è² æå®³è³ å„Ÿè²¬ä»»ã€‚"
        damage_items = self.generate_damage_items(input_data)
        conclusion = self.generate_conclusion(damage_items)
        final_text = self.lawsuit_template.format(
            case_facts=case_facts,
            legal_reference=legal_reference,
            damage_items=damage_items,
            conclusion=conclusion
        )
        final_text = re.sub(r"[\*\#]+", "", final_text)
        return final_text

    def process_excel(self, file_path: str):
        df = pd.read_excel(file_path, engine="openpyxl")
        if "å¾‹å¸«è¼¸å…¥" not in df.columns or "ç”Ÿæˆå…§å®¹" not in df.columns:
            print("Excel æ‡‰åŒ…å« 'å¾‹å¸«è¼¸å…¥' èˆ‡ 'ç”Ÿæˆå…§å®¹' æ¬„ä½")
            return
        for i, row in df.iterrows():
            if pd.notna(row['å¾‹å¸«è¼¸å…¥']):
                user_input = str(row['å¾‹å¸«è¼¸å…¥'])
                try:
                    final_text = self.generate_full_lawsuit(user_input)
                    print(f"ç¬¬ {i+1} ç­†ç”Ÿæˆçµæœï¼š\n{final_text}\n")
                    df.at[i, "ç”Ÿæˆå…§å®¹"] = final_text
                except Exception as e:
                    print(f"âš ï¸ ç¬¬ {i+1} ç­†è™•ç†å¤±æ•—ï¼š{e}")
                    df.at[i, "ç”Ÿæˆå…§å®¹"] = "âš ï¸ ç”Ÿæˆå¤±æ•—"
        out_path = file_path.replace(".xlsx", "_çµæœ.xlsx")
        df.to_excel(out_path, index=False, engine="openpyxl")
        print(f"âœ… å·²å„²å­˜è‡³ {out_path}")

if __name__ == "__main__":
    file_path = input("ğŸ“‚ è«‹è¼¸å…¥ Excel æª”æ¡ˆè·¯å¾‘ï¼š").strip()
    generator = LegalDocumentGenerator()
    generator.process_excel(file_path)
